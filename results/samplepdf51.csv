Text,Is Capitalized,Is Roman Numeral,Is Number,is_heading,is_figure_heading,is_table_heading
⇑,False,False,False,False,False,False
a,False,False,False,False,False,False
"Department of Systems and Computer Networks, Wroclaw University of Technology, Wroclaw, Poland",False,False,False,False,False,False
b,False,False,False,False,False,False
"Computational Intelligence Group, University of the Basque Country, San Sebastian, Spain",False,False,False,False,False,False
c,False,False,False,False,False,False
"Departamento de Informática y Automática, University of Salamanca, Salamanca, Spain",False,False,False,False,False,False
article info,False,False,False,False,False,False
Article history:,False,False,False,False,False,False
Available online 29 April 2013,False,False,False,False,False,False
Keywords:,False,False,False,False,False,False
Combined classiﬁer,False,False,False,False,False,False
Multiple classiﬁer system,False,False,False,False,False,False
Classiﬁer ensemble,False,False,False,False,False,False
Classiﬁer fusion,False,False,False,False,False,False
Hybrid classiﬁer,False,False,False,False,False,False
abstract,False,False,False,False,False,False
A current focus of intense research in pattern classiﬁcation is the combination of several classiﬁer sys-,False,False,False,False,False,False
"tems, which can be built following either the same or different models and/or datasets building",False,False,False,False,False,False
approaches. These systems perform information fusion of classiﬁcation decisions at different levels over-,False,False,False,False,False,False
coming limitations of traditional approaches based on single classiﬁers. This paper presents an up-to-,False,False,False,False,False,False
date survey on multiple classiﬁer system (MCS) from the point of view of Hybrid Intelligent Systems.,False,False,False,False,False,False
"The article discusses major issues, such as diversity and decision fusion methods, providing a vision of",False,False,False,False,False,False
the spectrum of applications that are currently being developed.,False,False,False,False,False,False
Ó 2013 Elsevier B.V. All rights reserved.,False,False,False,False,False,False
1. Introduction,False,False,True,True,False,False
Hybrid Intelligent Systems offer many alternatives for unortho-,False,False,False,False,False,False
"dox handling of realistic increasingly complex problems, involving",False,False,False,False,False,False
"ambiguity, uncertainty and high-dimensionality of data. They al-",False,False,False,False,False,False
low to use both a priori knowledge and raw data to compose inno-,False,False,False,False,False,False
"vative solutions. Therefore, there is growing attention to this",False,False,False,False,False,False
multidisciplinary research ﬁeld in the computer engineering re-,False,False,False,False,False,False
search community. Hybridization appears in many domains of hu-,False,False,False,False,False,False
man activity. It has an immediate natural inspiration in the human,False,False,False,False,False,False
"biological systems, such as the Central Nervous System, which is a",False,False,False,False,False,False
"de facto hybrid composition of many diverse computational units,",False,False,False,False,False,False
"as discussed since the early days of computer science, e.g., by",False,False,False,False,False,False
von Neumann [1] or Newell [2]. Hybrid approaches seek to exploit,False,False,False,False,False,False
"the strengths of the individual components, obtaining enhanced",False,False,False,False,False,False
performance by their combination. The famous ‘‘no free lunch’’ the-,False,False,False,False,False,False
orem [3] stated by Wolpert may be extrapolated to the point of,False,False,False,False,False,False
saying that there is no single computational view that solves all,False,False,False,False,False,False
problems. Fig. 1 is a rough representation of the computational do-,False,False,False,False,True,False
mains covered by the Hybrid Intelligent System approach. Some of,False,False,False,False,False,False
them deal with the uncertainty and ambiguity in the data by prob-,False,False,False,False,False,False
abilistic or fuzzy representations and feature extraction. Others,False,False,False,False,False,False
deal with optimization problems appearing in many facets of the,False,False,False,False,False,False
"intelligent system design and problem solving, either following a",False,False,False,False,False,False
"nature inspired or a stochastic process approach. Finally, classiﬁers",False,False,False,False,False,False
implementing the intelligent decision process are also subject to,False,False,False,False,False,False
"hybridization by various forms of combination. In this paper, we",False,False,False,False,False,False
"focus in this speciﬁc domain, which is in an extraordinary efferves-",False,False,False,False,False,False
"cence nowadays, under the heading of Multi-Classiﬁer Systems",False,False,False,False,False,False
"(MCS). Referring to classiﬁcation problems, Wolpert’s theorem",False,False,False,False,False,False
has an speciﬁc lecture: there is not a single classiﬁer modeling ap-,False,False,False,False,False,False
"proach which is optimal for all pattern recognition tasks, since",False,False,False,False,False,False
each has its own domain of competence. For a given classiﬁcation,False,False,False,False,False,False
"task, we expect the MCS to exploit the strengths of the individual",False,False,False,False,False,False
classiﬁer models at our disposal to produce the high quality com-,False,False,False,False,False,False
pound recognition system overcoming the performance of individ-,False,False,False,False,False,False
ual classiﬁers. Summarizing:,False,False,False,False,False,False
 Hybrid Intelligent Systems (HIS) are free combinations of compu-,False,False,False,False,False,False
"tational intelligence techniques to solve a given problem, cover-",False,False,False,False,False,False
ing al computational phases from data normalization up to ﬁnal,False,False,False,False,False,False
"decision making. Speciﬁcally, they mix heterogeneous funda-",False,False,False,False,False,False
mental views blending them into one effective working system.,False,False,False,False,False,False
 Information Fusion covers the ways to combine information,False,False,False,False,False,False
sources in a view providing new properties that may allow to,False,False,False,False,False,False
solve better or more efﬁciently the proposed problem. Informa-,False,False,False,False,False,False
tion sources can be the result of additional computational,False,False,False,False,False,False
processes.,False,False,False,False,False,False
 Multi-Classiﬁer Systems (MCS) focus on the combination of,False,False,False,False,False,False
classiﬁers form heterogenous or homogeneous modeling back-,False,False,False,False,False,False
grounds to give the ﬁnal decision. MCS are therefore a subcate-,False,False,False,False,False,False
gory of HIS.,False,False,False,False,False,False
1566-2535/$ - see front matter Ó 2013 Elsevier B.V. All rights reserved.,False,False,False,False,False,False
http://dx.doi.org/10.1016/j.inffus.2013.04.006,False,False,False,False,False,False
⇑,False,False,False,False,False,False
Corresponding author.,False,False,False,False,False,False
E-mail addresses: michal.wozniak@pwr.wroc.pl (M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
"niak), ccpgrrom@g-",False,False,False,False,False,False
"mail.com (M. Graña), escorchado@usal.es (E. Corchado).",False,False,False,False,False,False
Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
Contents lists available at SciVerse ScienceDirect,False,False,False,False,False,False
journal homepage: www.elsevier.com/locate/inffus,False,False,False,False,False,False
Historical perspective. The concept of MCS was ﬁrst presented by,False,False,False,False,False,False
"Chow [4], who gave conditions for optimality of the joint decision",False,False,False,False,False,False
1,False,False,False,False,False,False
of independent binary classiﬁers with appropriately deﬁned weights.,False,False,False,False,False,False
In 1979 Dasarathy and Sheela combined a linear classiﬁer and one k-,False,False,False,False,False,False
"NN classiﬁer [6], suggesting to identify the region of the feature",False,False,False,False,False,False
space where the classiﬁers disagree. The k-NN classiﬁer gives the an-,False,False,False,False,False,False
swer of the MCS for the objects coming from the conﬂictive region,False,False,False,False,False,False
and by the linear one for the remaining objects. Such strategy signif-,False,False,False,False,False,False
icantly decreases the exploitation cost of whole classiﬁer system.,False,False,False,False,False,False
"This was the ﬁrst work introducing a classiﬁer selection concept,",False,False,False,False,False,False
however the same idea was developed independently in 1981 by,False,False,False,False,False,False
Rastrigin and Erenstein [7] performing ﬁrst a feature space partition-,False,False,False,False,False,False
"ing and, second, assigning to each partition region an individual clas-",False,False,False,False,False,False
siﬁer that achieves the best classiﬁcation accuracy over it. Other,False,False,False,False,False,False
early relevant works formulated conclusions regarding MCS ’s classi-,False,False,False,False,False,False
"ﬁcation quality, such as [8] who considered a neural network ensem-",False,False,False,False,False,False
"ble, [9] with majority voting applied to handwriting recognition,",False,False,False,False,False,False
Turner in 1996 [10] showed that averaging outputs of an inﬁnite,False,False,False,False,False,False
number of unbiased and independent classiﬁers can lead to the same,False,False,False,False,False,False
"response as the optimal Bayes classiﬁer, Ho [11] underlined that a",False,False,False,False,False,False
decision combination function must receive useful representation,False,False,False,False,False,False
"of each classiﬁer’s decision. Speciﬁcally, they considered several",False,False,False,False,False,False
"method based on decision ranks, such as Borda count. Finally, the",False,False,False,False,False,False
landmark works devoted introducing bagging [12] and boosting,False,False,False,False,False,False
"[13,14] which are able to produce strong classiﬁers [15], in the (Prob-",False,False,False,False,False,False
"ably Approximately Correct) theory [16] sense, on the basis of the",False,False,False,False,False,False
"weak one. Nowadays MCS, are highlighted by review articles as a",False,False,False,False,False,False
hot topic and promising trend in pattern recognition [17–21]. These,False,False,False,False,False,False
"reviews include the books by Kuncheva [22], Rokach [23], Seni and",False,False,False,False,False,False
"Edler [24], and Baruque and Corchado [25]. Even leading-edge gen-",False,False,False,False,False,False
eral machine learning handbooks such as [26–28] include extensive,False,False,False,False,False,False
presentations of MCS concepts and architectures. The popularity of,False,False,False,False,False,False
this approach is conﬁrmed by the growing trend in the number of,False,False,False,False,False,False
publications shown in Fig. 2. The ﬁgure reproduces the evolution,False,False,False,False,False,False
of the number of references retrieved by the application of speciﬁc,False,False,False,False,False,False
keywords related to MCS since 1990. The experiment was repeated,False,False,False,False,False,False
on three well known academic search sites. The growth in the num-,False,False,False,False,False,False
ber of publications has an exponential trend. The last entry of the,False,False,False,False,False,False
1,False,False,False,False,False,False
We can retrace decision combination long way back in history. Perhaps the ﬁrst,False,False,False,False,False,False
worthy reference is the Greek democracy (meaning government of the people) ruling,False,False,False,False,False,False
that full citizens have an equal say in any decision that affects their life. Greeks,False,False,False,False,False,False
"believed in the community wisdom, meaning that the rule of the majority will",False,False,False,False,False,False
produce the optimal joint decision. In 1785 Condorcet formulated the Jury Theorem,False,False,False,False,False,False
"about the misclassiﬁcation probability of a group of independent voters [5]],",False,False,False,False,False,False
providing the ﬁrst result measuring the quality of classiﬁer committee.,False,False,False,False,False,False
Fig. 2. Evolution of the number of publications per year ranges retrieved from the,False,False,False,False,True,False
keywords speciﬁed in the plot legend. Each plot corresponds to searching site: the,False,False,False,False,False,False
"top to Google Scholar; the center to the Web of Knowledge, the bottom to Scopus.",False,False,False,False,False,False
The ﬁrst entry of the plots is for publications prior to 1990. The last entry is only for,False,False,False,False,False,False
the last 2 years.,False,False,False,False,False,False
Fig. 1. Domains of hybrid intelligent systems.,False,False,False,False,True,False
4 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
"plots corresponds to the last 2 years, and some of the keywords give",False,False,False,False,False,False
as many references as in the previous 5 years.,False,False,False,False,False,False
Advantages. Dietterich [29] summarized the beneﬁts of MCS: (a),False,False,False,False,False,False
"allowing to ﬁlter out hypothesis that, though accurate, might be",False,False,False,False,False,False
"incorrect due to a small training set, (b) combining classiﬁers",False,False,False,False,False,False
trained starting from different initial conditions could overcome,False,False,False,False,False,False
"the local optima problem, and (c) the true function may be impos-",False,False,False,False,False,False
"sible to be modeled by any single hypothesis, but combinations of",False,False,False,False,False,False
hypotheses may expand the space of representable functions.,False,False,False,False,False,False
"Rephrasing it, there is widespread acknowledgment of the follow-",False,False,False,False,False,False
ing advantages of MCS:,False,False,False,False,False,False
 MCS behave well in the two extreme cases of data availability:,False,False,False,False,False,False
"when we have very scarce data samples for learning, and when",False,False,False,False,False,False
we have a huge amount of them at our disposal. In the scarcity,False,False,False,False,False,False
"case, MCS can exploit bootstrapping methods, such as bagging",False,False,False,False,False,False
or boosting. Intuitive reasoning justiﬁes that the worst classiﬁer,False,False,False,False,False,False
"would be out of the selection by this method [30], e.g., by indi-",False,False,False,False,False,False
vidual classiﬁer output averaging [31]. In the event of availabil-,False,False,False,False,False,False
"ity of a huge amount of learning data samples, MCS allow to",False,False,False,False,False,False
train classiﬁers on dataset’s partitions and merge their decision,False,False,False,False,False,False
using appropriate combination rule [20].,False,False,False,False,False,False
 Combined classiﬁer can outperform the best individual classi-,False,False,False,False,False,False
"ﬁer [32]. Under some conditions (e.g., majority voting by a",False,False,False,False,False,False
group of independent classiﬁers) this improvement has been,False,False,False,False,False,False
proven analytically [10].,False,False,False,False,False,False
 Many machine learning algorithms are de facto heuristic search,False,False,False,False,False,False
algorithms. For example the popular decision tree induction,False,False,False,False,False,False
"method C4.5 [33] uses a greedy search approach, choosing the",False,False,False,False,False,False
search direction according to an heuristic attribute evaluation,False,False,False,False,False,False
function. Such an approach does not assure an optimal solution.,False,False,False,False,False,False
"Thus, the combined algorithm, which could start its work from",False,False,False,False,False,False
"different initial points of the search space, is equivalent to a",False,False,False,False,False,False
multi-start local random search which increases the probability,False,False,False,False,False,False
of ﬁnding an optimal model.,False,False,False,False,False,False
 MCS can easily be implemented in efﬁcient computing environ-,False,False,False,False,False,False
ments such as parallel and multithreaded computer architec-,False,False,False,False,False,False
tures [34]. Another attractive area of implementation,False,False,False,False,False,False
"solutions is distributed computing systems (i.e.: P2P, Grid or",False,False,False,False,False,False
"Cloud computing) [35,36], especially when a database is parti-",False,False,False,False,False,False
tioned for privacy reasons [37] so that partial solutions must,False,False,False,False,False,False
be computed on each partition and only the ﬁnal decision is,False,False,False,False,False,False
available as the combination of the networked decision.,False,False,False,False,False,False
 Wolpert stated that each classiﬁer has its speciﬁc competence,False,False,False,False,False,False
"domain [3], where they overcome other competing algorithms,",False,False,False,False,False,False
thus it is not possible to design a single classiﬁer which outper-,False,False,False,False,False,False
forms another ones for each classiﬁcation tasks. MCS try to,False,False,False,False,False,False
select always the local optimal model from the available pool,False,False,False,False,False,False
of trained classiﬁers.,False,False,False,False,False,False
System structure. The general structure of MCS is depicted in,False,False,False,False,False,False
Fig. 3 following a classical pattern recognition [38] application,False,False,False,False,True,False
structure. The most informative or discriminant features describ-,False,False,False,False,False,False
"ing the objects are input to the classiﬁer ensemble, formed by a",False,False,False,False,False,False
set of complementary and diverse classiﬁers. An appropriate fusion,False,False,False,False,False,False
method combines the individual classiﬁer outputs optimally to,False,False,False,False,False,False
"provide the system decision. According to Ho [39], two main",False,False,False,False,False,False
"MCS design approaches can be distinguished. On one hand, the",False,False,False,False,False,False
so-called coverage optimization approach tries to cover the space,False,False,False,False,False,False
of possible models by the generation of a set of mutually comple-,False,False,False,False,False,False
mentary classiﬁers whose combination provides optimal accuracy.,False,False,False,False,False,False
"On the other hand, the so-called decision optimization approach",False,False,False,False,False,False
concentrates on designing and training an appropriate decision,False,False,False,False,False,False
combination function over a set of individual classiﬁer given in ad-,False,False,False,False,False,False
vance [40].The main issues in MCS design are:,False,False,False,False,False,False
 System topology: How to interconnect individual classiﬁers.,False,False,False,False,False,False
 Ensemble design: How to drive the generation and selection of a,False,False,False,False,False,False
pool of valuable classiﬁers.,False,False,False,False,False,False
 Fuser design: How to build a decision combination function,False,False,False,False,False,False
(fuser) which can exploit the strengths of the selected classiﬁers,False,False,False,False,False,False
and combine them optimally.,False,False,False,False,False,False
2. System topology,False,False,True,True,False,False
Fig. 4 illustrates the two canonical topologies employed in MCS,False,False,False,False,True,False
design. The overwhelming majority of MCS reported in the litera-,False,False,False,False,False,False
"ture is structured in a parallel topology [22]. In this architecture,",False,False,False,False,False,False
"each classiﬁer is feed the same input data, so that the ﬁnal decision",False,False,False,False,False,False
of the combined classiﬁer output is made on the basis of the out-,False,False,False,False,False,False
puts of the individual classiﬁers obtained independently. Alterna-,False,False,False,False,False,False
"tively, in the serial (or conditional) topology, individual classiﬁers",False,False,False,False,False,False
"are applied in sequence, implying some kind of ranking or ordering",False,False,False,False,False,False
over them. When the primary classiﬁer cannot be trusted to clas-,False,False,False,False,False,False
"sify a given object, e.g., because of the low support/conﬁdence in",False,False,False,False,False,False
"its result, then the data is feed to a secondary classiﬁer [41,42],",False,False,False,False,False,False
"and so on, adding classiﬁers in sequence. This topology is adequate",False,False,False,False,False,False
"when the cost of classiﬁer exploitation is important, so that the pri-",False,False,False,False,False,False
"mary classiﬁer is the computationally cheapest one, and secondary",False,False,False,False,False,False
classiﬁers have higher exploitation cost [43]. This model can be ap-,False,False,False,False,False,False
plied to classiﬁers with the so-called reject option as well [44].In,False,False,False,False,False,False
[45] the ﬁrst classiﬁer in the pipeline gives an estimation of the,False,False,False,False,False,False
"certainty of the classiﬁcation, so that uncertain data samples are",False,False,False,False,False,False
"sent to a second classiﬁer, specialized in difﬁcult instances. We no-",False,False,False,False,False,False
tice the similarity of such approach to the ordered set of rules [46],False,False,False,False,False,False
"or decision list [47], when we consider each rule as the classiﬁer.",False,False,False,False,False,False
A very special case of sequential topology is the Adaboost intro-,False,False,False,False,False,False
"duced by Freund and Schapire in 1995 [48], widely applied in data",False,False,False,False,False,False
Fig. 3. Overview of multiple classiﬁer system.,False,False,False,False,True,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
5,False,False,False,False,False,False
mining problems [49]. The goal of boosting is to enhance the accu-,False,False,False,False,False,False
"racy of any given learning algorithm, even weak learning algo-",False,False,False,False,False,False
rithms with an accuracy slightly better than chance. Shapire [50],False,False,False,False,False,False
showed that weak learners can be boosted into a strong learning,False,False,False,False,False,False
algorithm by sequentially focusing on the subset of the training,False,False,False,False,False,False
data that is hardest to classify. The algorithms performs training,False,False,False,False,False,False
"of the weak learner multiple times, each time presenting it with",False,False,False,False,False,False
an updated distribution over the training examples. The distribu-,False,False,False,False,False,False
tion is altered so that hard parts of the feature space have higher,False,False,False,False,False,False
"probability, i.e. trying to achieve a hard margin distribution. The",False,False,False,False,False,False
decisions generated by the weak learners are combined into a ﬁnal,False,False,False,False,False,False
single decision. The novelty of Adaboost lies in the adaptability of,False,False,False,False,False,False
the successive distributions to the results of the previous weak,False,False,False,False,False,False
"learners, thus the name AdaptiveBoost. In the words of Kivinen",False,False,False,False,False,False
"et al. [51], AdaBoost ﬁnds a new distribution that is closest to the",False,False,False,False,False,False
old one but taking into consideration the restriction that the new,False,False,False,False,False,False
distribution must be orthogonal to the mistake vector of the cur-,False,False,False,False,False,False
rent weak learner.,False,False,False,False,False,False
3. Ensemble design,False,False,True,True,False,False
"Viewing MCS as a case of robust software [52–55], diversity",False,False,False,False,False,False
arises as the guiding measure of the design process. Classiﬁer,False,False,False,False,False,False
ensemble design aims to include mutually complementary individ-,False,False,False,False,False,False
ual classiﬁers which are characterized by high diversity and accu-,False,False,False,False,False,False
racy [56]. The emphasis from the Hybrid Intelligent System point,False,False,False,False,False,False
of view is in building MCS from components following different,False,False,False,False,False,False
"kinds of modeling and learning approaches, expecting an increase",False,False,False,False,False,False
in diversity and a decrease in classiﬁer output correlation [57].,False,False,False,False,False,False
"Unfortunately, the problem of how to measure classiﬁer diversity",False,False,False,False,False,False
is still an open research topic. Brown et al. [58] notice that we,False,False,False,False,False,False
can ensure diversity using implicit or explicit approaches. Implicit,False,False,False,False,False,False
approaches include techniques of independent generation of indi-,False,False,False,False,False,False
"vidual classiﬁers, often based on random techniques, while explicit",False,False,False,False,False,False
approaches focus on the optimization of a diversity metric over a,False,False,False,False,False,False
"given ensemble line-up. In this second kind of approaches, individ-",False,False,False,False,False,False
ual classiﬁer training is performed conditional to the previous clas-,False,False,False,False,False,False
siﬁers with the aim of exploiting the strengths of valuable,False,False,False,False,False,False
members of classiﬁer pool. This section discusses some diversity,False,False,False,False,False,False
"measures, and the procedures followed to ensure diversity in the",False,False,False,False,False,False
ensemble.,False,False,False,False,False,False
3.1. Diversity measures,False,False,False,False,False,False
"For regression problems, the variance of the outputs of ensem-",False,False,False,False,False,False
"ble members is a convenient diversity measure, because it was",False,False,False,False,False,False
proved that the error of a compound model based on a weighted,False,False,False,False,False,False
averaging of individual model outputs can be reduced according,False,False,False,False,False,False
"to increasing diversity [56,59]. Brown et al. [60] showed a func-",False,False,False,False,False,False
tional relation between diversity and individual regressor accu-,False,False,False,False,False,False
"racy, allowing to control the bias-variance tradeoff systematically.",False,False,False,False,False,False
For classiﬁcation problems such theoretical results have not,False,False,False,False,False,False
"been proved yet, however many diversity measures have been pro-",False,False,False,False,False,False
"posed till now. On the one hand, it is intuitive that increasing",False,False,False,False,False,False
diversity should lead to the better accuracy of the combined sys-,False,False,False,False,False,False
"tem, but there is no formal proof of this dependency [61], as con-",False,False,False,False,False,False
"ﬁrmed by the wide range of experimental results presented, e.g.,",False,False,False,False,False,False
in [62].In[53] authors decomposed the error of the classiﬁcation,False,False,False,False,False,False
"by majority voting into individual accuracy, good and bad diversi-",False,False,False,False,False,False
ties. The good diversity has positive impact on ensemble error,False,False,False,False,False,False
"reduction, whereas the bad diversity has the opposite effect. Shark-",False,False,False,False,False,False
ley et al. [55] proposed a hierarchy of four levels of diversity,False,False,False,False,False,False
"according to the answer of the majority rule, coincident failures,",False,False,False,False,False,False
and possibility of at least one correct answer of ensemble mem-,False,False,False,False,False,False
bers. Brown et al. [58] argue that this hierarchy is not appropriate,False,False,False,False,False,False
when the ensemble diversity varies between feature subspaces.,False,False,False,False,False,False
They formulated the following taxonomy of diversity measures:,False,False,False,False,False,False
 Pairwise measures averaging a measure between each classiﬁer,False,False,False,False,False,False
"pair in an ensemble, such as Q-statistic [58], kappa-statistics",False,False,False,False,False,False
"[63], disagreement [64] and double-fault measure [61,65].",False,False,False,False,False,False
 Non-pairwise diversity measures comparing outputs of a given,False,False,False,False,False,False
"classiﬁer and the entire ensemble, such as Kohavi–Wolpert var-",False,False,False,False,False,False
"iance [66], a measure of inter-rater (inter-classiﬁer) reliability",False,False,False,False,False,False
"[67], the entropy measure [68], the measure of difﬁculty [8],",False,False,False,False,False,False
"generalized diversity [52], and coincident failure diversity [69].",False,False,False,False,False,False
The analysis of several diversity measures [70] relating them to,False,False,False,False,False,False
"the concept of classiﬁers’ margin, showed their limitations and the",False,False,False,False,False,False
source of confusing empirical results. They relate the classiﬁer selec-,False,False,False,False,False,False
"tion to a NP-complete matrix cover problem, implying that ensem-",False,False,False,False,False,False
ble design in fact a quite difﬁcult combinatorial problem. Diversity,False,False,False,False,False,False
measures usually employ the most valuable sub-ensemble in,False,False,False,False,False,False
ensemble pruning processes [71]. To deal with the high computa-,False,False,False,False,False,False
"tional complexity of ensemble pruning, several hybrid approaches",False,False,False,False,False,False
"have been proposed such as heuristic techniques [72,73], evolution-",False,False,False,False,False,False
"ary algorithms [74,75], reinforcement learning [76], and competi-",False,False,False,False,False,False
"tive cross-validation techniques [77]. For classiﬁcation tasks, the",False,False,False,False,False,False
cost of acquiring feature values (which could be interpreted as the,False,False,False,False,False,False
price for examination or time required to collect the data for deci-,False,False,False,False,False,False
sion making) can be critical. Some authors take it into consideration,False,False,False,False,False,False
"during the component classiﬁer selection step [78,79].",False,False,False,False,False,False
Fig. 4. The canonical topologies of MCSs: parallel (top) and serial (bottom).,False,False,False,False,True,False
6 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
3.2. Ensuring diversity,False,False,False,False,False,False
"According to [22,38] we can enforce the diversity of a classiﬁer",False,False,False,False,False,False
"pool by the manipulation of either individual classiﬁer inputs, out-",False,False,False,False,False,False
"puts, or models.",False,False,False,False,False,False
3.2.1. Diversifying input data,False,False,False,False,False,False
This diversiﬁcation strategy assumes that classiﬁers trained on,False,False,False,False,False,False
different (disjoint) input subspaces become complementary. Three,False,False,False,False,False,False
general strategies are identiﬁed:,False,False,False,False,False,False
1. Using different data partitions.,False,False,True,True,False,False
2. Using different sets of features.,False,False,True,True,False,False
3. Taking into consideration the local specialization of individual,False,False,True,True,False,False
classiﬁers.,False,False,False,False,False,False
"Data partitions They may be compelled by several reasons, such",False,False,False,False,False,False
"as data privacy, or the need to learn over distributed data chunks",False,False,False,False,False,False
"stored in different databases [80–82]. Regarding data privacy, we",False,False,False,False,False,False
should notice that using distributed data may come up against le-,False,False,False,False,False,False
gal or commercial constraints which do not allow sharing raw,False,False,False,False,False,False
datasets and merging them into a common repository [37]. To en-,False,False,False,False,False,False
sure privacy we can train individual classiﬁers on each database,False,False,False,False,False,False
independently and merge their outputs using hybrid classiﬁer,False,False,False,False,False,False
principles [83]. The distributed data paradigm is strongly con-,False,False,False,False,False,False
nected with the big data analysis problem [84]. A huge database,False,False,False,False,False,False
may impede to deliver trained classiﬁers under speciﬁed time con-,False,False,False,False,False,False
"straints, imposing to resort to sampling techniques to obtain man-",False,False,False,False,False,False
ageable dataset partitions. A well known approach is cross-,False,False,False,False,False,False
validated committee which requires to minimize overlapping of,False,False,False,False,False,False
dataset partitions [56]. Providing individualized train datasets for,False,False,False,False,False,False
each classiﬁer is convenient in the case of shortage of learning,False,False,False,False,False,False
"examples. Most popular techniques, such as bagging [12] or boost-",False,False,False,False,False,False
"ing [14,19,64,85], have their origin in bootstrapping [13]. These",False,False,False,False,False,False
methods try to ascertain if a set of weak classiﬁer may produce a,False,False,False,False,False,False
strong one. Bagging applies sampling with replacement to obtain,False,False,False,False,False,False
independent training datasets for each individual classiﬁer. Boost-,False,False,False,False,False,False
ing modiﬁes the input data distribution perceived by each classiﬁer,False,False,False,False,False,False
"from the results of classiﬁers trained before, focusing on difﬁcult",False,False,False,False,False,False
"samples, making the ﬁnal decision by a weighted voting rule.",False,False,False,False,False,False
Data features May be selected to ensure diversity training of a,False,False,False,False,False,False
"pool of classiﬁers. The Random Subspace [86,87] was employed",False,False,False,False,False,False
for several types of the individual classiﬁers such as decision tree,False,False,False,False,False,False
"(Random Forest) [88], linear classiﬁers [89], or minimal distance",False,False,False,False,False,False
"classiﬁer [90,91]. It is worth pointing out the interesting proposi-",False,False,False,False,False,False
tions dedicated one-class classiﬁer presented by Nanni [92] or an,False,False,False,False,False,False
"hierarchical method of ensemble forming, based on feature space",False,False,False,False,False,False
splitting and then assigning two-class classiﬁers (i.e. Support Vec-,False,False,False,False,False,False
"tor Machines) locally presented in [93,94]. Attribute Bagging [95] is",False,False,False,False,False,False
a wrapper method that establishes the appropriate size of a feature,False,False,False,False,False,False
"subset, and then creates random projections of a given training set",False,False,False,False,False,False
by random selection of feature subsets. The classiﬁer ensemble are,False,False,False,False,False,False
train on the basis of the obtained set.,False,False,False,False,False,False
"Local specialization It is assumed for classiﬁer selection, select-",False,False,False,False,False,False
ing the best single classiﬁer from a pool of classiﬁers trained over,False,False,False,False,False,False
each partition of the feature space. It gives the MCS answer for all,False,False,False,False,False,False
objects included in the partition [7]. Some proposals assume clas-,False,False,False,False,False,False
"siﬁer local specialization, providing only locally optimal solutions",False,False,False,False,False,False
"[38,96–98,72], while others divide the feature space, selecting (or",False,False,False,False,False,False
training) a classiﬁer for each partition. Static and dynamic ap-,False,False,False,False,False,False
proaches are distinguished:,False,False,False,False,False,False
 Static classiﬁer selection [99]: the relation between region of,False,False,False,False,False,False
competence and assigned classiﬁer is ﬁxed. Kuncheva’s Cluster-,False,False,False,False,False,False
ing and Selection algorithm [100] partitions the feature space,False,False,False,False,False,False
"by a clustering algorithm, and selects the best individual classi-",False,False,False,False,False,False
ﬁer for each cluster according to its local accuracy. Adaptive,False,False,False,False,False,False
Splitting and Selection algorithm in [101] partitions the feature,False,False,False,False,False,False
space and assigns classiﬁers to each partition into one inte-,False,False,False,False,False,False
grated process. The main advantage of AdaSS is that the training,False,False,False,False,False,False
algorithm considers an area contour to determine the classiﬁer,False,False,False,False,False,False
"content and, conversely, that the region shapes adapt to the",False,False,False,False,False,False
"competencies of the classiﬁers. Additionally, the majority vot-",False,False,False,False,False,False
ing or more sophisticated rules are proposed as combination,False,False,False,False,False,False
method of area classiﬁers [102]. Lee et al. [103] used the fuzzy,False,False,False,False,False,False
entropy measure to partition the feature space and select the,False,False,False,False,False,False
relevant features with good separability for each of them.,False,False,False,False,False,False
 Dynamic classiﬁer selection: the competencies of the individual,False,False,False,False,False,False
classiﬁers are calculated during classiﬁcation operation [104–,False,False,False,False,False,False
107]. There are several interesting proposals which extend this,False,False,False,False,False,False
"concept, e.g., by using preselected committee of the individual",False,False,False,False,False,False
classiﬁer and making the ﬁnal decision on the basis of a voting,False,False,False,False,False,False
"rule [108].In[109,110] authors propose dynamic ensemble",False,False,False,False,False,False
selection based on the original competence measure using clas-,False,False,False,False,False,False
siﬁcation of so-called random reference classiﬁer.,False,False,False,False,False,False
Both static [111–113] and dynamic [114–116] classiﬁer special-,False,False,False,False,False,False
ization are widely used for data stream classiﬁcation.,False,False,False,False,False,False
3.2.2. Diversifying outputs,False,False,False,False,False,False
MCS diversity can be enforced by the manipulation of the indi-,False,False,False,False,False,False
"vidual classiﬁer outputs, so that an individual classiﬁer is designed",False,False,False,False,False,False
to classify only some classes in the problem.,False,False,False,False,False,False
The combination method should restore the whole class label,False,False,False,False,False,False
"set, e.g., a multi-class classiﬁcation problem can be decomposed",False,False,False,False,False,False
"into a set of binary classiﬁcation problems [117,118]. The most",False,False,False,False,False,False
popular propositions of two-class classiﬁer combinations are:,False,False,False,False,False,False
"OAO (one-against-one) and OAA (one-against-all)[119], where at",False,False,False,False,False,False
least one predictor relates to each class. The model that a given ob-,False,False,False,False,False,False
ject belongs to a chosen class is tested against the alternative of the,False,False,False,False,False,False
"feature vector belonging to any other class. In the OAA method, a",False,False,False,False,False,False
classiﬁer is trained to separate a chosen class from the remaining,False,False,False,False,False,False
ones. OAA returns class with maximum support. In more general,False,False,False,False,False,False
"approaches, the combination of individual outputs is made by ﬁnd-",False,False,False,False,False,False
"ing the closest class, in some sense, to the code given by the out-",False,False,False,False,False,False
puts of the individual classiﬁers. ECOC (Error Correcting Output,False,False,False,False,False,False
"Codes) model was proposed by Dieterich and Bakiri [118], who as-",False,False,False,False,False,False
sumed that a set of classiﬁers produces sequence of bits which is,False,False,False,False,False,False
related to code-words during training. The ECOC points at the class,False,False,False,False,False,False
with the smallest Hamming distance to its codeword. Passerini,False,False,False,False,False,False
et al. showed advantages of this method over traditional ones for,False,False,False,False,False,False
the ensemble of support vector machines [120].,False,False,False,False,False,False
Recently several interesting propositions on how to combine,False,False,False,False,False,False
the binary classiﬁers were proposed. Wu et al. [121] used pairwise,False,False,False,False,False,False
"coupling, Friedman employed Max-Win rule [122], Hüllermeier",False,False,False,False,False,False
proposed the adaptive weighted voting procedure [123]. A com-,False,False,False,False,False,False
prehensive recent survey of binary classiﬁer ensembles is [124].,False,False,False,False,False,False
It worth mentioning the one-class classiﬁcation model which is,False,False,False,False,False,False
the special case of binary classiﬁer trained in the absence of coun-,False,False,False,False,False,False
terexamples. Its main goal is to model normality in order to detect,False,False,False,False,False,False
anomaly or outliers from the target class [125]. To combine such,False,False,False,False,False,False
classiﬁers the typical methods developed for binary ones are used,False,False,False,False,False,False
[126] but it is worth mention the work by Wilk and Wozniak,False,False,False,False,False,False
where authors restored multi-class classiﬁcation task using a pool,False,False,False,False,False,False
of one-class classiﬁers and the fuzzy inference system [127]. The,False,False,False,False,False,False
combination methods dedicated the one-class classiﬁers still await,False,False,False,False,False,False
a proper attention [128].,False,False,False,False,False,False
3.2.3. Diversifying models,False,False,False,False,False,False
Ensembles with individual classiﬁers based on different classiﬁ-,False,False,False,False,False,False
cation models take advantage of the different biases of each classi-,False,False,False,False,False,False
"ﬁer model [3]. However, the combination rule should be carefully",False,False,False,False,False,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
7,False,False,False,False,False,False
chosen. We can combine the class labels but in the case of contin-,False,False,False,False,False,False
"uous outputs we have to normalize them, e.g., using fuzzy ap-",False,False,False,False,False,False
proach [127]. We could use the different versions of the same,False,False,False,False,False,False
"model as well, because many machine learning algorithms do not",False,False,False,False,False,False
guarantee to ﬁnd the optimal classiﬁer. Combining the results of,False,False,False,False,False,False
"various initializations may give good results. Alternatively, a pool",False,False,False,False,False,False
of classiﬁers can be produced by noise injection. Regarding neural,False,False,False,False,False,False
networks [129] it is easy to train pools of networks where each of,False,False,False,False,False,False
them is trained starting from randomly chosen initial weights.,False,False,False,False,False,False
Regarding decisions tree we can choose randomly the test for a gi-,False,False,False,False,False,False
ven node among the possible tests according to the value of a split-,False,False,False,False,False,False
ting criterion.,False,False,False,False,False,False
4. Fuser design,False,False,True,True,False,False
Some works consider the answers from a given Oracle as the,False,False,False,False,False,False
reference combination model [130]. The Oracle is an abstract com-,False,False,False,False,False,False
"bination model, built such that if at least one of the individual clas-",False,False,False,False,False,False
"siﬁers provides the correct answer, then the MCS committee",False,False,False,False,False,False
outputs the correct class too. Some researches used the Oracle in,False,False,False,False,False,False
comparative experiments to provide a performance upper bound,False,False,False,False,False,False
for classiﬁer committee [10] or information fusion methods,False,False,False,False,False,False
[131]. A simple example shows the risks of the Oracle model: as-,False,False,False,False,False,False
"sume we have two classiﬁers for a binary class problem, a random",False,False,False,False,False,False
one and the other that always returns the opposite decision; hence,False,False,False,False,False,False
the Oracle will always return the correct answer. As a consequence,False,False,False,False,False,False
the Oracle model does not ﬁt in the Bayesian paradigm. Raudys,False,False,False,False,False,False
[132] noticed that Oracle is a kind of quality measure of a given,False,False,False,False,False,False
individual classiﬁer pool. Let us systematize methods of classiﬁer,False,False,False,False,False,False
"fusion, which on the one hand could use class labels or support",False,False,False,False,False,False
"function, on the other hand combination rules could be given or",False,False,False,False,False,False
be the results of training. The taxonomy of decision fusion strate-,False,False,False,False,False,False
gies is depicted in Fig. 5.,False,False,False,False,False,False
4.1. Class label fusion,False,False,False,False,False,False
Early algorithms performing fusion of classiﬁer responses,False,False,False,False,False,False
"[9,10,61] only implemented majority voting schemes in three main",False,False,False,False,False,False
versions [22]:,False,False,False,False,False,False
" unanimous voting, so that the answer requires that all classiﬁ-",False,False,False,False,False,False
"ers agree,",False,False,False,False,False,False
" simple majority, so that the answer is given if majority is",False,False,False,False,False,False
"greater than half the pool of classiﬁers,",False,False,False,False,False,False
" majority voting, taking the answer with the highest number of",False,False,False,False,False,False
votes.,False,False,False,False,False,False
The expected error of majority voting (for independent classiﬁ-,False,False,False,False,False,False
ers with the same quality) was estimated in 1794 according to Ber-,False,False,False,False,False,False
"noulli’s equation, proven as the Condorcet Jury Theorem [5]. Later",False,False,False,False,False,False
works focused on the analytically derived classiﬁcation perfor-,False,False,False,False,False,False
mance of combined classiﬁers hold only when strong conditions,False,False,False,False,False,False
are met [8] so that they are not useful from practical point of view.,False,False,False,False,False,False
Alternative voting methods weight differently the decisions com-,False,False,False,False,False,False
"ing from different committee members [22,133]. The typical archi-",False,False,False,False,False,False
tecture of combined classiﬁer based on class labels is presented in,False,False,False,False,False,False
the left diagram of Fig. 6.In[134] authors distinguished the types,False,False,False,False,False,False
"of weighted voting according to the classiﬁer, both to the classiﬁer",False,False,False,False,False,False
"and the class, and, ﬁnally, to features values, the classiﬁer and the",False,False,False,False,False,False
"class. Anyway, no one of these models can improve over the Oracle.",False,False,False,False,False,False
"To achieve that we need additional information, such as the feature",False,False,False,False,False,False
"values [132,135,136] as depicted in the right diagram of Fig. 6.",False,False,False,False,False,False
4.2. Support function fusion,False,False,False,False,False,False
Support function fusion system architecture is depicted in,False,False,False,False,False,False
Fig. 7. Support functions provide a score for the decision taken,False,False,False,False,True,False
by an individual classiﬁer. The value of a support function is the,False,False,False,False,False,False
"estimated likelihood of a class, computed either as a neural net-",False,False,False,False,False,False
"work output, a posteriori probability, or fuzzy membership func-",False,False,False,False,False,False
"tion. First to be mentioned, the Borda count [11] computes an",False,False,False,False,False,False
score for each class on the basis of its ranking by each individual,False,False,False,False,False,False
classiﬁer. The most popular form of support function is the a pos-,False,False,False,False,False,False
"teriori probability [26], produced by the probabilistic models",False,False,False,False,False,False
embodied by the classiﬁers [137–139]. There are many works fol-,False,False,False,False,False,False
"lowing this approach, such as the optimal projective fuser of [140],",False,False,False,False,False,False
the combination of neural networks outputs according to their,False,False,False,False,False,False
"accuracy [141], and Naïve Bayes as the MCS combination method",False,False,False,False,False,False
[142].,False,False,False,False,False,False
Some analytical properties and experimental evaluations of,False,False,False,False,False,False
"aggregating methods were presented in [10,31,143,144]. The",False,False,False,False,False,False
aggregating methods use simple operators such as supremum or,False,False,False,False,False,False
"the mean value. They do not involve learning. However, they have",False,False,False,False,False,False
little practical applicability because of the hard conditions imposed,False,False,False,False,False,False
by them [145]. The main aggregating advantage is that it counter-,False,False,False,False,False,False
"acts over-ﬁtting of individual classiﬁers. According to [134], the",False,False,False,False,False,False
following types of weighted aggregation can be identiﬁed depend-,False,False,False,False,False,False
"ing on: (a) only the classiﬁer id, (b) the classiﬁer and the feature",False,False,False,False,False,False
"vector, (c) on the classiﬁer and the class, and (d) on the classiﬁer,",False,False,False,False,False,False
"the class, and the feature vector. For two-class recognition prob-",False,False,False,False,False,False
lems only the last two types of aggregation allow to produce com-,False,False,False,False,False,False
pound classiﬁer which may improve the Oracle. For many-class,False,False,False,False,False,False
"problems, it is possible to improve the Oracle [131] using any of",False,False,False,False,False,False
"these aggregation methods. Finally, another salient approach is",False,False,False,False,False,False
"the mixture of experts [146,147] which combines classiﬁer outputs",False,False,False,False,False,False
using so-called input dependent gating function. Tresp and Tanig-,False,False,False,False,False,False
"uchi [148] proposed a linear function for this fuser model, and",False,False,False,False,False,False
Cheeseman [149] proposed a mixture of Gaussian.,False,False,False,False,False,False
4.3. Trainable Fuser,False,False,False,False,False,False
Fuser weight selection can be treated as a speciﬁc learning pro-,False,False,False,False,False,False
"cess [31,136]. Shlien [150] used Dempster and Shafer’s theory to",False,False,False,False,False,False
reach a consensus on the weights to combine decision trees. Woz-,False,False,False,False,False,False
"niak [151] trained the fuser using perceptron-like learning, evolu-",False,False,False,False,False,False
"tionary algorithm [152,153]. Zheng used data envelopment",False,False,False,False,False,False
Fig. 5. A taxonomy of fusing strategies for the combination of MCS individual,False,False,False,False,True,False
decisions.,False,False,False,False,False,False
8 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
analysis [154]. Other fuser trainable methods may be strictly re-,False,False,False,False,False,False
"lated to ensemble pruning methods, when authors use some heu-",False,False,False,False,False,False
"ristic search algorithm to select the classiﬁer ensemble, as [72,141]",False,False,False,False,False,False
according to the chosen fuser.,False,False,False,False,False,False
We have to mention the group of combination methods built,False,False,False,False,False,False
"from pools of heterogenous classiﬁers, i.e. using different classiﬁca-",False,False,False,False,False,False
"tion models, such as stacking [155]. This method trains combina-",False,False,False,False,False,False
tion block using individual classiﬁer outputs presented during,False,False,False,False,False,False
classiﬁcation of the whole training set. Most of the combination,False,False,False,False,False,False
methods do not take into consideration possible relations among,False,False,False,False,False,False
individual classiﬁers. Huang and Suen [156] proposed Behavior-,False,False,False,False,False,False
Knowledge Space method which aggregates the individual classiﬁ-,False,False,False,False,False,False
ers decision on the basis of the statistical approach.,False,False,False,False,False,False
5. Concept Drift,False,False,True,True,False,False
Before entering the discussion of practical applications we con-,False,False,False,False,False,False
sider a very speciﬁc topic of real life relevance which is known as,False,False,False,False,False,False
"Concept Drift in knowledge engineering domains, or non-station-",False,False,False,False,False,False
ary processes in signal processing and statistics domains. Most of,False,False,False,False,False,False
the conventional classiﬁers do not take into consideration this phe-,False,False,False,False,False,False
nomenon. Concept Drift means that the statistical dependencies,False,False,False,False,False,False
"between object features and its classiﬁcation may change in time,",False,False,False,False,False,False
so that future data may be badly processed if we maintain the,False,False,False,False,False,False
"same classiﬁcation, because the object category or its properties",False,False,False,False,False,False
will be changing. Concept drift occurs frequently in real life,False,False,False,False,False,False
[157]. MCS are specially well suited to deal with Concept Drift.,False,False,False,False,False,False
Machine learning methods in security applications (like spam,False,False,False,False,False,False
ﬁlters or IDS/IPS) [158] or decision support systems for marketing,False,False,False,False,False,False
departments [159] require to take into account new training data,False,False,False,False,False,False
with potentially different statistical properties [116]. The occur-,False,False,False,False,False,False
rence of Concept Drift decreases the true classiﬁcation accuracy,False,False,False,False,False,False
dramatically. The most popular approaches are the Streaming,False,False,False,False,False,False
Ensemble Algorithm (SEA) [111] and the Accuracy Weighted,False,False,False,False,False,False
"Ensemble (AWE) [160]. Incoming data are collected in data chunks,",False,False,False,False,False,False
which are used to train new models. The individual classiﬁers eval-,False,False,False,False,False,False
uation is done on their accuracy on the new data. The best per-,False,False,False,False,False,False
forming classiﬁers are selected to constitute the MCS committee,False,False,False,False,False,False
"in the next time epoch. As the decision rule, the SEA uses a major-",False,False,False,False,False,False
"ity voting, whereas the AWE uses a weighted voting strategy. Ko-",False,False,False,False,False,False
tler et al. present the Dynamic Weighted Majority (DWM),False,False,False,False,False,False
algorithm [114] which modiﬁes the decision combination weights,False,False,False,False,False,False
and updates the ensemble according to number of incorrect deci-,False,False,False,False,False,False
sions made by individual classiﬁers. When a classiﬁer weight is,False,False,False,False,False,False
"too small, then it is removed from the ensemble, a new classiﬁer",False,False,False,False,False,False
is trained and added to the ensemble in its place.,False,False,False,False,False,False
"A difﬁcult problem is drift detection, which is the problem of",False,False,False,False,False,False
deciding that the Concept Drift has taken place. The current re-,False,False,False,False,False,False
search direction is to propose an additional binary classiﬁer giv-,False,False,False,False,False,False
ing the decision to rebuild the classiﬁers. The drift detector can,False,False,False,False,False,False
be based on changes in the probability distribution of the in-,False,False,False,False,False,False
"stances [161–163] or classiﬁcation accuracy [164,165]. Not all",False,False,False,False,False,False
classiﬁcation algorithms dealing with concept drift require drift,False,False,False,False,False,False
"detection, because they can adjust the model to incoming data",False,False,False,False,False,False
[166][?].,False,False,False,False,False,False
6. Applications,False,False,True,True,False,False
Reported applications of classiﬁer ensembles have grown,False,False,False,False,False,False
astoundingly in the recent years due to the increase in computa-,False,False,False,False,False,False
tional power allowing training of large collections of classiﬁers in,False,False,False,False,False,False
practical application time constraints. A recent review appears in,False,False,False,False,False,False
"[18]. Sometimes the works combine diverse kinds of classiﬁers,",False,False,False,False,False,False
"so-called heterogeneous MCS. Homogeneous MCS, such as Random",False,False,False,False,False,False
"Forest (RF), are composed of classiﬁers of the same kind. In the",False,False,False,False,False,False
"works revised below, basic classiﬁers are Multi-Layer Perceptron",False,False,False,False,False,False
"(MLP), k-Nearest Neighbor (kNN), Radial Basis Function (RBF), Sup-",False,False,False,False,False,False
"port Vector Machines (SVM), Probabilistic Neural Networks",False,False,False,False,False,False
"(PNNs), and Maximum Likelihood (ML) classiﬁers.",False,False,False,False,False,False
Fig. 6. Architecture of the MCS making decision on the basis of class label fusion only (left diagram). The right diagram corresponds to a MCS using additional information,False,False,False,False,True,False
from the feature values.,False,False,False,False,False,False
Fig. 7. Architecture of the MCS which computes the decision on the basis of support function combination.,False,False,False,False,True,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
9,False,False,False,False,False,False
We review in this section recent applications to remote sensing,False,False,False,False,False,False
"data, computer security, ﬁnancial risk assessment, fraud detection,",False,False,False,False,False,False
"recommender systems, and medical computer aided diagnosis.",False,False,False,False,False,False
6.1. Remote sensing,False,False,False,False,False,False
The main problems addressed by MCS in remote sensing do-,False,False,False,False,False,False
mains are the land cover mapping and change detection. Land cov-,False,False,False,False,False,False
er mapping consists in the identiﬁcation of materials that are in the,False,False,False,False,False,False
"surface of the area being covered. Depending on the application, a",False,False,False,False,False,False
"few general classes may be identiﬁed, i.e. vegetation, water, build-",False,False,False,False,False,False
"ings, roads, or a more precise classiﬁcation can be required, i.e.",False,False,False,False,False,False
"identifying tree or crop types. Applications include agriculture, for-",False,False,False,False,False,False
"estry, geology, urban planning, infrastructure degradation assess-",False,False,False,False,False,False
ment. Change detection consists in the identiﬁcation of places,False,False,False,False,False,False
"where the land cover has changed in time, it implies the computa-",False,False,False,False,False,False
tion over time series of images. Change detection may or may not,False,False,False,False,False,False
be based on previous or separate land cover maps. Remote sensing,False,False,False,False,False,False
"classiﬁcation can be done on a variety of data sources, sometimes",False,False,False,False,False,False
performing fusion of different data modalities. Optical data has,False,False,False,False,False,False
"better interpretability by humans, but land is easily occluded by",False,False,False,False,False,False
"weather conditions, i.e. cloud formations. Hyperspectral sensing",False,False,False,False,False,False
"provides high-dimensional data at each image pixel, with high",False,False,False,False,False,False
spectral resolution. Synthetic Aperture Radar (SAR) is not affected,False,False,False,False,False,False
"by weather or other atmospheric conditions, so that observations",False,False,False,False,False,False
are better suited for continuous monitoring of seasonally changing,False,False,False,False,False,False
land covers. SAR can provide also multivariate data from varying,False,False,False,False,False,False
"radar frequencies. Other data sources are elevation maps, and",False,False,False,False,False,False
"other ancillary information, such as the measurements of environ-",False,False,False,False,False,False
mental sensors.,False,False,False,False,False,False
6.1.1. Land cover mapping,False,False,False,False,False,False
Early application of MCS to land cover mapping consisted in,False,False,False,False,False,False
overproducing a large set of classiﬁers and searching for the opti-,False,False,False,False,False,False
"mal subset [38,65,167]. To avoid the combinatorial complexity,",False,False,False,False,False,False
"the approach performs clustering of classiﬁer error, aggregating",False,False,False,False,False,False
similar classiﬁers. The approach was proven to be optimal under,False,False,False,False,False,False
"some conditions on the classiﬁers. Interestingly, testing was per-",False,False,False,False,False,False
"formed on multi-source data, composing the pixel’s feature vector",False,False,False,False,False,False
"of joining multi-spectral with radar data channels, to compute the",False,False,False,False,False,False
"land cover map. The MCS was heterogenous, composed of MLP,",False,False,False,False,False,False
"RBF, and PNN.",False,False,False,False,False,False
The application of RF to processing remote sensing data has,False,False,False,False,False,False
been abundant in the literature. It has been applied to estimate,False,False,False,False,False,False
"land cover on Landsat data over Granada, Spain [168] and multi-",False,False,False,False,False,False
"source data in a Colorado mountainous area [169]. Speciﬁcally,",False,False,False,False,False,False
"Landsat Multi-Spectral, elevation, slope and aspect data are used",False,False,False,False,False,False
as input features. The RF approach is able to successfully fuse these,False,False,False,False,False,False
inhomogeneous informations. Works on hyperspectral images ac-,False,False,False,False,False,False
quired by the HyMap sensor have been addressed to build vegeta-,False,False,False,False,False,False
"tion thematic maps [170], comparing RF and decision tree-based",False,False,False,False,False,False
"Adaboost, as well as two feature selection methods: the out-of-",False,False,False,False,False,False
bag and a best-ﬁrst search wrapper feature subset selection meth-,False,False,False,False,False,False
"od. Diverse feature subsets are tested, and the general conclusion is",False,False,False,False,False,False
that tree ecotopes are better discriminated than grass ecotopes.,False,False,False,False,False,False
Further work with RF has been done assessing the uncertainty in,False,False,False,False,False,False
"modeling the distribution of vegetation types [171], performing",False,False,False,False,False,False
"classiﬁcation on the basis of environmental variables, in an ap-",False,False,False,False,False,False
proach that combines spatial distribution modeling by spatial,False,False,False,False,False,False
"interpolation, using sequential Gaussian simulation and the clus-",False,False,False,False,False,False
tering of species into vegetation types. Dealing with labeled data,False,False,False,False,False,False
"scarcity, there are methods [172] based on the combination of RF",False,False,False,False,False,False
and the enrichment of the training dataset with artiﬁcially gener-,False,False,False,False,False,False
"ated samples in order to increase classiﬁer diversity, which is ap-",False,False,False,False,False,False
plied to Landsat multispectral data. Artiﬁcial data is generated,False,False,False,False,False,False
from the Gaussian modeling of the data distribution. The applica-,False,False,False,False,False,False
tion of RF to SAR multitemporal data aims to achieve season invari-,False,False,False,False,False,False
"ant detection of several classes of land cover, i.e. grassland, ceral,",False,False,False,False,False,False
"forest, etc. [173]. RF performed best, with lowest spatial variability.",False,False,False,False,False,False
"Images were coregistered and some model portability was tested,",False,False,False,False,False,False
where the model trained on one SAR image was applied on other,False,False,False,False,False,False
SAR images of the same site obtained at different times. The suc-,False,False,False,False,False,False
cess of RF for remote sensing images has prompted the proposal,False,False,False,False,False,False
of an speciﬁc computational environment [174].,False,False,False,False,False,False
Ensambles of SVM have been also applied to land cover map. In-,False,False,False,False,False,False
"deed, the ground truth data scarcity has been attacked by an active",False,False,False,False,False,False
learning approach to semi-supervised SVM training [175]. The ac-,False,False,False,False,False,False
tive learning approach is based on the clustering of the unlabeled,False,False,False,False,False,False
data samples according to the clustering of the SVM outputs on,False,False,False,False,False,False
the current training dataset. Samples with higher membership,False,False,False,False,False,False
"coefﬁcient are added to the corresponding class data, and the clas-",False,False,False,False,False,False
siﬁer is retrained in an iterative process. These semi-supervised,False,False,False,False,False,False
SVM are combined in a majority voting ensemble and applied to,False,False,False,False,False,False
the classiﬁcation SPOT and Landsat optical data. Land cover classi-,False,False,False,False,False,False
ﬁcation in the speciﬁc context of shallow waters has the additional,False,False,False,False,False,False
"difﬁculties of the scattering, refraction and reﬂection effects intro-",False,False,False,False,False,False
duced by the water cover. A robust process combines a parallel and,False,False,False,False,False,False
"a serial architecture [176], where initial classiﬁcation results ob-",False,False,False,False,False,False
tained by SVM are reﬁned in a second SVM classiﬁer and the ﬁnal,False,False,False,False,False,False
result is given by a linear combination of two ensembles of SVM,False,False,False,False,False,False
"classiﬁers and a minimum distance classiﬁer. Besides, the system",False,False,False,False,False,False
estimates the water depth by a bathymetry estimation process.,False,False,False,False,False,False
The approach is applied to Landsat images for the estimation of,False,False,False,False,False,False
coral population in coastal waters. Polarimetric SAR data used for,False,False,False,False,False,False
the classiﬁcation of Boreal forests require an ensemble of SVM,False,False,False,False,False,False
"[177]. Each of the SVM is speciﬁcally tuned to a class, with speciﬁc",False,False,False,False,False,False
feature selection process. Best results are obtained when multi-,False,False,False,False,False,False
"temporal data is used, joining two images from two different sea-",False,False,False,False,False,False
sons (summer and winter) and performing the feature selection,False,False,False,False,False,False
and training on the joint data vectors.,False,False,False,False,False,False
6.1.2. Change detection,False,False,False,False,False,False
Early application of MCS to land cover change detection was,False,False,False,False,False,False
"based on non-parametric algorithms, speciﬁcally MLP, k-NN, RBF,",False,False,False,False,False,False
"and ML classiﬁers [178,179], where classiﬁer fusion was performed",False,False,False,False,False,False
"either by majority voting, Bayesian average and maximum a poste-",False,False,False,False,False,False
riori probability. Testing data were Thematic Mapper multispectral,False,False,False,False,False,False
"images, and the Synthetic Aperture Radar (SAR) of Landsat 5 satel-",False,False,False,False,False,False
lite. Recent works on change detection in panchromatic images,False,False,False,False,False,False
with MCS follow three different decision fuser strategies: majority,False,False,False,False,False,False
"voting, Dempster-Shafer evidence theory, and the Fuzzy Integral",False,False,False,False,False,False
[180]. The sequential process of the images previous to classiﬁca-,False,False,False,False,False,False
"tion includes pan-sharpening of the multi-temporal images, co-",False,False,False,False,False,False
"registration, raw radiometric change detection by image subtrac-",False,False,False,False,False,False
"tion and automatic thresholding, and a ﬁnal MCS decision com-",False,False,False,False,False,False
puted on the multi-spectral data and the change detection data,False,False,False,False,False,False
obtained from the various pan-sharpening approaches.,False,False,False,False,False,False
6.2. Computer security,False,False,False,False,False,False
Computer security is at the core of most critical services nowa-,False,False,False,False,False,False
"days, from universities, banking, companies, communication. Se-",False,False,False,False,False,False
"cure information processing is a growing concern, and the",False,False,False,False,False,False
machine learning approaches are trying to provide predictive solu-,False,False,False,False,False,False
tions that may allow to avoid the negative impact of such attacks.,False,False,False,False,False,False
"Here we introduce some of the problems, with current solutions",False,False,False,False,False,False
proposed from the MCS paradigm.,False,False,False,False,False,False
6.2.1. Distributed denial of service,False,False,False,False,False,False
Distributed denial of service (DDoS) are among the most threat-,False,False,False,False,False,False
ening attacks that an Internet Service Provider may face. Distrib-,False,False,False,False,False,False
"uted service providers, such as military applications, e-healthcare",False,False,False,False,False,False
10 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
"and e-governance can be very sensitive to this type of attacks,",False,False,False,False,False,False
"which can produce network performance degradation, service",False,False,False,False,False,False
"unavailability, and revenue loss. There is a need for intelligent sys-",False,False,False,False,False,False
tems able to discriminate legitimate ﬂash crowds from an attack. A,False,False,False,False,False,False
general architecture for automatic detection of DDoS attacks is,False,False,False,False,False,False
needed where the attack detection may be performed by a MCS.,False,False,False,False,False,False
The MCS constituent classiﬁers may be ANNs trained with robust,False,False,False,False,False,False
"learning algorithms, i.e. Resilient Back Propagation (RBP). Speciﬁ-",False,False,False,False,False,False
"cally, a boosting strategy is deﬁned on the ensemble of RBP trained",False,False,False,False,False,False
"ANNs, and a Neyman Pearson approach is used to make the ﬁnal",False,False,False,False,False,False
decision [181]. This architecture may be based on Sugeno Adaptive,False,False,False,False,False,False
Neuro-Fuzzy Inference Systems (ANFIS) [182]. A critical issue of,False,False,False,False,False,False
"the approach is the need to report validation results, which can",False,False,False,False,False,False
only be based on recorded real life DDoS attacks. There are some,False,False,False,False,False,False
public available datasets to perform and report these results. How-,False,False,False,False,False,False
"ever, results reported on these datasets may not be informative of",False,False,False,False,False,False
the system performance on new attacks which may have quite dif-,False,False,False,False,False,False
ferent features. This is a pervasive concern in all security applica-,False,False,False,False,False,False
tions of machine learning algorithms.,False,False,False,False,False,False
6.2.2. Malware,False,False,False,False,False,False
"Malicious code, such as trojans, virus, spyware, detection by",False,False,False,False,False,False
anti-virus approaches can only be performed after some instance,False,False,False,False,False,False
"of the code has been analyzed ﬁnding some kind of signature,",False,False,False,False,False,False
therefore some degree of damage has already been done. Predic-,False,False,False,False,False,False
tive approaches based on Machine Learning techniques may al-,False,False,False,False,False,False
low anticipative detection at the cost of some false positives.,False,False,False,False,False,False
Classiﬁers learn patterns in the known malicious codes extrapo-,False,False,False,False,False,False
lating to yet unseen codes. A taxonomy of such approaches is gi-,False,False,False,False,False,False
ven in [183]. describing the basic code representation by byte,False,False,False,False,False,False
"and opcode n-grams, strings, and others like portable executable",False,False,False,False,False,False
"features. Feature selection processes, such as the Fisher score,",False,False,False,False,False,False
"are applied to ﬁnd the most informative features. Finally, classi-",False,False,False,False,False,False
ﬁers tested in this problem include a wide variety of MCS com-,False,False,False,False,False,False
bining diverse base classiﬁers with all standard fuser designs.,False,False,False,False,False,False
Results have been reported that MCS overcome other ap-,False,False,False,False,False,False
"proaches, are better suitable for active learning needed to keep",False,False,False,False,False,False
the classiﬁers updated and tuned to the changing malicious code,False,False,False,False,False,False
versions.,False,False,False,False,False,False
6.2.3. Intrusion detection,False,False,False,False,False,False
Intrusion Detection and Intrusion Prevention deal with the,False,False,False,False,False,False
identiﬁcation of intruder code in a networked environment via,False,False,False,False,False,False
the monitoring of communication patterns. Intruder detection per-,False,False,False,False,False,False
formed as an anomaly detection process allows to detect previ-,False,False,False,False,False,False
"ously unseen patterns, at the cost of false alarms, contrary to",False,False,False,False,False,False
signature based approaches. The problem is attacked by modular,False,False,False,False,False,False
MCS whose compounding base classiﬁers are one-class classiﬁers,False,False,False,False,False,False
built by the Parzen window probability density estimation ap-,False,False,False,False,False,False
proach [128]. Each module is specialized in a speciﬁc protocol or,False,False,False,False,False,False
"network service, so that different thresholds can be tuned for each",False,False,False,False,False,False
module allowing some optimization of the false alarm rate. On the,False,False,False,False,False,False
"other hand, Intrusion Prevention tries to impede the execution of",False,False,False,False,False,False
"the intruder code by fail-safe semantics, automatic response and",False,False,False,False,False,False
adaptive enforcement. An approach relies on the fact that Instruc-,False,False,False,False,False,False
"tion Set Randomization prevents code injection attacks, so that de-",False,False,False,False,False,False
tected injected code can be used for adaptation of the anomaly,False,False,False,False,False,False
classiﬁer and the signature-based ﬁltering [184]. Clustering of n-,False,False,False,False,False,False
grams is performed to obtain a model of the normal communica-,False,False,False,False,False,False
tion behavior which is accurate allowing zero-day detection of,False,False,False,False,False,False
worm infection even in the case of low payload or slow penetration,False,False,False,False,False,False
[185]. The interesting proposed hybrid intrusion detection was,False,False,False,False,False,False
"presented in [186], where decision trees and support vector ma-",False,False,False,False,False,False
chines are combined as a hierarchical hybrid intelligent system,False,False,False,False,False,False
model.,False,False,False,False,False,False
6.2.4. Wireless sensor networks,False,False,False,False,False,False
Wireless sensor networks (WSNs) are collections of inexpen-,False,False,False,False,False,False
"sive, low power devices deployed over a geographical space for",False,False,False,False,False,False
"monitoring, measuring and event detection. Anomalies in the",False,False,False,False,False,False
"WSN can be due to failures in software or hardware, or to mali-",False,False,False,False,False,False
cious attacks compelling the sensors to bias or drop their informa-,False,False,False,False,False,False
tion and measurements. Anomaly detection in WSN is performed,False,False,False,False,False,False
"using an ensemble of binary classiﬁers, each tuned on diverse",False,False,False,False,False,False
"parameters and built following a different approach (Average,",False,False,False,False,False,False
"autorregresive, neural network, ANFIS). The decision is made by a",False,False,False,False,False,False
weighted combination of the classiﬁers outputs [187].,False,False,False,False,False,False
"6.3. Banking, credit risk, fraud detection",False,False,False,False,False,False
"In the current economical situation, the intelligent processing of",False,False,False,False,False,False
"ﬁnancial information, the assessing of ﬁnancial or credit risks, and",False,False,False,False,False,False
related issues have become a prime concern for society and for the,False,False,False,False,False,False
computational intelligence community. Developing new tools may,False,False,False,False,False,False
allow to avoid in the future the dire problems faced today by soci-,False,False,False,False,False,False
"ety. In this section we review some of the most important issues,",False,False,False,False,False,False
gathering current attempts to the deal with them.,False,False,False,False,False,False
6.3.1. Fraud detection,False,False,False,False,False,False
Fraud detection involves identifying fraud as soon as possible,False,False,False,False,False,False
after it has been perpetrated. Fraud detection [188] is big area of,False,False,False,False,False,False
"research and applications of machine learning, which has provided",False,False,False,False,False,False
"techniques to counteract fraudsters in credit card fraud, money",False,False,False,False,False,False
"laundering, telecommunications fraud, and computer intrusion.",False,False,False,False,False,False
MCS have been also applied successfully in this domain. A key task,False,False,False,False,False,False
is modeling the normal behavior in order to be able to establish,False,False,False,False,False,False
suspicion scores for outliers. Probabilistic networks are speciﬁc,False,False,False,False,False,False
"one-class classiﬁers that are well suited to this task, and bagging",False,False,False,False,False,False
of probabilistic networks has been proposed as a general tool for,False,False,False,False,False,False
fraud detection because the MCS approach improves the robust-,False,False,False,False,False,False
ness of the normal behavior modeling [189].,False,False,False,False,False,False
6.3.2. Credit card fraud,False,False,False,False,False,False
Speciﬁc works on credit card fraud detection use real-life data,False,False,False,False,False,False
of transactions from an international creditcard operation [190].,False,False,False,False,False,False
The exploration of the sensitivity to the ratio of fraud to non-fraud,False,False,False,False,False,False
of the random undersampling approach to deal with unbalanced,False,False,False,False,False,False
class sizes is required to validate the approaches. Comparing RF,False,False,False,False,False,False
"against SVM and logisti regression [190], RF was the best per-",False,False,False,False,False,False
former in all experimental conditions as measured by almost all,False,False,False,False,False,False
performance measurements. Other approaches to this problem in-,False,False,False,False,False,False
clude a bagged ensemble of SVM tested on a british card applica-,False,False,False,False,False,False
tion approval dataset [191].,False,False,False,False,False,False
6.3.3. Stock market,False,False,False,False,False,False
Trade based stock market manipulation try to inﬂuence the,False,False,False,False,False,False
stock values simply by buying and then selling. It is difﬁcult to de-,False,False,False,False,False,False
tect because rules for detection quickly become outdated. An inno-,False,False,False,False,False,False
vative research track is the use of peer-group analysis for trade,False,False,False,False,False,False
"stock manipulation detection, based on the detection of outliers",False,False,False,False,False,False
whose dynamic behavior separates from that of the previously,False,False,False,False,False,False
"similar stock values, its peers [192]. Dynamic clustering allows to",False,False,False,False,False,False
track in time the evolution of the community of peers related to,False,False,False,False,False,False
"the stocks under observation, and outlier detection techniques",False,False,False,False,False,False
are required to detect the manipulation events.,False,False,False,False,False,False
6.3.4. Credit risk,False,False,False,False,False,False
Credit risk prediction models seek to predict whether an indi-,False,False,False,False,False,False
vidual will default on a loan or not. It is greatly affected by the,False,False,False,False,False,False
"unavailability, scarcity and incompleteness of data. The application",False,False,False,False,False,False
of machine learning to this problem includes the evaluation of bag-,False,False,False,False,False,False
"ging, boosting, stacking as well as other conventional classiﬁers",False,False,False,False,False,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
11,False,False,False,False,False,False
"over three benchmarking datasets, including sensitivity to noise",False,False,False,False,False,False
added to the attributes [193]. Another approach for this problem,False,False,False,False,False,False
is the Error Trimmed Boosting (ETB) [194] which has been tested,False,False,False,False,False,False
over a privative dataset provided by a company. ETB consists in,False,False,False,False,False,False
the iterative selection of subsets of samples based on their error,False,False,False,False,False,False
under the current classiﬁer. An special case of credit risk is enter-,False,False,False,False,False,False
prise risk assessment which has a strong economic effect due to,False,False,False,False,False,False
the ﬁnancial magnitude of the entities involved. To deal with this,False,False,False,False,False,False
problem a combination of bagging and random subspace feature,False,False,False,False,False,False
selection using SVM as the base classiﬁer has been developed,False,False,False,False,False,False
and tested. The resulting method has increased diversity improv-,False,False,False,False,False,False
ing results over a dataset provided by the Bank of China [195].,False,False,False,False,False,False
Bankruptcy prediction is a dramatic special case of credit risk.,False,False,False,False,False,False
Ensemble systems with diversity ensured by genetic algorithm,False,False,False,False,False,False
based selection of component classiﬁers is proposed in [196] for,False,False,False,False,False,False
bankruptcy prediction in South Korean ﬁrms. The prediction of fail-,False,False,False,False,False,False
ure of dotcom companies has been a matter of research since the,False,False,False,False,False,False
"bubble explosion after the year 2000. Tuning a hybrid of PNN,",False,False,False,False,False,False
MLP and genetic programming classiﬁers over a set of features se-,False,False,False,False,False,False
lected applying a t-test and F-test for relevance to the categorical,False,False,False,False,False,False
variable has given some solutions [197]. The same approach is re-,False,False,False,False,False,False
ported in [198] to detect fraud in the ﬁnancial statement of big,False,False,False,False,False,False
companies.,False,False,False,False,False,False
6.3.5. Financial risks,False,False,False,False,False,False
Uncertainty in the ﬁnancial operations is identiﬁed with the,False,False,False,False,False,False
"ﬁnancial risks such as credit, business, investment, and operational",False,False,False,False,False,False
risks. Financial distress can be detected by clustering and MCS in,False,False,False,False,False,False
four different combination models. Clustering is performed by,False,False,False,False,False,False
classical SOM and k-means algorithms and used to partition the,False,False,False,False,False,False
data space prior to MCS training [199]. Experimental framework,False,False,False,False,False,False
"for the evaluation of ﬁnancial risk assessment models, giving a spe-",False,False,False,False,False,False
ciﬁc performance measures allow the exploration of computational,False,False,False,False,False,False
solutions to these problems [200]. Several conventional classiﬁers,False,False,False,False,False,False
and MCS have been tested in this framework using a large pool of,False,False,False,False,False,False
datasets. Bank performance and bankruptcy prediction is ad-,False,False,False,False,False,False
"dressed using a widely heterogenous MCS including PNN,RBF,",False,False,False,False,False,False
"MLP, SVM, CART trees, and a fuzzy rule system. The effect of PCA",False,False,False,False,False,False
initial dimensionality reduction is also tested [201]. The effect of,False,False,False,False,False,False
feature construction from previous experience and a priori infor-,False,False,False,False,False,False
mation in the efﬁciency of classiﬁers for early warning of bank fail-,False,False,False,False,False,False
ures is reported in [202].,False,False,False,False,False,False
6.3.6. New fraud trends,False,False,False,False,False,False
Prescription fraud has been identiﬁed as a cause of substantial,False,False,False,False,False,False
"monetary loss in health care systems, it consists in the prescription",False,False,False,False,False,False
of unnecessary medicaments. The research works need to real life,False,False,False,False,False,False
data from a large multi-center medical prescription database [203].,False,False,False,False,False,False
The authors use a novel distance based on data-mining approach in,False,False,False,False,False,False
a system which is capable of self-learning by regular updates. The,False,False,False,False,False,False
system is designed to perform on-line risky prescription detection,False,False,False,False,False,False
followed by off-line expert evaluation.,False,False,False,False,False,False
A new brand of frauds appear in the online gaming and lotter-,False,False,False,False,False,False
"ies, i.e. intended for money laundering, whose detection is dealt",False,False,False,False,False,False
with a mixture of supervised and unsupervised classiﬁers [204].,False,False,False,False,False,False
"To be adaptive to fraudster evolving strategies, it is required to",False,False,False,False,False,False
"emphasize online learning, and online cluster detection. Fraud in",False,False,False,False,False,False
telecommunication systems involving usage beyond contract spec-,False,False,False,False,False,False
"iﬁcations is dealt with in [205] by a preprocessing, clustering and",False,False,False,False,False,False
classiﬁcation pipeline. Clustering has been found to improve clas-,False,False,False,False,False,False
"siﬁcation performance, and boosted trees are the best performing",False,False,False,False,False,False
approach. The analysis of social networks by means of MCS may al-,False,False,False,False,False,False
"low the detection of fraud in automobile insurance, consisting in",False,False,False,False,False,False
staging trafﬁc accidents and issuing fake insurance claims to their,False,False,False,False,False,False
general or vehicle insurance company [206].,False,False,False,False,False,False
6.4. Medicine,False,False,False,False,False,False
Medicine is a big area of application of any innovative compu-,False,False,False,False,False,False
"tational approach, dealing with massive amounts of data in some",False,False,False,False,False,False
"instances, and with very imprecise or ambiguous data in other sit-",False,False,False,False,False,False
"uations. The range of applications is quite big, so here we only give",False,False,False,False,False,False
a scrap of all the current problems and approaches related with the,False,False,False,False,False,False
"MCS paradigm. In Medicine, a speciﬁc research area since the",False,False,False,False,False,False
inception of Artiﬁcial Intelligence is the construction of Computer,False,False,False,False,False,False
Aided Diagnosis (CAD) systems or Clinical Decision Support Sys-,False,False,False,False,False,False
"tems (CDSS) [207], which involve as the ﬁnal step some kind of",False,False,False,False,False,False
classiﬁer predicting the subject’s disease or normal status. In CDSS,False,False,False,False,False,False
"development, there are several steps such as the deﬁnition of the",False,False,False,False,False,False
"sensor providing the data, the preprocessing of the data to normal-",False,False,False,False,False,False
"ize it and remove noise, the selection of features, and the ﬁnal",False,False,False,False,False,False
selection of the classiﬁer.,False,False,False,False,False,False
6.4.1. Coronary diseases,False,False,False,False,False,False
A recent instance of CDSS is the application to cardiovascular,False,False,False,False,False,False
"disease diagnosis of an heterogenous collection of classiﬁers, com-",False,False,False,False,False,False
"posed of SVM, bayesian networks and ANN [208] ﬁnding ten new",False,False,False,False,False,False
biomarkers. In this AptaCDSS-E process starts with the use of an,False,False,False,False,False,False
aptamer biochip scanning protein expression levels which is the,False,False,False,False,False,False
input to physician taking the decisions afterwards. Feature selec-,False,False,False,False,False,False
tion is performed by an ANOVA analysis. Doctor decisions are,False,False,False,False,False,False
stored for system retraining. Classiﬁer combination is done by,False,False,False,False,False,False
majority voting or hierarchical fusion. Many CAD systems related,False,False,False,False,False,False
with coronary diseases are based on the information provided by,False,False,False,False,False,False
"the electrocardiogram (ECG), so that many of them rely on the fea-",False,False,False,False,False,False
tures extracted from them. Coronary artery disease is a broad term,False,False,False,False,False,False
that encompasses any condition that affects the heart. It is a,False,False,False,False,False,False
chronic disease in which the coronary arteries gradually harden,False,False,False,False,False,False
"and narrow, there have approaches to provide CAD for this condi-",False,False,False,False,False,False
"tion, such as the use of a mixture of three ANNs for the prediction",False,False,False,False,False,False
of coronary artery disease [209]. The dysfunction or abnormality of,False,False,False,False,False,False
one or more of the heart four valves is called valvular heart disease.,False,False,False,False,False,False
Its diagnosis is performed by neural network ensembles in,False,False,False,False,False,False
"[209,210] over features selected by a correlation analysis with",False,False,False,False,False,False
the categorical variable. Two separate ANNs are trained to identify,False,False,False,False,False,False
myocardial infarction on training sets with different statistics,False,False,False,False,False,False
regarding the percentage of patients in [211]. The network special-,False,False,False,False,False,False
"ized in healthy controls is applied to the new data, if the output is",False,False,False,False,False,False
"below a threshold the subject is deemed healthy, otherwise the",False,False,False,False,False,False
disease-speciﬁc network is applied to decide.,False,False,False,False,False,False
6.4.2. Proteomics,False,False,False,False,False,False
Proteins are said to have a common fold if they have the same,False,False,False,False,False,False
major secondary structure in the same arrangement and with the,False,False,False,False,False,False
same topology. Machine learning techniques have been proposed,False,False,False,False,False,False
for three-dimensional protein structure prediction. Early ap-,False,False,False,False,False,False
"proaches consisted in hybrid systems, such as the ANN, statistical",False,False,False,False,False,False
classiﬁer and case base reasoning classiﬁer combined by majority,False,False,False,False,False,False
"voting of [212]. For instance, an ensemble of K-local hyperplanes",False,False,False,False,False,False
based on random subspace and feature selection has been tested,False,False,False,False,False,False
"[213], where feature selection is done according to distance to",False,False,False,False,False,False
the class centroids. A recent approach is the MarFold [214] com-,False,False,False,False,False,False
bining by majority voting three margin-based classiﬁers for protein,False,False,False,False,False,False
"fold recognition: the adaptive local hyperplane (ALH), the k-neigh-",False,False,False,False,False,False
borhood ALH and the SVM.,False,False,False,False,False,False
6.4.3. Neuroscience,False,False,False,False,False,False
"In the ﬁeld of Neurosciences, the machine learning approach is",False,False,False,False,False,False
gaining widespread acceptation. It is used for the classiﬁcation of,False,False,False,False,False,False
image data searching for predictive non-invasive biomarkers that,False,False,False,False,False,False
may allow early or prodromal diagnosis of a number of degenera-,False,False,False,False,False,False
tive diseases which have increasing impact in the society due to,False,False,False,False,False,False
12 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
the aging of populations around the world. Diverse MCS ap-,False,False,False,False,False,False
"proaches have been applied to structural MRI data, speciﬁcally",False,False,False,False,False,False
"for the classiﬁcation of Alzheimer disease patients, such as an",False,False,False,False,False,False
"RVM based two stage pipeline [45], variations of Adaboost [215],",False,False,False,False,False,False
hybridizations of kernel and Dendritic Computing approaches,False,False,False,False,False,False
[216]. Classiﬁer Ensembles have been applied to the classiﬁcation,False,False,False,False,False,False
"of fMRI data [217,218] and its visual decoding [219], which is the",False,False,False,False,False,False
reconstruction of the visual stimuli from the fMRI data.,False,False,False,False,False,False
6.5. Recommender systems,False,False,False,False,False,False
"Nowadays, recommender systems are the focus of intense re-",False,False,False,False,False,False
search [220]. They try to help consumers to select the product that,False,False,False,False,False,False
may be interesting for them based on their previous searches and,False,False,False,False,False,False
"transactions, but such systems are expanding beyond typical sales.",False,False,False,False,False,False
They are used to predict which mobile telephone subscribers are in,False,False,False,False,False,False
"risk of switching to another provider, or to advice conference orga-",False,False,False,False,False,False
nizers about assigning papers to peer reviewers [221]. Burke [222],False,False,False,False,False,False
proposed hybrid recommender systems combining two or more,False,False,False,False,False,False
recommendation techniques to improve performance avoiding,False,False,False,False,False,False
the drawbacks of an individual recommender. Similar observations,False,False,False,False,False,False
were conﬁrmed by Balabanovic et al. [223] and Pazzani [224] who,False,False,False,False,False,False
demonstrated that hybrid method recommentations improve col-,False,False,False,False,False,False
laborative and content-based approaches.,False,False,False,False,False,False
There are several interesting works which apply the hybrid and,False,False,False,False,False,False
combined approach to recommender systems. Jahrer and Töscher,False,False,False,False,False,False
[225] demonstrated the advantage of ensemble learning applied,False,False,False,False,False,False
to the combination of different collaborative ﬁltering algorithms,False,False,False,False,False,False
on the Netix Prize dataset. Porcel et al. [226] developed an hybrid,False,False,False,False,False,False
fuzzy recommender system to help disseminate information about,False,False,False,False,False,False
research resources in the ﬁeld of interest of a user. Claypool et al.,False,False,False,False,False,False
[227] performed a linear combination of the ratings obtained from,False,False,False,False,False,False
"individual recommender systems into one ﬁnal recommendation,",False,False,False,False,False,False
while Pazzani proposed to use a voting scheme [224]. Billsus and,False,False,False,False,False,False
Pazzani [228] selected the best recommendation on the basis of a,False,False,False,False,False,False
recommendation quality metric as the level of conﬁdence while,False,False,False,False,False,False
Tran and Cohen [229] preferred an individual which is the most,False,False,False,False,False,False
consistent with the previous ratings of the user. Kunaver et al.,False,False,False,False,False,False
[230] proposed Combined Collaborative Recommender based on,False,False,False,False,False,False
three different collaborative recommender techniques. Goksedef,False,False,False,False,False,False
and Gundoz-Oguducu [231] combined the results of several rec-,False,False,False,False,False,False
ommender techniques based on Web usage mining.,False,False,False,False,False,False
7. Final remarks,False,False,True,True,False,False
We have summarized the main research streams on multiple,False,False,False,False,False,False
"classiﬁer systems, also known in the literature as combined classi-",False,False,False,False,False,False
ﬁer or classiﬁer ensemble. Such hybrid systems are the focus of in-,False,False,False,False,False,False
"tense research recently, so fruitful that our review could not be",False,False,False,False,False,False
exhaustive. Key issues related to the problem under consideration,False,False,False,False,False,False
are classiﬁer diversity and methods of classiﬁer combination.,False,False,False,False,False,False
The diversity is believed to provide improved accuracy and clas-,False,False,False,False,False,False
siﬁer performance. Most works try to obtain maximum diversity,False,False,False,False,False,False
"by different means: introducing classiﬁer heterogeneity, boot-",False,False,False,False,False,False
"strapping the training data, randomizing feature selection, ran-",False,False,False,False,False,False
"domizing subspace projections, boosting the data weights, and",False,False,False,False,False,False
"many combinations of these ideas. Nowadays, the diversity",False,False,False,False,False,False
"hypothesis has not been fully proven, either theoretically or empir-",False,False,False,False,False,False
"ically. However, the fact is that MCSs show in most instances im-",False,False,False,False,False,False
"proved performance, resilience and robustness to high data",False,False,False,False,False,False
"dimensionality and diverse forms of noise, such as labeling noise.",False,False,False,False,False,False
The there are several propositions how to combine the classiﬁer,False,False,False,False,False,False
"outputs, what was presented in this work, nonetheless we point",False,False,False,False,False,False
out that classiﬁer combination is not the only way to produce hy-,False,False,False,False,False,False
brid classiﬁer systems. We envisage further possibilities of hybrid-,False,False,False,False,False,False
ization such as:,False,False,False,False,False,False
 Merging the raw data from different sources into one repository,False,False,False,False,False,False
and then train the classiﬁer.,False,False,False,False,False,False
" Merging the raw data and a prior expert knowledge (e.g., learn-",False,False,False,False,False,False
ing sets and human expert rules to improve rules on the basis of,False,False,False,False,False,False
incoming data).,False,False,False,False,False,False
 Merging a prior expert knowledge and classiﬁcation models,False,False,False,False,False,False
returned by machine learning procedures.,False,False,False,False,False,False
For such a problem we have to take into consideration issues re-,False,False,False,False,False,False
"lated to data privacy, computational and memory efﬁciency.",False,False,False,False,False,False
Acknowledgments,False,False,False,False,False,False
We would like to thank the anonymous reviewers for their dil-,False,False,False,False,False,False
igent work and efﬁcient efforts. We are also grateful to the Editor-,False,False,False,False,False,False
"in-Chief, Prof. Belur V. Dasarathy, who encouraged us to write this",False,False,False,False,False,False
survey for this prestigious journal.,False,False,False,False,False,False
MichałWoz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak was supported by The Polish National Science,False,False,False,False,False,False
Centre under the Grant No. N519 576638 which is being realized,False,False,False,False,False,False
in years 2010–2013.,False,False,False,False,False,False
References,False,False,False,True,False,False
"[1] J. Neumann, The Computer and the Brain, Yale University Press, New Haven,",False,False,False,False,False,False
"CT, USA, 1958",True,False,False,True,False,False
.,False,True,False,True,False,False
"[2] A. Newell, Intellectual issues in the history of artiﬁcial intelligence, in: F.",False,False,False,False,False,False
"Machlup, U. Mansﬁeld (Eds.), The Study of Information: Interdisciplinary",False,False,False,False,False,False
"Messages, John Wiley & Sons Inc., New York, NY, USA, 1983, pp. 187–294",False,False,False,False,False,False
.,False,True,False,True,False,False
"[3] D. Wolpert, The supervised learning no-free-lunch theorems, in: Proceedings",False,False,False,False,False,False
of the 6th Online World Conference on Soft Computing in Industrial,False,False,False,False,False,False
"Applications, 2001, pp. 25–42.",False,False,False,False,False,False
"[4] C.K. Chow, Statistical independence and threshold functions, IEEE",False,False,False,False,False,False
Transactions on Electronic Computers EC-14 (1) (1965) 66–68,False,False,False,False,False,False
.,False,True,False,True,False,False
"[5] L. Shapley, B. Grofman, Optimizing group judgmental accuracy in the",False,False,False,False,False,False
"presence of interdependencies, Public Choice 43 (3) (1984) 329–333",False,False,False,False,False,False
.,False,True,False,True,False,False
"[6] B.V. Dasarathy, B.V. Sheela, A composite classiﬁer system design: concepts",False,False,False,False,False,False
"and methodology, Proceedings of the IEEE 67 (5) (1979) 708–713",False,False,False,False,False,False
.,False,True,False,True,False,False
"[7] L. Rastrigin, R.H. Erenstein, Method of Collective Recognition, Energoizdat,",False,False,False,False,False,False
"Moscow, 1981.",False,False,False,False,False,False
"[8] L. Hansen, P. Salamon, Neural network ensembles, IEEE Transactions on",False,False,False,False,False,False
"Pattern Analysis and Machine Intelligence 12 (10) (1990) 993–1001, http://",False,False,False,False,False,False
dx.doi.org/10.1109/34.58871.,False,False,False,False,False,False
"[9] L. Xu, A. Krzyzak, C. Suen, Methods of combining multiple classiﬁers and their",False,False,False,False,False,False
"applications to handwriting recognition, IEEE Transactions on Systems, Man",False,False,False,False,False,False
and Cybernetics 22 (3) (1992) 418–435,False,False,False,False,False,False
.,False,True,False,True,False,False
"[10] K. Tumer, J. Ghosh, Analysis of decision boundaries in linearly combined",False,False,False,False,False,False
"neural classiﬁers, Pattern Recognition 29 (2) (1996) 341–348",False,False,False,False,False,False
.,False,True,False,True,False,False
"[11] T. Ho, J.J. Hull, S. Srihari, Decision combination in multiple classiﬁer systems,",False,False,False,False,False,False
IEEE Transactions on Pattern Analysis and Machine Intelligence 16 (1) (1994),False,False,False,False,False,False
66–75,False,False,False,False,False,False
.,False,True,False,True,False,False
"[12] L. Breiman, Bagging predictors, Machine Learning 24 (2) (1996) 123–140.",False,False,False,False,False,False
"[13] R. Schapire, The strength of weak learnability, Machine Learning 5 (2) (1990)",False,False,False,False,False,False
197–227,False,False,False,False,False,False
.,False,True,False,True,False,False
"[14] Y. Freund, Boosting a weak learning algorithm by majority, Information",False,False,False,False,False,False
Computing 121 (2) (1995) 256–285,False,False,False,False,False,False
.,False,True,False,True,False,False
"[15] M. Kearns, U. Vazirani, An Introduction to Computational Learning Theory,",False,False,False,True,False,False
"MIT Press, Cambridge, MA, USA, 1994",False,False,False,False,False,False
.,False,True,False,True,False,False
"[16] D. Angluin, Queries and concept learning, Machine Learning 2 (4) (1988) 319–",False,False,False,False,False,False
342,False,False,False,False,False,False
.,False,True,False,True,False,False
"[17] A. Jain, R. Duin, M. Jianchang, Statistical pattern recognition: a review, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis and Machine Intelligence 22 (1) (2000) 4–,False,False,False,False,False,False
37,False,False,False,False,False,False
.,False,True,False,True,False,False
"[18] N. Oza, K. Tumer, Classiﬁer ensembles: select real-world applications,",False,False,False,False,False,False
Information Fusion 9 (1) (2008) 4–20,False,False,False,False,False,False
.,False,True,False,True,False,False
"[19] R. Polikar, Ensemble based systems in decision making, IEEE Circuits and",False,False,False,False,False,False
Systems Magazine 6 (3) (2006) 21–45,False,False,False,False,False,False
.,False,True,False,True,False,False
"[20] R. Polikar, Ensemble learning, Scholarpedia 3 (12) (2008) 2776.",False,False,False,False,False,False
"[21] L. Rokach, Taxonomy for characterizing ensemble methods in classiﬁcation",False,False,False,False,False,False
"tasks: a review and annotated bibliography, Computational Statistics and",False,False,False,False,False,False
Data Analysis 53 (12) (2009) 4046–4072,False,False,False,False,False,False
.,False,True,False,True,False,False
"[22] L. Kuncheva, Combining Pattern Classiﬁers: Methods and Algorithms, Wiley-",False,False,False,False,False,False
"Interscience, 2004",False,False,False,False,False,False
.,False,True,False,True,False,False
"[23] L. Rokach, Pattern Classiﬁcation Using Ensemble Methods, Series in Machine",False,False,False,False,False,False
"Perception and Artiﬁcial Intelligence, World Scientiﬁc, 2010",False,False,False,False,False,False
.,False,True,False,True,False,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
13,False,False,False,False,False,False
"[24] G. Seni, J. Elder, Ensemble Methods in Data Mining: Improving Accuracy",False,False,False,False,False,False
"Through Combining Predictions, Morgan and Claypool Publishers, 2010",False,False,False,False,False,False
.,False,True,False,True,False,False
"[25] B. Baruque, E. Corchado, Fusion Methods for Unsupervised Learning",False,False,False,False,False,False
"Ensembles, Springer Verlag New York, Inc., 2011",False,False,False,False,False,False
.,False,True,False,True,False,False
"[26] R. Duda, P. Hart, D. Stork, Pattern Classiﬁcation, second ed., Wiley, New York,",False,False,False,False,False,False
2001,False,False,False,False,False,False
.,False,True,False,True,False,False
"[27] E. Alpaydin, Introduction to Machine Learning, second ed., The MIT Press,",False,False,False,True,False,False
2010,False,False,False,False,False,False
.,False,True,False,True,False,False
"[28] C. Bishop, Pattern Recognition and Machine Learning (Information Science",False,False,False,False,False,False
"and Statistics), Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006",False,False,False,False,False,False
.,False,True,False,True,False,False
"[29] T. Dietterich, Ensemble methods in machine learning, in: Multiple Classiﬁer",False,False,False,False,False,False
"Systems, Lecture Notes in Computer Science, vol. 1857, Springer, Berlin,",False,False,False,False,False,False
"Heidelberg, 2000, pp. 1–15",False,False,False,False,False,False
.,False,True,False,True,False,False
"[30] G. Marcialis, F. Roli, Fusion of face recognition algorithms for video-based",False,False,False,False,False,False
"surveillance systems, in: G.L. Foresti, C. Regazzoni, P. Varshney (Eds.), 2003,",False,False,False,False,False,False
pp. 235–250.,False,False,False,False,False,False
"[31] S. Hashem, Optimal linear combinations of neural networks, Neural Networks",False,False,False,False,False,False
10 (4) (1997) 599–614,False,False,False,False,False,False
.,False,True,False,True,False,False
"[32] R. Clemen, Combining forecasts: a review and annotated bibliography,",False,False,False,False,False,False
International Journal of Forecasting 5 (4) (1989) 559–583,False,False,False,False,False,False
.,False,True,False,True,False,False
"[33] J. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann Series in",False,False,False,False,False,False
"Machine Learning, Morgan Kaufman Publishers, 1993",False,False,False,False,False,False
.,False,True,False,True,False,False
"[34] T. Wilk, M. Wozniak, Complexity and multithreaded implementation analysis",False,False,False,False,False,False
"of one class-classiﬁers fuzzy combiner, in: E. Corchado, M. Kurzynski, M.",False,False,False,False,False,False
"Wozniak (Eds.), Hybrid Artiﬁcial Intelligent Systems, Lecture Notes in",False,False,False,False,False,False
"Computer Science, vol. 6679, Springer, Berlin/Heidelberg, 2011, pp. 237–244",False,False,False,False,False,False
.,False,True,False,True,False,False
"[35] T. Kacprzak, K. Walkowiak, M. Wozniak, Optimization of overlay distributed",False,False,False,False,False,False
"computing systems for multiple classiﬁer system – heuristic approach, Logic",False,False,False,False,False,False
"Journal of IGPL, doi:10.1093/jigpal/jzr020.",False,False,False,False,False,False
"[36] K. Walkowiak, Anycasting in connection-oriented computer networks:",False,False,False,False,False,False
"models, algorithms and results, International Journal of Applied",False,False,False,False,False,False
Mathematics and Computer Sciences 20 (1) (2010) 207–220,False,False,False,False,False,False
.,False,True,False,True,False,False
"[37] R. Agrawal, R. Srikant, Privacy-preserving data mining, SIGMOD Records 29",False,False,False,False,False,False
(2) (2000) 439–450,False,False,False,False,False,False
.,False,True,False,True,False,False
"[38] G. Giacinto, F. Roli, G. Fumera, Design of effective multiple classiﬁer systems",False,False,False,False,False,False
"by clustering of classiﬁers, in: Proceedings of the 15th International",False,False,False,False,False,False
"Conference on Pattern Recognition, 2000, vol. 2, 2000, pp. 160–163.",False,False,False,False,False,False
"[39] T. Ho, Complexity of classiﬁcation problems and comparative advantages of",False,False,False,False,False,False
"combined classiﬁers, in: Proceedings of the First International Workshop on",False,False,False,False,False,False
"Multiple Classiﬁer Systems, MCS ’00, Springer-Verlag, London, UK, 2000, pp.",False,False,False,False,False,False
97–106,False,False,False,False,False,False
.,False,True,False,True,False,False
"[40] F. Roli, G. Giacinto, Design of Multiple Classiﬁer Systems, World Scientiﬁc",False,False,False,False,False,False
"Publishing, 2002",False,False,False,False,False,False
.,False,True,False,True,False,False
"[41] L. Lam, Classiﬁer combinations: implementations and theoretical issues, in:",False,False,False,False,False,False
Proceedings of the First International Workshop on Multiple Classiﬁer,False,False,False,False,False,False
"Systems, MCS ’00, Springer-Verlag, London, UK, 2000, pp. 77–86",False,False,False,False,False,False
.,False,True,False,True,False,False
"[42] A.F.R. Rahman, M.C. Fairhurst, Serial combination of multiple experts: a",False,False,False,False,False,False
"uniﬁed evaluation, Pattern Analysis and Applications 2 (1999) 292–311",False,False,False,False,False,False
.,False,True,False,True,False,False
"[43] G. Fumera, I. Pillai, F. Roli, A two-stage classiﬁer with reject option for text",False,False,False,False,False,False
"categorisation, 5th International Workshop on Statistical Techniques in",False,False,False,False,False,False
"Pattern Recognition (SPR 2004), vol. 3138, Springer, Lisbon, Portugal, 2004,",False,False,False,False,False,False
pp. 771–779,False,False,False,False,False,False
.,False,True,False,True,False,False
"[44] P. Bartlett, M. Wegkamp, Classiﬁcation with a reject option using a hinge loss,",False,False,False,False,False,False
Journal of Machine Learning Research 9 (2008) 1823–1840,False,False,False,False,False,False
.,False,True,False,True,False,False
"[45] M. Termenon, M. Graña, A two stage sequential ensemble applied to the",False,False,False,False,False,False
"classiﬁcation of alzheimer’s disease based on MRI features, Neural Processing",False,False,False,False,False,False
Letters 35 (1) (2012) 1–12,False,False,False,False,False,False
.,False,True,False,True,False,False
"[46] P. Clark, T. Niblett, The CN2 induction algorithm, Machine Learning 3 (4)",False,False,False,False,False,False
(1989) 261–283,False,False,False,False,False,False
.,False,True,False,True,False,False
"[47] R. Rivest, Learning decision lists, Machine Learning 2 (3) (1987) 229–246.",False,False,False,False,False,False
"[48] Y. Freund, R. Schapire, A decision-theoretic generalization of on-line learning",False,False,False,False,False,False
"and an application to boosting, Journal of Computer and System Sciences 55",False,False,False,False,False,False
"(1) (1997) 119–139, http://dx.doi.org/10.1006/jcss.1997.1504.",False,False,False,False,False,False
"[49] X. Wu, V. Kumar, J.R. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G.J. McLachlan, A.",False,False,False,False,False,False
"Ng, B. Liu, P.S. Yu, Z.-H. Zhou, M. Steinbach, D.J. Hand, D. Steinberg, Top 10",False,False,False,False,False,False
"algorithms in data mining, Knowledge and Information Systems 14 (1) (2008)",False,False,False,False,False,False
"1–37, http://dx.doi.org/10.1007/s10115-007-0114-2.",False,False,False,False,False,False
"[50] R. Schapire, The strength of weak learnability, Machine Learning 5 (2) (1990)",False,False,False,False,False,False
"197–227, http://dx.doi.org/10.1023/A:1022648800760.",False,False,False,False,False,False
"[51] J. Kivinen, M.K. Warmuth, Boosting as entropy projection, in: Proceedings of",False,False,False,False,False,False
"the Twelfth Annual Conference on Computational Learning Theory, 1999.",False,False,False,False,False,False
<http://dl.acm.org/citation.cfm?id=307424>.,False,False,False,False,False,False
"[52] D. Partridge, W. Krzanowski, Software diversity: practical statistics for its",False,False,False,False,False,False
"measurement and exploitation, Information and Software Technology 39 (10)",False,False,False,False,False,False
(1997) 707–717,False,False,False,False,False,False
.,False,True,False,True,False,False
"[53] G. Brown, L. Kuncheva, ‘‘good’’ And ‘‘bad’’ diversity in majority vote",False,False,False,False,False,False
"ensembles, in: Proceedings MCS 2010, pp. 124–133.",False,False,False,False,False,False
"[54] M. Smetek, B. Trawinski, Selection of heterogeneous fuzzy model ensembles",False,False,False,False,False,False
"using self-adaptive genetic algorithms, New Generation Computing 29 (2011)",False,False,False,False,False,False
309–327,False,False,False,False,False,False
.,False,True,False,True,False,False
"[55] A.J.C. Sharkey, N. Sharkey, Combining diverse neural nets, Knowledge",False,False,False,False,False,False
Engineering Review 12 (3) (1997) 231–247,False,False,False,False,False,False
.,False,True,False,True,False,False
"[56] A. Krogh, J. Vedelsby, Neural network ensembles, cross validation, and active",False,False,False,False,False,False
"learning, Advances in Neural Information Processing Systems 7 (1995) 231–",False,False,False,False,False,False
238,False,False,False,False,False,False
.,False,True,False,True,False,False
"[57] G. Zenobi, P. Cunningham, Using diversity in preparing ensembles of",False,False,False,False,False,False
classiﬁers based on different feature subsets to minimize generalization,False,False,False,False,False,False
"error, Machine Learning: ECML 2001 (2001) 576–587",False,False,False,False,False,False
.,False,True,False,True,False,False
"[58] G. Brown, J. Wyatt, R. Harris, X. Yao, Diversity creation methods: a survey and",False,False,False,False,False,False
"categorisation, Information Fusion 6 (1) (2005) 5–20",False,False,False,False,False,False
.,False,True,False,True,False,False
"[59] N. Ueda, R. Nakano, Generalization error of ensemble estimators, in:",False,False,False,False,False,False
"Proceedings of IEEE International Conference on Neural Networks,",False,False,False,False,False,False
"Washington, USA, 1996, pp. 90–95.",False,False,False,False,False,False
"[60] G. Brown, J. Wyatt, P. Tin",False,False,False,False,False,False
ˇ,False,False,False,False,False,False
"o, Managing diversity in regression ensembles,",False,False,False,False,False,False
Journal of Machine Learning Research 6 (2005) 1621–1650,False,False,False,False,False,False
.,False,True,False,True,False,False
"[61] L. Kuncheva, C. Whitaker, C. Shipp, R. Duin, Limits on the majority vote",False,False,False,False,False,False
"accuracy in classiﬁer fusion, Pattern Analysis and Applications 6 (2003) 22–",False,False,False,False,False,False
31,False,False,False,False,False,False
.,False,True,False,True,False,False
"[62] Y. Bi, The impact of diversity on the accuracy of evidential classiﬁer",False,False,False,False,False,False
"ensembles, International Journal of Approximate Reasoning 53 (4) (2012)",False,False,False,False,False,False
584–607,False,False,False,False,False,False
.,False,True,False,True,False,False
"[63] D. Margineantu, T. Dietterich, Pruning adaptive boosting, in: Proceedings of",False,False,False,False,False,False
"the Fourteenth International Conference on Machine Learning, ICML ’97,",False,False,False,False,False,False
"Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997, pp. 211–218",False,False,False,False,False,False
.,False,True,False,True,False,False
"[64] D. Skalak, The sources of increased accuracy for two proposed boosting",False,False,False,False,False,False
"algorithms, in: Proceedings of the American Association for Arti Intelligence,",False,False,False,False,False,False
"AAAI-96, Integrating Multiple Learned Models Workshop, 1996, pp. 120–125.",False,False,False,False,False,False
"[65] G. Giacinto, F. Roli, Design of effective neural network ensembles for image",False,False,False,False,False,False
"classiﬁcation purposes, Image Vision Computing 19 (9-10) (2001) 699–707",False,False,False,False,False,False
.,False,True,False,True,False,False
"[66] R. Kohavi, D. Wolpert, Bias plus variance decomposition for zero-one loss",False,False,False,False,False,False
"functions, in: ICML-96, 1996.",False,False,False,False,False,False
"[67] J. Fleiss, J. Cuzick, The reliability of dichotomous judgments: unequal",False,False,False,False,False,False
"numbers of judgments per subject, Applied Psychological Measurement 4",False,False,False,False,False,False
(3) (1979) 537–542,False,False,False,False,False,False
.,False,True,False,True,False,False
"[68] P. Cunningham, J. Carney, Diversity versus quality in classiﬁcation ensembles",False,False,False,False,False,False
"based on feature selection, in: Proceedings of the 11th European Conference",False,False,False,False,False,False
"on Machine Learning, ECML ’00, Springer-Verlag, London, UK, 2000, pp. 109–",False,False,False,False,False,False
116,False,False,False,False,False,False
.,False,True,False,True,False,False
"[69] C. Shipp, L. Kuncheva, Relationships between combination methods and",False,False,False,False,False,False
"measures of diversity in combining classiﬁers, Information Fusion 3 (2)",False,False,False,False,False,False
(2002) 135–148,False,False,False,False,False,False
.,False,True,False,True,False,False
"[70] E.K. Tang, P.N. Suganthan, X. Yao, An analysis of diversity measures, Machine",False,False,False,False,False,False
Learning 65 (1) (2006) 247–271,False,False,False,False,False,False
.,False,True,False,True,False,False
"[71] G. Martinez-Mu/ noz, D. Hern/’andez-Lobato, A. Suarez, An analysis of",False,False,False,False,False,False
"ensemble pruning techniques based on ordered aggregation, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis and Machine Intelligence 31 (2) (2009),False,False,False,False,False,False
245–259,False,False,False,False,False,False
.,False,True,False,True,False,False
"[72] D. Ruta, B. Gabrys, Classiﬁer selection for majority voting, Information Fusion",False,False,False,False,False,False
6 (1) (2005) 63–81,False,False,False,False,False,False
.,False,True,False,True,False,False
"[73] R. Banﬁeld, L. Hall, K. Bowyer, W. Kegelmeyer, Ensemble diversity measures",False,False,False,False,False,False
"and their application to thinning, Information Fusion 6 (1) (2005) 49–62",False,False,False,False,False,False
.,False,True,False,True,False,False
"[74] Z.-H. Zhou, J. Wu, W. Tang, Ensembling neural networks: many could be",False,False,False,False,False,False
"better than all, Artiﬁcial Intelligence 137 (1-2) (2002) 239–263",False,False,False,False,False,False
.,False,True,False,True,False,False
"[75] B. Gabrys, D. Ruta, Genetic algorithms in classiﬁer fusion, Applied Soft",False,False,False,False,False,False
Computing 6 (4) (2006) 337–347,False,False,False,False,False,False
.,False,True,False,True,False,False
"[76] I. Partalas, G. Tsoumakas, I. Vlahavas, Pruning an ensemble of classiﬁers via",False,False,False,False,False,False
"reinforcement learning, Neurocomputing 72 (7–9) (2009) 1900–1909",False,False,False,False,False,False
.,False,True,False,True,False,False
"[77] Q. Dai, A competitive ensemble pruning approach based on cross-validation",False,False,False,False,False,False
"technique, Knowledge-Based Systems (0) (2012), http://dx.doi.org/10.1016/",False,False,False,False,False,False
j.knosys.2012.08.024.,False,False,False,False,False,False
"[78] Y. Peng, Q. Huang, P. Jiang, J. Jiang, Cost-sensitive ensemble of support vector",False,False,False,False,False,False
machines for effective detection of microcalciﬁcation in breast cancer,False,False,False,False,False,False
"diagnosis, in: L. Wang, Y. Jin (Eds.), Fuzzy Systems and Knowledge",False,False,False,False,False,False
"Discovery, Lecture Notes in Computer Science, vol. 3614, Springer, Berlin/",False,False,False,False,False,False
"Heidelberg, 2005, pp. 483–493",False,False,False,False,False,False
.,False,True,False,True,False,False
"[79] K. Jackowski, B. Krawczyk, M. Woniak, Cost-sensitive splitting and selection",False,False,False,False,False,False
"method for medical decision support system, in: H. Yin, J.A. Costa, G. Barreto",False,False,False,False,False,False
"(Eds.), Intelligent Data Engineering and Automated Learning – IDEAL 2012,",False,False,False,False,False,False
"Lecture Notes in Computer Science, vol. 7435, Springer, Berlin Heidelberg,",False,False,False,False,False,False
"2012, pp. 850–857",False,False,False,False,False,False
.,False,True,False,True,False,False
"[80] W. Du, Z. Zhan, Building decision tree classiﬁer on private data, in:",False,False,False,False,False,False
"Proceedings of the IEEE International Conference on Privacy, Security and",False,False,False,False,False,False
"Data Mining – Volume 14, CRPIT ’14, Australian Computer Society, Inc.,",False,False,False,False,False,False
"Darlinghurst, Australia, 2002, pp. 1–8.",False,False,False,False,False,False
"[81] B. Krawczyk, M. Wozniak, Privacy preserving models of k-NN algorithm, in:",False,False,False,False,False,False
"R. Burduk, M. Kurzynski, M. Wozniak, A. Zolnierek (Eds.), Computer",False,False,False,False,False,False
"Recognition Systems 4, Advances in Intelligent and Soft Computing, vol. 95,",False,False,False,False,False,False
"Springer, Berlin/Heidelberg, 2011, pp. 207–217",False,False,False,False,False,False
.,False,True,False,True,False,False
"[82] Y. Lindell, B. Pinkas, Secure multiparty computation for privacy-preserving",False,False,False,False,False,False
"data mining, IACR Cryptology ePrint Archive 2008 (2008) 197",False,False,False,False,False,False
.,False,True,False,True,False,False
"[83] K. Walkowiak, S. Sztajer, M. Wozniak, Decentralized distributed computing",False,False,False,False,False,False
system for privacy-preserving combined classiﬁers – modeling and,False,False,False,False,False,False
"optimization, in: B. Murgante, O. Gervasi, A. Iglesias, D. Taniar, B. Apduhan",False,False,False,False,False,False
"(Eds.), Computational Science and Its Applications – ICCSA 2011, Lecture",False,False,False,False,False,False
"Notes in Computer Science, Vol. 6782, Springer, Berlin/Heidelberg, 2011, pp.",False,False,False,False,False,False
512–525,False,False,False,False,False,False
.,False,True,False,True,False,False
"[84] A. Pavlo, E. Paulson, A. Rasin, D. Abadi, D. DeWitt, S. Madden, M. Stonebraker,",False,False,False,False,False,False
"A comparison of approaches to large-scale data analysis, in: Proceedings of",False,False,False,False,False,False
"the 2009 ACM SIGMOD International Conference on Management of Data,",False,False,False,False,False,False
"SIGMOD ’09, ACM, New York, NY, USA, 2009, pp. 165–178",False,False,False,False,False,False
.,False,True,False,True,False,False
14 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
"[85] R.E. Schapire, The boosting approach to machine learning: an overview, in:",False,False,False,False,False,False
"MSRI Workshop on Nonlinear Estimation and Classiﬁcation, Berkeley, CA,",False,False,False,False,False,False
"USA, 2001.",True,False,False,True,False,False
"[86] T. Ho, Random decision forests, in: Proceedings of the Third International",False,False,False,False,False,False
"Conference on Document Analysis and Recognition (Volume 1)–Volume 1,",False,False,False,False,False,False
"ICDAR ’95, IEEE Computer Society, Washington, DC, USA, 1995, pp. 278–.",False,False,False,False,False,False
"[87] T. Ho, The random subspace method for constructing decision forests, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis and Machine Intelligence 20 (1998) 832–,False,False,False,False,False,False
844,False,False,False,False,False,False
.,False,True,False,True,False,False
"[88] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 5–32.",False,False,False,False,False,False
"[89] M. Skurichina, R. Duin, Bagging, boosting and the random subspace method",False,False,False,False,False,False
"for linear classiﬁers, Pattern Analysis and Applications 5 (2) (2002) 121–135",False,False,False,False,False,False
.,False,True,False,True,False,False
"[90] G. Tremblay, R. Sabourin, P. Maupin, Optimizing nearest neighbour in random",False,False,False,False,False,False
"subspaces using a multi-objective genetic algorithm, in: Proceedings of the",False,False,False,False,False,False
"Pattern Recognition, 17th International Conference on (ICPR’04) Volume 1–",False,False,False,False,False,False
"Volume 01, ICPR ’04, IEEE Computer Society, Washington, DC, USA, 2004, pp.",False,False,False,False,False,False
208–.,False,False,False,False,False,False
"[91] S. Bay, Nearest neighbor classiﬁcation from multiple feature subsets,",False,False,False,False,False,False
Intelligent Data Analysis 3 (3) (1999) 191–209,False,False,False,False,False,False
.,False,True,False,True,False,False
"[92] L. Nanni, Letters: Experimental comparison of one-class classiﬁers for online",False,False,False,False,False,False
"signature veriﬁcation, Neurocomputing 69 (7–9) (2006) 869–873",False,False,False,False,False,False
.,False,True,False,True,False,False
"[93] D. Tao, X. Tang, X. Li, X. Wu, Asymmetric bagging and random subspace for",False,False,False,False,False,False
"support vector machines-based relevance feedback in image retrieval, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis Machine Intelligence 28 (7) (2006) 1088–,False,False,False,False,False,False
1099,False,False,False,False,False,False
.,False,True,False,True,False,False
"[94] K. Ting, J. Wells, S. Tan, S. Teng, G. Webb, Feature-subspace aggregating:",False,False,False,False,False,False
"ensembles for stable and unstable learners, Machine Learning 82 (2011) 375–",False,False,False,False,False,False
397,False,False,False,False,False,False
.,False,True,False,True,False,False
"[95] R. Bryll, R. Gutierrez-Osuna, F. Quek, Attribute bagging: improving accuracy",False,False,False,False,False,False
"of classiﬁer ensembles by using random feature subsets, Pattern Recognition",False,False,False,False,False,False
36 (6) (2003) 1291–1302,False,False,False,False,False,False
.,False,True,False,True,False,False
"[96] Y. Baram, Partial classiﬁcation: the beneﬁt of deferred decision, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis and Machine Intelligence 20 (8) (1998),False,False,False,False,False,False
769–776,False,False,False,False,False,False
.,False,True,False,True,False,False
"[97] L. Cordella, P. Foggia, C. Sansone, F. Tortorella, M. Vento, A cascaded multiple",False,False,False,False,False,False
"expert system for veriﬁcation, in: Multiple Classiﬁer Systems, Lecture Notes",False,False,False,False,False,False
"in Computer Science, vol. 1857, Springer, Berlin/Heidelberg, 2000, pp. 330–",False,False,False,False,False,False
339,False,False,False,False,False,False
.,False,True,False,True,False,False
"[98] K. Goebel, W. Yan, Choosing classiﬁers for decision fusion, in: Proceedings of",False,False,False,False,False,False
"the Seventh International Conference on Information Fusion, 2004, pp. 563–",False,False,False,False,False,False
568.,False,False,False,False,False,False
"[99] B. Baruque, S. Porras, E. Corchado, Hybrid classiﬁcation ensemble using",False,False,False,False,False,False
"topology-preserving clustering, New Generation Computing 29 (2011) 329–",False,False,False,False,False,False
344,False,False,False,False,False,False
.,False,True,False,True,False,False
"[100] L. Kuncheva, Clustering-and-selection model for classiﬁer combination, in:",False,False,False,False,False,False
Proceedings of the Fourth International Conference on Knowledge-Based,False,False,False,False,False,False
"Intelligent Engineering Systems and Allied Technologies, 2000, vol. 1, 2000,",False,False,False,False,False,False
pp. 185–188.,False,False,False,False,False,False
"[101] K. Jackowski, M. Wozniak, Algorithm of designing compound recognition",False,False,False,False,False,False
system on the basis of combining classiﬁers with simultaneous splitting,False,False,False,False,False,False
"feature space into competence areas, Pattern Analysis and Applications 12 (4)",False,False,False,False,False,False
(2009) 415–425,False,False,False,False,False,False
.,False,True,False,True,False,False
"[102] M. Wozniak, B. Krawczyk, Combined classiﬁer based on feature space",False,False,False,False,False,False
"partitioning, International Journal of Applied Mathematics and Computer",False,False,False,False,False,False
Sciences 22 (4) (2012) 855–866.,False,False,False,False,False,False
"[103] H. Lee, C. Chen, J. Chen, Y. Jou, An efﬁcient fuzzy classiﬁer with feature",False,False,False,False,False,False
"selection based on fuzzy entropy, IEEE Transactions on Systems, Man, and",False,False,False,False,False,False
"Cybernetics, Part B: Cybernetics 31 (3) (2001) 426–432",False,False,False,False,False,False
.,False,True,False,True,False,False
"[104] J. Hong, J. Min, U. Cho, S. Cho, Fingerprint classiﬁcation using one-vs-all",False,False,False,False,False,False
"support vector machines dynamically ordered with naïve bayes classiﬁers,",False,False,False,False,False,False
Pattern Recognition 41 (2008) 662–671,False,False,False,False,False,False
.,False,True,False,True,False,False
"[105] A.R. Ko, R. Sabourin, A. Britto, From dynamic classiﬁer selection to dynamic",False,False,False,False,False,False
"ensemble selection, Pattern Recognition 41 (5) (2008) 1735–1748",False,False,False,False,False,False
.,False,True,False,True,False,False
"[106] L. Didaci, G. Giacinto, F. Roli, G. Marcialis, A study on the performances of",False,False,False,False,False,False
"dynamic classiﬁer selection based on local accuracy estimation, Pattern",False,False,False,False,False,False
Recognition 38 (11) (2005) 2188–2191,False,False,False,False,False,False
.,False,True,False,True,False,False
"[107] G. Giacinto, F. Roli, Dynamic classiﬁer selection based on multiple classiﬁer",False,False,False,False,False,False
"behavior, Pattern Recognition 34 (9) (2001) 1879–1881",False,False,False,False,False,False
.,False,True,False,True,False,False
"[108] M. de Souto, R. Soares, A. Santana, A. Canuto, Empirical comparison of",False,False,False,False,False,False
dynamic classiﬁer selection methods based on diversity and accuracy for,False,False,False,False,False,False
"building ensembles, in: IJCNN 2008, IEEE International Joint Conference on",False,False,False,False,False,False
"Neural Networks, 2008, IEEE World Congress on Computational Intelligence,",False,False,False,False,False,False
"2008, pp. 1480–1487.",False,False,False,False,False,False
"[109] T. Woloszynski, M. Kurzynski, A probabilistic model of classiﬁer competence",False,False,False,False,False,False
"for dynamic ensemble selection, Pattern Recognition 44 (1011) (2011) 2656–",False,False,False,False,False,False
2668,False,False,False,False,False,False
.,False,True,False,True,False,False
"[110] T. Woloszynski, M. Kurzynski, P. Podsiadlo, G. Stachowiak, A measure of",False,False,False,False,False,False
"competence based on random classiﬁcation for dynamic ensemble selection,",False,False,False,False,False,False
Information Fusion 13 (3) (2012) 207–213,False,False,False,False,False,False
.,False,True,False,True,False,False
"[111] W. Street, Y. Kim, A streaming ensemble algorithm (sea) for large-scale",False,False,False,False,False,False
"classiﬁcation, in: Proceedings of the Seventh ACM SIGKDD International",False,False,False,False,False,False
"Conference on Knowledge Discovery and Data Mining, KDD ’01, ACM, New",False,False,False,False,False,False
"York, NY, USA, 2001, pp. 377–382",False,False,False,False,False,False
.,False,True,False,True,False,False
"[112] H. Wang, W. Fan, P. Yu, J. Han, Mining concept-drifting data streams using",False,False,False,False,False,False
"ensemble classiﬁers, in: Proceedings of the Ninth ACM SIGKDD International",False,False,False,False,False,False
"Conference on Knowledge Discovery and Data Mining, KDD ’03, ACM, New",False,False,False,False,False,False
"York, NY, USA, 2003, pp. 226–235",False,False,False,False,False,False
.,False,True,False,True,False,False
"[113] Y. Zhang, X. Jin, An automatic construction and organization strategy for",False,False,False,False,False,False
"ensemble learning on data streams, SIGMOD Record 35 (3) (2006) 28–33",False,False,False,False,False,False
.,False,True,False,True,False,False
"[114] J. Kolter, M. Maloof, Dynamic weighted majority: a new ensemble method for",False,False,False,False,False,False
"tracking concept drift, in: ICDM 2003, Third IEEE International Conference on",False,False,False,False,False,False
"Data Mining, 2003, 2003, pp. 123–130.",False,False,False,False,False,False
"[115] A. Tsymbal, M. Pechenizkiy, P. Cunningham, S. Puuronen, Dynamic",False,False,False,False,False,False
"integration of classiﬁers for handling concept drift, Information Fusion 9",False,False,False,False,False,False
(1) (2008) 56–68,False,False,False,False,False,False
.,False,True,False,True,False,False
"[116] X. Zhu, X. Wu, Y. Yang, Effective classiﬁcation of noisy data streams with",False,False,False,False,False,False
"attribute-oriented dynamic classiﬁer selection, Knowledge Information",False,False,False,False,False,False
Systems 9 (3) (2006) 339–363,False,False,False,False,False,False
.,False,True,False,True,False,False
"[117] D. Tax, R. Duin, Using two-class classiﬁers for multiclass classiﬁcation, in:",False,False,False,False,False,False
"Proceedings of the 16th International Conference on Pattern Recognition,",False,False,False,False,False,False
"2002, vol. 2, 2002, pp. 124 –127.",False,False,False,False,False,False
"[118] T. Dietterich, G. Bakiri, Solving multiclass learning problems via error-",False,False,False,False,False,False
"correcting output codes, Journal of Artiﬁcial Intelligence Research 2 (1995)",False,False,False,False,False,False
263–286,False,False,False,False,False,False
.,False,True,False,True,False,False
"[119] K. Duan, S. Keerthi, W. Chu, S. Shevade, A. Poo, Multi-category classiﬁcation",False,False,False,False,False,False
"by soft-max combination of binary classiﬁers, in: Proceedings of the 4th",False,False,False,False,False,False
"International Conference on Multiple Classiﬁer Systems, MCS’03, Springer-",False,False,False,False,False,False
"Verlag, Berlin, Heidelberg, 2003, pp. 125–134",False,False,False,False,False,False
.,False,True,False,True,False,False
"[120] A. Passerini, M. Pontil, P. Frasconi, New results on error correcting output",False,False,False,False,False,False
"codes of kernel machines, IEEE Transactions on Neural Networks 15 (1)",False,False,False,False,False,False
(2004) 45–54,False,False,False,False,False,False
.,False,True,False,True,False,False
"[121] T. Wu, C. Lin, R. Weng, Probability estimates for multi-class classiﬁcation by",False,False,False,False,False,False
"pairwise coupling, Journal of Machine Learning Research 5 (2004) 975–1005",False,False,False,False,False,False
.,False,True,False,True,False,False
"[122] J. Friedman, Another Approach to Polychotomous Classiﬁcation, Tech. rep.,",False,False,False,False,False,False
"Department of Statistics, Stanford University, 1996.",False,False,False,False,False,False
"[123] E. Hüllermeier, S. Vanderlooy, Combining predictions in pairwise",False,False,False,False,False,False
classiﬁcation: an optimal adaptive voting strategy and its relation to,False,False,False,False,False,False
"weighted voting, Pattern Recognition 43 (1) (2010) 128–142",False,False,False,False,False,False
.,False,True,False,True,False,False
"[124] M. Galar, A. Fernandez, E. Barrenechea, H. Bustince, F. Herrera, An overview of",False,False,False,False,False,False
ensemble methods for binary classiﬁers in multi-class problems:,False,False,False,False,False,False
"Experimental study on one-vs-one and one-vs-all schemes, Pattern",False,False,False,False,False,False
Recognition 44 (8) (2011) 1761–1776,False,False,False,False,False,False
.,False,True,False,True,False,False
"[125] D. Tax, R.P.W. Duin, Characterizing one-class datasets, in: Proceedings of the",False,False,False,False,False,False
Sixteenth Annual Symposium of the Pattern Recognition Association of South,False,False,False,False,False,False
"Africa, 2005, pp. 21–26.",False,False,False,False,False,False
"[126] D. Tax, R. Duin, Combining one-class classiﬁers, in: Proceedings of the Second",False,False,False,False,False,False
"International Workshop on Multiple Classiﬁer Systems, MCS ’01, Springer-",False,False,False,False,False,False
"Verlag, London, UK, 2001, pp. 299–308",False,False,False,False,False,False
.,False,True,False,True,False,False
"[127] T. Wilk, M. Wozniak, Soft computing methods applied to combination of one-",False,False,False,False,False,False
"class classiﬁers, Neurocomputing 75 (2012) 185–193",False,False,False,False,False,False
.,False,True,False,True,False,False
"[128] G. Giacinto, R. Perdisci, M. Del Rio, F. Roli, Intrusion detection in computer",False,False,False,False,False,False
"networks by a modular ensemble of one-class classiﬁers, Information Fusion",False,False,False,False,False,False
9 (2008) 69–82,False,False,False,False,False,False
.,False,True,False,True,False,False
"[129] Y. Hu, Handbook of Neural Network Signal Processing, 1st ed., CRC Press, Inc.,",False,False,False,False,False,False
"Boca Raton, FL, USA, 2000",False,False,False,False,False,False
.,False,True,False,True,False,False
"[130] K. Woods, W.P. Kegelmeyer Jr., K. Bowyer, Combination of multiple classiﬁers",False,False,False,False,False,False
"using local accuracy estimates, IEEE Transactions on Pattern Analysis and",False,False,False,False,False,False
Machine Intelligence 19 (4) (1997) 405–410,False,False,False,False,False,False
.,False,True,False,True,False,False
"[131] M. Wozniak, M. Zmyslony, Combining classiﬁers using trained fuser –",False,False,False,False,False,False
"analytical and experimental results, Neural Network World 13 (7) (2010)",False,False,False,False,False,False
925–934,False,False,False,False,False,False
.,False,True,False,True,False,False
"[132] S. Raudys, Trainable fusion rules. I. Large sample size case, Neural Networks",False,False,False,False,False,False
19 (10) (2006) 1506–1516,False,False,False,False,False,False
.,False,True,False,True,False,False
"[133] M. van Erp, L. Vuurpijl, L. Schomaker, An overview and comparison of voting",False,False,False,False,False,False
"methods for pattern recognition, in: Proceedings of the Eighth International",False,False,False,False,False,False
"Workshop on Frontiers in Handwriting Recognition, 2002, 2002, pp. 195–200.",False,False,False,False,False,False
"[134] M. Wozniak, K. Jackowski, Some remarks on chosen methods of classiﬁer",False,False,False,False,False,False
"fusion based on weighted voting, in: E. Corchado, X. Wu, E. Oja, A. Herrero, B.",False,False,False,False,False,False
"Baruque (Eds.), Hybrid Artiﬁcial Intelligence Systems, Lecture Notes in",False,False,False,False,False,False
"Computer Science, vol. 5572, Springer, Berlin/Heidelberg, 2009, pp. 541–548",False,False,False,False,False,False
.,False,True,False,True,False,False
"[135] S. Raudys, Trainable fusion rules. II. Small sample-size effects, Neural",False,False,False,False,False,False
Networks 19 (10) (2006) 1517–1527,False,False,False,False,False,False
.,False,True,False,True,False,False
"[136] H. Inoue, H. Narihisa, Optimizing a multiple classiﬁer system, in: M. Ishizuka,",False,False,False,False,False,False
"A. Sattar (Eds.), PRICAI 2002: Trends in Artiﬁcial Intelligence, Lecture Notes in",False,False,False,False,False,False
"Computer Science, vol. 2417, Springer, Berlin/Heidelberg, 2002, pp. 1–16",False,False,False,False,False,False
.,False,True,False,True,False,False
"[137] L. Alexandre, A. Campilho, M. Kamel, Combining independent and unbiased",False,False,False,False,False,False
"classiﬁers using weighted average., in: Proceedings ICPR 2000, 2000, pp.",False,False,False,False,False,False
2495–2498.,False,False,False,False,False,False
"[138] B. Biggio, G. Fumera, F. Roli, Bayesian analysis of linear combiners, in:",False,False,False,False,False,False
Proceedings of the 7th International Conference on Multiple Classiﬁer,False,False,False,False,False,False
"Systems, MCS ’07, Springer-Verlag, Berlin, Heidelberg, 2007, pp. 292–301",False,False,False,False,False,False
.,False,True,False,True,False,False
"[139] J. Kittler, F. Alkoot, Sum versus vote fusion in multiple classiﬁer systems, IEEE",False,False,False,False,False,False
Transactions on Pattern Analysis and Machine Intelligence 25 (1) (2003),False,False,False,False,False,False
110–115,False,False,False,False,False,False
.,False,True,False,True,False,False
"[140] N. Rao, A generic sensor fusion problem: classiﬁcation and function",False,False,False,False,False,False
"estimation, in: F. Roli, J. Kittler, T. Windeatt (Eds.), Multiple Classiﬁer",False,False,False,False,False,False
"Systems, Lecture Notes in Computer Science, vol. 3077, Springer, 2004, pp.",False,False,False,False,False,False
16–30,False,False,False,False,False,False
.,False,True,False,True,False,False
"[141] D. Opitz, J. Shavlik, Generating accurate and diverse members of a neural-",False,False,False,False,False,False
"network ensemble, in: NIPS, 1995, pp. 535–541.",False,False,False,False,False,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
15,False,False,False,False,False,False
"[142] L. Rokach, O. Maimon, Feature set decomposition for decision trees,",False,False,False,False,False,False
Intelligent Data Analysis 9 (2) (2005) 131–158,False,False,False,False,False,False
.,False,True,False,True,False,False
"[143] G. Fumera, F. Roli, A theoretical and experimental analysis of linear",False,False,False,False,False,False
"combiners for multiple classiﬁer systems, IEEE Transactions on Pattern",False,False,False,False,False,False
"Analysis and Machine Intelligence 27 (6) (2005) 942–956, http://dx.doi.org/",False,False,False,False,False,False
10.1109/TPAMI.2005.109.,True,False,False,True,False,False
"[144] M. Wozniak, Experiments on linear combiners, in: E. Pietka, J. Kawa (Eds.),",False,False,False,False,False,False
"Information Technologies in Biomedicine, Advances in Soft Computing, vol.",False,False,False,False,False,False
"47, Springer, Berlin/Heidelberg, 2008, pp. 445–452",False,False,False,False,False,False
.,False,True,False,True,False,False
"[145] R. Duin, The combining classiﬁer: to train or not to train? in: Proceedings of",False,False,False,False,False,False
"the 16th International Conference on Pattern Recognition, 2002, vol. 2, 2002,",False,False,False,False,False,False
pp. 765–770.,False,False,False,False,False,False
"[146] R. Jacobs, M. Jordan, S. Nowlan, G. Hinton, Adaptive mixtures of local experts,",False,False,False,False,False,False
Neural Computation 3 (1991) 79–87,False,False,False,False,False,False
.,False,True,False,True,False,False
"[147] R. Jacobs, Methods for combining experts’ probability assessments, Neural",False,False,False,False,False,False
Computation 7 (5) (1995) 867–888,False,False,False,False,False,False
.,False,True,False,True,False,False
"[148] V. Tresp, M. Taniguchi, Combining estimators using non-constant weighting",False,False,False,False,False,False
"functions, Advances in Neural Information Processing Systems, vol. 7, MIT",False,False,False,False,False,False
"Press, 1995, pp. 419–426",False,False,False,False,False,False
.,False,True,False,True,False,False
"[149] P. Cheeseman, M. Self, J. Kelly, J. Stutz, W. Taylor, D. Freeman, AutoClass: a",False,False,False,False,False,False
"Bayesian classiﬁcation system, in: Machine Learning: Proceedings of the Fifth",False,False,False,False,False,False
"International Workshop, Morgan Kaufman, 1988",False,False,False,False,False,False
.,False,True,False,True,False,False
"[150] S. Shlien, Multiple binary decision tree classiﬁers, Pattern Recognition 23 (7)",False,False,False,False,False,False
(1990) 757–763,False,False,False,False,False,False
.,False,True,False,True,False,False
"[151] M. Wozniak, Experiments with trained and untrained fusers, in: E. Corchado,",False,False,False,False,False,False
"J. Corchado, A. Abraham (Eds.), Innovations in Hybrid Intelligent Systems,",False,False,False,False,False,False
"Advances in Soft Computing, vol. 44, Springer, Berlin/Heidelberg, 2007, pp.",False,False,False,False,False,False
144–150,False,False,False,False,False,False
.,False,True,False,True,False,False
"[152] M. Wozniak, Evolutionary approach to produce classiﬁer ensemble based on",False,False,False,False,False,False
"weighted voting, in: NaBIC 2009, World Congress on Nature & Biologically",False,False,False,False,False,False
"Inspired Computing, 2009, IEEE, 2009, pp. 648–653",False,False,False,False,False,False
.,False,True,False,True,False,False
"[153] L. Lin, X. Wang, B. Liu, Combining multiple classiﬁers based on statistical",False,False,False,False,False,False
"method for handwritten chinese character recognition, in: Proceedings of the",False,False,False,False,False,False
"2002 International Conference on Machine Learning and Cybernetics, 2002,",False,False,False,False,False,False
"vol. 1, 2002, pp. 252–255.",False,False,False,False,False,False
"[154] Z. Zheng, B. Padmanabhan, Constructing ensembles from data envelopment",False,False,False,False,False,False
"analysis, INFORMS Journal on Computing 19 (4) (2007) 486–496",False,False,False,False,False,False
.,False,True,False,True,False,False
"[155] D. Wolpert, Stacked generalization, Neural Networks 5 (1992) 241–259.",False,False,False,False,False,False
"[156] Y. Huang, C. Suen, A method of combining multiple experts for the",False,False,False,False,False,False
"recognition of unconstrained handwritten numerals, IEEE Transactions on",False,False,False,False,False,False
Pattern Analysis and Machine Intelligence 17 (1) (1995) 90–94,False,False,False,False,False,False
.,False,True,False,True,False,False
"[157] M.M. Gaber, A. Zaslavsky, S. Krishnaswamy, Mining data streams: a review,",False,False,False,False,False,False
SIGMOD Record 34 (2) (2005) 18–26,False,False,False,False,False,False
.,False,True,False,True,False,False
"[158] A. Patcha, J.-M. Park, An overview of anomaly detection techniques: existing",False,False,False,False,False,False
"solutions and latest technological trends, Computer Network 51 (12) (2007)",False,False,False,False,False,False
3448–3470,False,False,False,False,False,False
.,False,True,False,True,False,False
"[159] M.M. Black, R.J. Hickey, Classiﬁcation of customer call data in the presence of",False,False,False,False,False,False
"concept drift and noise, in: Proceedings of the First International Conference",False,False,False,False,False,False
"on Computing in an Imperfect World, Soft-Ware 2002, Springer-Verlag,",False,False,False,False,False,False
"London, UK, 2002, pp. 74–87",False,False,False,False,False,False
.,False,True,False,True,False,False
"[160] H. Wang, W. Fan, P.S. Yu, J. Han, Mining concept-drifting data streams using",False,False,False,False,False,False
"ensemble classiﬁers, in: Proceedings of the Ninth ACM SIGKDD International",False,False,False,False,False,False
"Conference on Knowledge Discovery and Data Mining, KDD ’03, ACM, New",False,False,False,False,False,False
"York, NY, USA, 2003, pp. 226–235",False,False,False,False,False,False
.,False,True,False,True,False,False
"[161] M.M. Gaber, P.S. Yu, Classiﬁcation of changes in evolving data streams using",False,False,False,False,False,False
"online clustering result deviation, in: Proc. Of International Workshop on",False,False,False,False,False,False
"Knowledge Discovery in Data Streams, 2006.",False,False,False,False,False,False
"[162] M. Markou, S. Singh, Novelty detection: a review – Part 1: Statistical",False,False,False,False,False,False
"approaches, Signal Process 83 (12) (2003) 2481–2497",False,False,False,False,False,False
.,False,True,False,True,False,False
"[163] M. Salganicoff, Density-adaptive learning and forgetting, in: Machine",False,False,False,False,False,False
"Learning: Proceedings of the Tenth Annual Conference, Morgan Kaufmann,",False,False,False,False,False,False
"San Francisco, CA, 1993",False,False,False,False,False,False
.,False,True,False,True,False,False
"[164] R. Klinkenberg, T. Joachims, Detecting concept drift with support vector",False,False,False,False,False,False
"machines, in: Proceedings of the Seventeenth International Conference on",False,False,False,False,False,False
"Machine Learning, ICML ’00, Morgan Kaufmann Publishers Inc., San Francisco,",False,False,False,False,False,False
"CA, USA, 2000, pp. 487–494",False,False,False,False,False,False
.,False,True,False,True,False,False
[165] M. Baena-Garcı,False,False,False,False,False,False
´,False,False,False,False,False,False
"a, J. del Campo-Ávila, R. Fidalgo, A. Bifet, R. Gavaldá, R.",False,False,False,False,False,False
"Morales-Bueno, Early drift detection method, in: Fourth International",False,False,False,False,False,False
"Workshop on Knowledge Discovery from Data Streams, 2006.",False,False,False,False,False,False
"[166] I. Zliobaite, Change with delayed labeling: when is it detectable?, in:",False,False,False,False,False,False
Proceedings of the 2010 IEEE International Conference on Data Mining,False,False,False,False,False,False
"Workshops, ICD-MW ’10, IEEE Computer Society, Washington, DC, USA, 2010,",False,False,False,False,False,False
pp 843–850,False,False,False,False,False,False
.,False,True,False,True,False,False
"[167] G. Giacinto, F. Roli, L. Bruzzone, Combination of neural and statistical",False,False,False,False,False,False
"algorithms for supervised classiﬁcation of remote-sensing images, Pattern",False,False,False,False,False,False
Recognition Letters 21 (5) (2000) 385–397,False,False,False,False,False,False
.,False,True,False,True,False,False
"[168] V. Rodriguez-Galiano, B. Ghimire, J. Rogan, M. Chica-Olmo, J. Rigol-Sanchez,",False,False,False,False,False,False
An assessment of the effectiveness of a random forest classiﬁer for land-cover,False,False,False,False,False,False
"classiﬁcation, ISPRS Journal of Photogrammetry and Remote Sensing 67 (0)",False,False,False,False,False,False
(2012) 93–104,False,False,False,False,False,False
.,False,True,False,True,False,False
"[169] P. Gislason, J. Benediktsson, J. Sveinsson, Random forests for land cover",False,False,False,False,False,False
"classiﬁcation, Pattern Recognition Letters 27 (4) (2006) 294–300",False,False,False,False,False,False
.,False,True,False,True,False,False
"[170] J.-W. Chan, D. Paelinckx, Evaluation of random forest and Adaboost tree-",False,False,False,False,False,False
based ensemble classiﬁcation and spectral band selection for ecotope,False,False,False,False,False,False
"mapping using airborne hyperspectral imagery, Remote Sensing of",False,False,False,False,False,False
Environment 112 (6) (2008) 2999–3011,False,False,False,False,False,False
.,False,True,False,True,False,False
"[171] J. Peters, N. Verhoest, R. Samson, M. Meirvenne, L. Cockx, B. Baets,",False,False,False,False,False,False
Uncertainty propagation in vegetation distribution models based on,False,False,False,False,False,False
"ensemble classiﬁers, Ecological Modelling 220 (6) (2009) 791–804",False,False,False,False,False,False
.,False,True,False,True,False,False
"[172] M. Han, X. Zhu, W. Yao, Remote sensing image classiﬁcation based on neural",False,False,False,False,False,False
"network ensemble algorithm, Neurocomputing 78 (1) (2012) 133–138",False,False,False,False,False,False
.,False,True,False,True,False,False
"[173] B. Waske, M. Braun, Classiﬁer ensembles for land cover mapping using",False,False,False,False,False,False
"multitemporal SAR imagery, ISPRS Journal of Photogrammetry and Remote",False,False,False,False,False,False
Sensing 64 (5) (2009) 450–457 (theme Issue: Mapping with SAR: Techniques,False,False,False,False,False,False
and Applications),False,False,False,False,False,False
.,False,True,False,True,False,False
"[174] B. Waske, S. van der Linden, C. Oldenburg, B. Jakimow, A. Rabe, P. Hostert,",False,False,False,False,False,False
imageRF – a user-oriented implementation for remote sensing image analysis,False,False,False,False,False,False
"with random forests, Environmental Modelling & Software 35 (0) (2012)",False,False,False,False,False,False
192–193,False,False,False,False,False,False
.,False,True,False,True,False,False
"[175] U. Maulik, D. Chakraborty, A self-trained ensemble with semisupervised",False,False,False,False,False,False
"SVM: an application to pixel classiﬁcation of remote sensing imagery, Pattern",False,False,False,False,False,False
Recognition 44 (3) (2011) 615–623,False,False,False,False,False,False
.,False,True,False,True,False,False
"[176] A. Henriques, A. Doria-Neto, R. Amaral, Classiﬁcation of multispectral images",False,False,False,False,False,False
"in coral environments using a hybrid of classiﬁer ensembles,",False,False,False,False,False,False
Neurocomputing 73 (7–9) (2010) 1256–1264,False,False,False,False,False,False
.,False,True,False,True,False,False
"[177] Y. Maghsoudi, M. Collins, D. Leckie, Polarimetric classiﬁcation of boreal forest",False,False,False,False,False,False
"using nonparametric feature selection and multiple classiﬁers, International",False,False,False,False,False,False
Journal of Applied Earth Observation and Geoinformation 19 (0) (2012) 139–,False,False,False,False,False,False
150,False,False,False,False,False,False
.,False,True,False,True,False,False
"[178] L. Bruzzone, R. Cossu, G. Vernazza, Combining parametric and non-",False,False,False,False,False,False
parametric algorithms for a partially unsupervised classiﬁcation of,False,False,False,False,False,False
"multitemporal remote-sensing images, Information Fusion 3 (4) (2002)",False,False,False,False,False,False
289–297,False,False,False,False,False,False
.,False,True,False,True,False,False
"[179] L. Bruzzone, R. Cossu, G. Vernazza, Detection of land-cover transitions by",False,False,False,False,False,False
"combining multidate classiﬁers, Pattern Recognition Letters 25 (13) (2004)",False,False,False,False,False,False
1491–1500,False,False,False,False,False,False
.,False,True,False,True,False,False
"[180] P. Du, S. Liu, J. Xia, Y. Zhao, Information fusion techniques for change",False,False,False,False,False,False
"detection from multi-temporal remote sensing images, Information Fusion",False,False,False,False,False,False
14 (1) (2013) 19–27,False,False,False,False,False,False
.,False,True,False,True,False,False
"[181] P. Arun-Raj-Kumar, S. Selvakumar, Distributed denial of service attack",False,False,False,False,False,False
"detection using an ensemble of neural classiﬁer, Computer",False,False,False,False,False,False
Communications 34 (11) (2011) 1328–1341,False,False,False,False,False,False
.,False,True,False,True,False,False
"[182] P. Kumar, S. Selvakumar, Detection of distributed denial of service attacks",False,False,False,False,False,False
"using an ensemble of adaptive and hybrid neuro-fuzzy systems, Computer",False,False,False,False,False,False
Communications (0) (2012),False,False,False,False,False,False
.,False,True,False,True,False,False
"[183] A. Shabtai, R. Moskovitch, Y. Elovici, C. Glezer, Detection of malicious code by",False,False,False,False,False,False
applying machine learning classiﬁers on static features: a state-of-the-art,False,False,False,False,False,False
"survey, Information Security Technical Report 14 (1) (2009) 16–29",False,False,False,False,False,False
.,False,True,False,True,False,False
"[184] M. Locasto, K. Wang, A. Keromytis, S. Stolfo, Flips: hybrid adaptive intrusion",False,False,False,False,False,False
"prevention, in: Proceedings of the 8th International Conference on Recent",False,False,False,False,False,False
"Advances in Intrusion Detection, RAID’05, Springer-Verlag, Berlin,",False,False,False,False,False,False
"Heidelberg, 2006, pp. 82–101",False,False,False,False,False,False
.,False,True,False,True,False,False
"[185] K. Wang, G. Cretu, S. Stolfo, Anomalous payload-based worm detection and",False,False,False,False,False,False
"signature generation, in: Proceedings of the 8th International Conference on",False,False,False,False,False,False
"Recent Advances in Intrusion Detection, RAID’05, Springer-Verlag, Berlin,",False,False,False,False,False,False
"Heidelberg, 2006, pp. 227–246",False,False,False,False,False,False
.,False,True,False,True,False,False
"[186] S. Peddabachigari, A. Abraham, C. Grosan, J. Thomas, Modeling intrusion",False,False,False,False,False,False
"detection system using hybrid intelligent systems, Journal of Network and",False,False,False,False,False,False
Computer Applications 30 (1) (2007) 114–132,False,False,False,False,False,False
.,False,True,False,True,False,False
"[187] D.-I. Curiac, C. Volosencu, Ensemble based sensing anomaly detection in",False,False,False,False,False,False
"wireless sensor networks, Expert Systems with Applications 39 (10) (2012)",False,False,False,False,False,False
9087–9096,False,False,False,False,False,False
.,False,True,False,True,False,False
"[188] R.J. Bolton, D.J. Hand, Statistical fraud detection: a review, Statistical Science",False,False,False,False,False,False
17 (3) (2002) 235–255,False,False,False,False,False,False
.,False,True,False,True,False,False
"[189] F. Louzada, A. Ara, Bagging k-dependence probabilistic networks: an",False,False,False,False,False,False
"alternative powerful fraud detection tool, Expert Systems with Applications",False,False,False,False,False,False
39 (14) (2012) 11583–11592,False,False,False,False,False,False
.,False,True,False,True,False,False
"[190] S. Bhattacharyya, S. Jha, K. Tharakunnel, J. Westland, Data mining for credit",False,False,False,False,False,False
"card fraud: a comparative study, Decision Support Systems 50 (3) (2011)",False,False,False,False,False,False
602–613,False,False,False,False,False,False
.,False,True,False,True,False,False
"[191] L. Yu, W. Yue, S. Wang, K. Lai, Support vector machine based multiagent",False,False,False,False,False,False
"ensemble learning for credit risk evaluation, Expert Systems with",False,False,False,False,False,False
Applications 37 (2) (2010) 1351–1360,False,False,False,False,False,False
.,False,True,False,True,False,False
"[192] Y. Kim, S. Sohn, Stock fraud detection using peer group analysis, Expert",False,False,False,False,False,False
Systems with Applications 39 (10) (2012) 8986–8992,False,False,False,False,False,False
.,False,True,False,True,False,False
"[193] B. Twala, Multiple classiﬁer application to credit risk assessment, Expert",False,False,False,False,False,False
Systems with Applications 37 (4) (2010) 3326–3336,False,False,False,False,False,False
.,False,True,False,True,False,False
"[194] S. Finlay, Multiple classiﬁer architectures and their application to credit risk",False,False,False,False,False,False
"assessment, European Journal of Operational Research 210 (2) (2011) 368–",False,False,False,False,False,False
378,False,False,False,False,False,False
.,False,True,False,True,False,False
"[195] G. Wang, J. Ma, A hybrid ensemble approach for enterprise credit risk",False,False,False,False,False,False
"assessment based on support vector machine, Expert Systems with",False,False,False,False,False,False
Applications 39 (5) (2012) 5325–5331,False,False,False,False,False,False
.,False,True,False,True,False,False
"[196] M. Kim, D. Kang, Classiﬁers selection in ensembles using genetic algorithms",False,False,False,False,False,False
"for bankruptcy prediction, Expert Systems with Applications 39 (10) (2012)",False,False,False,False,False,False
9308–9314,False,False,False,False,False,False
.,False,True,False,True,False,False
"[197] P. Ravisankar, V. Ravi, I. Bose, Failure prediction of dotcom companies using",False,False,False,False,False,False
"neural network–genetic programming hybrids, Information Sciences 180 (8)",False,False,False,False,False,False
(2010) 1257–1267,False,False,False,False,False,False
.,False,True,False,True,False,False
"[198] P. Ravisankar, V. Ravi, G. Rao, I. Bose, Detection of ﬁnancial statement fraud",False,False,False,False,False,False
"and feature selection using data mining techniques, Decision Support",False,False,False,False,False,False
Systems 50 (2) (2011) 491–500,False,False,False,False,False,False
.,False,True,False,True,False,False
16 M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
"[199] C. Tsai, Combining cluster analysis with classiﬁer ensembles to predict",False,False,False,False,False,False
"ﬁnancial distress, Information Fusion (0) (2011)",False,False,False,False,False,False
.,False,True,False,True,False,False
"[200] Y. Peng, G. Wang, G. Kou, Y. Shi, An empirical study of classiﬁcation algorithm",False,False,False,False,False,False
"evaluation for ﬁnancial risk prediction, Applied Soft Computing 11 (2) (2011)",False,False,False,False,False,False
2906–2915,False,False,False,False,False,False
.,False,True,False,True,False,False
"[201] V. Ravi, H. Kurniawan, P. Nwee-Kok-Thai, P. Ravi-Kumar, Soft computing",False,False,False,False,False,False
"system for bank performance prediction, Applied Soft Computing 8 (1) (2008)",False,False,False,False,False,False
305–315,False,False,False,False,False,False
.,False,True,False,True,False,False
"[202] H. Zhao, A. Sinha, W. Ge, Effects of feature construction on classiﬁcation",False,False,False,False,False,False
"performance: an empirical study in bank failure prediction, Expert Systems",False,False,False,False,False,False
"with Applications 36 (2, Part 2) (2009) 2633–2644",False,False,False,False,False,False
.,False,True,False,True,False,False
"[203] K. Aral, H. Guvenir, I. Sabuncuoglu, A. Akar, A prescription fraud detection",False,False,False,False,False,False
"model, Computer Methods and Programs in Biomedicine 106 (1) (2012) 37–",False,False,False,False,False,False
46,False,False,False,False,False,False
.,False,True,False,True,False,False
"[204] I. Christou, M. Bakopoulos, T. Dimitriou, E. Amolochitis, S. Tsekeridou, C.",False,False,False,False,False,False
"Dimitriadis, Detecting fraud in online games of chance and lotteries, Expert",False,False,False,False,False,False
Systems with Applications 38 (10) (2011) 13158–13169,False,False,False,False,False,False
.,False,True,False,True,False,False
"[205] H. Farvaresh, M. Sepehri, A data mining framework for detecting subscription",False,False,False,False,False,False
"fraud in telecommunication, Engineering Applications of Artiﬁcial",False,False,False,False,False,False
Intelligence 24 (1) (2011) 182–194,False,False,False,False,False,False
.,False,True,False,True,False,False
"[206] L. Subelj, S. Furlan, M. Bajec, An expert system for detecting automobile",False,False,False,False,False,False
"insurance fraud using social network analysis, Expert Systems with",False,False,False,False,False,False
Applications 38 (1) (2011) 1039–1052,False,False,False,False,False,False
.,False,True,False,True,False,False
"[207] A.X. Garg, N.K.J. Adhikari, H. McDonald, M.P. Rosas-Arellano, P.J. Devereaux, J.",False,False,False,False,False,False
"Beyene, J. Sam, R.B. Haynes, Effects of computerized clinical decision support",False,False,False,False,False,False
systems on practitioner performance and patient outcomes: a systematic,False,False,False,False,False,False
"review, Journal of the American Medical Association 293 (10) (2005) 1223–",False,False,False,False,False,False
1238,False,False,False,False,False,False
.,False,True,False,True,False,False
"[208] J. Eom, S. Kim, B. Zhang, AptaCDSS-E: a classiﬁer ensemble-based clinical",False,False,False,False,False,False
"decision support system for cardiovascular disease level prediction, Expert",False,False,False,False,False,False
Systems with Applications 34 (4) (2008) 2465–2479,False,False,False,False,False,False
.,False,True,False,True,False,False
"[209] R. Das, I. Turkoglu, A. Sengur, Effective diagnosis of heart disease through",False,False,False,False,False,False
"neural networks ensembles, Expert Systems with Applications 36 (4) (2009)",False,False,False,False,False,False
7675–7680,False,False,False,False,False,False
.,False,True,False,True,False,False
"[210] R. Das, I. Turkoglu, A. Sengur, Diagnosis of valvular heart disease through",False,False,False,False,False,False
"neural networks ensembles, Computer Methods and Programs in",False,False,False,False,False,False
Biomedicine 93 (2) (2009) 185–191,False,False,False,False,False,False
.,False,True,False,True,False,False
"[211] W. Baxt, Improving the accuracy of an artiﬁcial neural network using",False,False,False,False,False,False
"multiple differently trained networks, Neural Computation 4 (5) (1992) 772–",False,False,False,False,False,False
780,False,False,False,False,False,False
.,False,True,False,True,False,False
"[212] X. Zhang, J. Mesirov, D. Waltz, Hybrid system for protein secondary structure",False,False,False,False,False,False
"prediction, Journal of Molecular Biology 225 (4) (1992) 1049–1063",False,False,False,False,False,False
.,False,True,False,True,False,False
"[213] L. Nanni, Ensemble of classiﬁers for protein fold recognition,",False,False,False,False,False,False
Neurocomputing 69 (7) (2006) 850–853,False,False,False,False,False,False
.,False,True,False,True,False,False
"[214] T. Yang, V. Kecman, L. Cao, C. Zhang, J.Z. Huang, Margin-based ensemble",False,False,False,False,False,False
"classiﬁer for protein fold recognition, Expert Systems with Applications 38",False,False,False,False,False,False
(10) (2011) 12348–12355,False,False,False,False,False,False
.,False,True,False,True,False,False
"[215] A. Savio, M. Garcia-Sebastian, D. Chyzyk, C. Hernandez, M. Graña, A. Sistiaga,",False,False,False,False,False,False
"A.L. de Munain, J. Villanua, Neurocognitive disorder detection based on",False,False,False,False,False,False
"feature vectors extracted from VBM analysis of structural MRI, Computers in",False,False,False,False,False,False
Biology and Medicine 41 (8) (2011) 600–610,False,False,False,False,False,False
.,False,True,False,True,False,False
"[216] D. Chyzhyk, M. Graña, A. Savio, J. Maiora, Hybrid dendritic computing with",False,False,False,False,False,False
"kernel-LICA applied to alzheimer’s disease detection in MRI, Neurocomputing",False,False,False,False,False,False
75 (1) (2012) 72–77,False,False,False,False,False,False
.,False,True,False,True,False,False
"[217] L. Kuncheva, J. Rodriguez, Classiﬁer ensembles for fMRI data analysis: an",False,False,False,False,False,False
"experiment, Magnetic Resonance Imaging 28 (4) (2010) 583–593",False,False,False,False,False,False
.,False,True,False,True,False,False
"[218] C. Plumpton, L. Kuncheva, N. Oosterhof, S. Johnston, Naive random subspace",False,False,False,False,False,False
"ensemble with linear classiﬁers for real-time classiﬁcation of fMRI data,",False,False,False,False,False,False
Pattern Recognition 45 (6) (2012) 2101–2108,False,False,False,False,False,False
.,False,True,False,True,False,False
"[219] C. Cabral, M. Silveira, P. Figueiredo, Decoding visual brain states from fMRI",False,False,False,False,False,False
"using an ensemble of classiﬁers, Pattern Recognition 45 (6) (2012) 2064–",False,False,False,False,False,False
2074,False,False,False,False,False,False
.,False,True,False,True,False,False
"[220] G. Adomavicius, R. Sankaranarayanan, S. Sen, A. Tuzhilin, Incorporating",False,False,False,False,False,False
contextual information in recommender systems using a multidimensional,False,False,False,False,False,False
"approach, ACM Transactions Information Systems 23 (1) (2005) 103–145",False,False,False,False,False,False
.,False,True,False,True,False,False
"[221] J. Konstan, J. Riedl, How online merchants predict your preferences and prod",False,False,False,False,False,False
"you to purchase, IEEE Spectrum 49 (10) (2012) 48–56",False,False,False,False,False,False
.,False,True,False,True,False,False
"[222] R. Burke, Hybrid recommender systems: survey and experiments, User",False,False,False,False,False,False
Modeling and User-Adapted Interaction 12 (4) (2002) 331–370,False,False,False,False,False,False
.,False,True,False,True,False,False
[223] M. Balabanovic,False,False,False,False,False,False
´,False,False,False,False,False,False
", Y. Shoham, Fab: content-based, collaborative",False,False,False,False,False,False
"recommendation, Communications of the ACM 40 (3) (1997) 66–72",False,False,False,False,False,False
.,False,True,False,True,False,False
"[224] M.J. Pazzani, A framework for collaborative, content-based and demographic",False,False,False,False,False,False
"ﬁltering, Artiﬁcial Intelligence Review 13 (5–6) (1999) 393–408",False,False,False,False,False,False
.,False,True,False,True,False,False
"[225] M. Jahrer, A. Töscher, R. Legenstein, Combining predictions for accurate",False,False,False,False,False,False
"recommender systems, in: Proceedings of the 16th ACM SIGKDD",False,False,False,False,False,False
"International Conference on Knowledge Discovery and Data Mining, KDD",False,False,False,False,False,False
"’10, ACM, New York, NY, USA, 2010, pp. 693–702",False,False,False,False,False,False
.,False,True,False,True,False,False
"[226] C. Porcel, A. Tejeda-Lorente, M. Martı",False,False,False,False,False,False
´,False,False,False,False,False,False
"nez, E. Herrera-Viedma, A hybrid",False,False,False,False,False,False
recommender system for the selective dissemination of research resources in,False,False,False,False,False,False
"a technology transfer ofﬁce, Information Sciences 184 (1) (2012) 1–19",False,False,False,False,False,False
.,False,True,False,True,False,False
"[227] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, D. Netes, M. Sartin,",False,False,False,False,False,False
"Combining content-based and collaborative ﬁlters in an online newspaper,",False,False,False,False,False,False
in: Proceedings of the ACM SIGIR ’99 Workshop on Recommender Systems:,False,False,False,False,False,False
"Algorithms and Evaluation, ACM, 1999",False,False,False,False,False,False
.,False,True,False,True,False,False
"[228] D. Billsus, M. Pazzani, User modeling for adaptive news access, User Modeling",False,False,False,False,False,False
and User-Adapted Interaction 10 (2–3) (2000) 147–180,False,False,False,False,False,False
.,False,True,False,True,False,False
"[229] T. Tran, R. Cohen, Hybrid recommender systems for electronic commerce, in:",False,False,False,False,False,False
"Knowledge-Based Electronic Markets, Papers from the AAAI Workshop, AAAI",False,False,False,False,False,False
"Technical Report WS-00-04, AAAI Press, Menlo Park, CA, 2000, pp. 78–83.",False,False,False,False,False,False
"[230] M. Kunaver, T. Pozrl, M. Pogacnik, J. Tasic, Optimisation of combined",False,False,False,False,False,False
"collaborative recommender systems, AEU – International Journal of",False,False,False,False,False,False
Electronics and Communications 61 (7) (2007) 433–443,False,False,False,False,False,False
.,False,True,False,True,False,False
"[231] M. Goksedef, S. Gundoz-Oguducu, Combination of web page recommender",False,False,False,False,False,False
"systems, Expert Systems with Applications 37 (4) (2010) 2911–2922",False,False,False,False,False,False
.,False,True,False,True,False,False
M. Woz,False,False,False,False,False,False
´,False,False,False,False,False,False
niak et al. / Information Fusion 16 (2014) 3–17,False,False,False,False,False,False
17,False,False,False,False,False,False
