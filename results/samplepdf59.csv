Text,Is Capitalized,Is Roman Numeral,Is Number,is_heading,is_figure_heading,is_table_heading
" Hybrid Prediction Model with missing value Imputationfor medical dataArchana Purwar1 and Sandeep Kumar Singh21,2Department of Computer Science and Information Technology, JIIT Noida, India1archana.purwar@jiit.ac.in2sandeepk.singh@jiit.ac.inAbstract Accurate prediction in the presence of largenumber of missing values in the data set has alwaysbeen a challenging problem. Most of hybrid models toaddress this challenge have either deleted the missinginstances from the data set (popularly known as casedeletion) or have used some default way to fill themissing values. This paper, presents a novel HybridPrediction Model with missing value imputation (HPM-MI) that analyze various imputation techniques usingSimple K-means clustering and apply the best one to adata set. The proposed hybrid model is the first one touse combination of K-means clustering with MultilayerPerceptron. K-means Clustering is also used to validateclass labels of given data (incorrectly classifiedinstances are deleted i.e. pattern extracted from originaldata) before applying classifier. The proposed systemhas significantly improved data quality by use of bestimputation technique after quantitative analysis ofeleven imputation approaches. The efficiency ofproposed model as predictive classification system isinvestigated on three benchmark medical data setsnamely Pima Indians Diabetes, Wisconsin BreastCancer, and Hepatitis from the UCI Repository ofMachine Learning. In addition to accuracy, sensitivity,specificity; kappa statistics and the area under ROC arealso computed. The experimental results show HPM-MIhas produced accuracy, sensitivity, specificity, kappaand ROC as 99.82%, 100%, 99.7%, 0.996 and 1.0respectively for Pima Indian diabetes data set, 99.39%,99.31%, 99.54%, 0.986, and 1.0 respectively for BreastCancer data set and 99.98 %, 100%, 96.55%, 0.978 and0.99 respectively for Hepatitis data set. Results are bestin comparison with existing methods. Further, theperformance of our model is observed as function ofmissing rate and train-test ratio using 2D synthetic dataset and Wisconsin Diagnostics Breast Cancer data sets.Results are promising and therefore the proposed modelwill be very useful in prediction for medical domainespecially when numbers of missing value are large inthe data set.Keyword Missing Value Imputation, MultilayerPerceptron (MLP), K-Means Clustering, Data mining.1. IntroductionResearch in the field of predictive data miningfor medical applications is a significant andmoving area. Generally, a medical practitionercollects his/her knowledge from patient’ssymptoms and confirmed diagnosis. Diagnosis isusually made either by evaluating the current testresults of the patients or by referring to theprevious decisions made on other patients withsame test results. The accuracy of diagnosis ofpatient’s disease like diabetes, breast cancer andothers is greatly relenting on a experts’ experience(Meesad & Yen, 2003). Due to the pace at whichnumbers of patients are increasing, it has becomecumbersome to make diagnostics decisions. On theother side, development of new computationalmethods and tools makes relatively easy to makedecisions from dedicated databases of electronicpatient records. As an example, numerousclassifiers have been developed for diagnosis andscreening of diabetes, cancer and liver disorders(Seera & Lim, 2014). A number of  classificationsystems have been developed in the literature likeRBQ, LVQ C4.5, CART, Bayesian Tree, ANN +FNN, HPM, Sim + F2, real coded GA, and FMM-CART-RF to support diagnosis of diabetes as wellas breast cancer disease (Seera & Lim, 2014).Various hybrid models (Kahramanli & Allahverdi,2008; llango & Ramaraj, 2010; Patil, Joshi, &Toshniwal, 2010) namely hybrid system consistingof artificial neural network and fuzzy neuralnetwork, HPM consisting of K-means clusteringwith J48 and  HPM with F-score consisting offeature selection using F-score, K-means clusteringand SVM are also proposed in the literature.Researchers have developed a largenumber of classification systems to improveaccuracy of predictive classification.  Theproposed model uses K-means clustering as a",False,False,False,True,False,False
" Hybrid Prediction Model with missing value Imputationfor medical dataArchana Purwar1 and Sandeep Kumar Singh21,2Department of Computer Science and Information Technology, JIIT Noida, India1archana.purwar@jiit.ac.in2sandeepk.singh@jiit.ac.inAbstract Accurate prediction in the presence of largenumber of missing values in the data set has alwaysbeen a challenging problem. Most of hybrid models toaddress this challenge have either deleted the missinginstances from the data set (popularly known as casedeletion) or have used some default way to fill themissing values. This paper, presents a novel HybridPrediction Model with missing value imputation (HPM-MI) that analyze various imputation techniques usingSimple K-means clustering and apply the best one to adata set. The proposed hybrid model is the first one touse combination of K-means clustering with MultilayerPerceptron. K-means Clustering is also used to validateclass labels of given data (incorrectly classifiedinstances are deleted i.e. pattern extracted from originaldata) before applying classifier. The proposed systemhas significantly improved data quality by use of bestimputation technique after quantitative analysis ofeleven imputation approaches. The efficiency ofproposed model as predictive classification system isinvestigated on three benchmark medical data setsnamely Pima Indians Diabetes, Wisconsin BreastCancer, and Hepatitis from the UCI Repository ofMachine Learning. In addition to accuracy, sensitivity,specificity; kappa statistics and the area under ROC arealso computed. The experimental results show HPM-MIhas produced accuracy, sensitivity, specificity, kappaand ROC as 99.82%, 100%, 99.7%, 0.996 and 1.0respectively for Pima Indian diabetes data set, 99.39%,99.31%, 99.54%, 0.986, and 1.0 respectively for BreastCancer data set and 99.98 %, 100%, 96.55%, 0.978 and0.99 respectively for Hepatitis data set. Results are bestin comparison with existing methods. Further, theperformance of our model is observed as function ofmissing rate and train-test ratio using 2D synthetic dataset and Wisconsin Diagnostics Breast Cancer data sets.Results are promising and therefore the proposed modelwill be very useful in prediction for medical domainespecially when numbers of missing value are large inthe data set.Keyword Missing Value Imputation, MultilayerPerceptron (MLP), K-Means Clustering, Data mining.1. IntroductionResearch in the field of predictive data miningfor medical applications is a significant andmoving area. Generally, a medical practitionercollects his/her knowledge from patient’ssymptoms and confirmed diagnosis. Diagnosis isusually made either by evaluating the current testresults of the patients or by referring to theprevious decisions made on other patients withsame test results. The accuracy of diagnosis ofpatient’s disease like diabetes, breast cancer andothers is greatly relenting on a experts’ experience(Meesad & Yen, 2003). Due to the pace at whichnumbers of patients are increasing, it has becomecumbersome to make diagnostics decisions. On theother side, development of new computationalmethods and tools makes relatively easy to makedecisions from dedicated databases of electronicpatient records. As an example, numerousclassifiers have been developed for diagnosis andscreening of diabetes, cancer and liver disorders(Seera & Lim, 2014). A number of  classificationsystems have been developed in the literature likeRBQ, LVQ C4.5, CART, Bayesian Tree, ANN +FNN, HPM, Sim + F2, real coded GA, and FMM-CART-RF to support diagnosis of diabetes as wellas breast cancer disease (Seera & Lim, 2014).Various hybrid models (Kahramanli & Allahverdi,2008; llango & Ramaraj, 2010; Patil, Joshi, &Toshniwal, 2010) namely hybrid system consistingof artificial neural network and fuzzy neuralnetwork, HPM consisting of K-means clusteringwith J48 and  HPM with F-score consisting offeature selection using F-score, K-means clusteringand SVM are also proposed in the literature.Researchers have developed a largenumber of classification systems to improveaccuracy of predictive classification.  Theproposed model uses K-means clustering as a",False,False,False,True,False,False
 ,False,False,False,False,False,False
Hybrid Prediction Model with missing value Imputation,False,False,False,False,False,False
for medical data,False,False,False,False,False,False
Archana Purwar,False,False,False,False,False,False
1,False,False,False,False,False,False
 and Sandeep Kumar Singh,False,False,False,False,False,False
2,False,False,False,False,False,False
"1,2",False,False,False,False,False,False
"Department of Computer Science and Information Technology, JIIT Noida, India",False,False,False,False,False,False
1,False,False,False,False,False,False
archana.purwar@jiit.ac.in,False,False,False,False,False,False
2,False,False,False,False,False,False
sandeepk.singh@jiit.ac.in,False,False,False,False,False,False
Abstract Accurate prediction in the presence of large,False,False,False,True,False,False
number of missing values in the data set has always,False,False,False,False,False,False
been a challenging problem. Most of hybrid models to,False,False,False,False,False,False
address this challenge have either deleted the missing,False,False,False,False,False,False
instances from the data set (popularly known as case,False,False,False,False,False,False
deletion) or have used some default way to fill the,False,False,False,False,False,False
"missing values. This paper, presents a novel Hybrid",False,False,False,False,False,False
Prediction Model with missing value imputation (HPM-,False,False,False,False,False,False
MI) that analyze various imputation techniques using,False,False,False,False,False,False
Simple K-means clustering and apply the best one to a,False,False,False,False,False,False
data set. The proposed hybrid model is the first one to,False,False,False,False,False,False
use combination of K-means clustering with Multilayer,False,False,False,False,False,False
Perceptron. K-means Clustering is also used to validate,False,False,False,False,False,False
class labels of given data (incorrectly classified,False,False,False,False,False,False
instances are deleted i.e. pattern extracted from original,False,False,False,False,False,False
data) before applying classifier. The proposed system,False,False,False,False,False,False
has significantly improved data quality by use of best,False,False,False,False,False,False
imputation technique after quantitative analysis of,False,False,False,False,False,False
eleven imputation approaches. The efficiency of,False,False,False,False,False,False
proposed model as predictive classification system is,False,False,False,False,False,False
investigated on three benchmark medical data sets,False,False,False,False,False,False
"namely Pima Indians Diabetes, Wisconsin Breast",False,False,False,False,False,False
"Cancer, and Hepatitis from the UCI Repository of",False,False,False,False,False,False
"Machine Learning. In addition to accuracy, sensitivity,",False,False,False,False,False,False
specificity; kappa statistics and the area under ROC are,False,False,False,False,False,False
also computed. The experimental results show HPM-MI,False,False,False,False,False,False
"has produced accuracy, sensitivity, specificity, kappa",False,False,False,False,False,False
"and ROC as 99.82%, 100%, 99.7%, 0.996 and 1.0",False,False,False,False,False,False
"respectively for Pima Indian diabetes data set, 99.39%,",False,False,False,False,False,False
"99.31%, 99.54%, 0.986, and 1.0 respectively for Breast",False,False,False,False,False,False
"Cancer data set and 99.98 %, 100%, 96.55%, 0.978 and",False,False,False,False,False,False
0.99 respectively for Hepatitis data set. Results are best,False,False,False,False,False,False
"in comparison with existing methods. Further, the",False,False,False,False,False,False
performance of our model is observed as function of,False,False,False,False,False,False
missing rate and train-test ratio using 2D synthetic data,False,False,False,False,False,False
set and Wisconsin Diagnostics Breast Cancer data sets.,False,False,False,False,False,False
Results are promising and therefore the proposed model,False,False,False,False,False,False
will be very useful in prediction for medical domain,False,False,False,False,False,False
especially when numbers of missing value are large in,False,False,False,False,False,False
the data set.,False,False,False,False,False,False
"Keyword Missing Value Imputation, Multilayer",False,False,False,False,False,False
"Perceptron (MLP), K-Means Clustering, Data mining.",False,False,False,False,False,False
1. Introduction,False,False,True,True,False,False
Research in the field of predictive data mining,False,False,False,False,False,False
for medical applications is a significant and,False,False,False,False,False,False
"moving area. Generally, a medical practitioner",False,False,False,False,False,False
collects his/her knowledge from patient’s,False,False,False,False,False,False
symptoms and confirmed diagnosis. Diagnosis is,False,False,False,False,False,False
usually made either by evaluating the current test,False,False,False,False,False,False
results of the patients or by referring to the,False,False,False,False,False,False
previous decisions made on other patients with,False,False,False,False,False,False
same test results. The accuracy of diagnosis of,False,False,False,False,False,False
"patient’s disease like diabetes, breast cancer and",False,False,False,False,False,False
others is greatly relenting on a experts’ experience,False,False,False,False,False,False
"(Meesad & Yen, 2003). Due to the pace at which",False,False,False,False,False,False
"numbers of patients are increasing, it has become",False,False,False,False,False,False
cumbersome to make diagnostics decisions. On the,False,False,False,False,False,False
"other side, development of new computational",False,False,False,False,False,False
methods and tools makes relatively easy to make,False,False,False,False,False,False
decisions from dedicated databases of electronic,False,False,False,False,False,False
"patient records. As an example, numerous",False,False,False,False,False,False
classifiers have been developed for diagnosis and,False,False,False,False,False,False
"screening of diabetes, cancer and liver disorders",False,False,False,False,False,False
"(Seera & Lim, 2014). A number of  classification",False,False,False,False,False,False
systems have been developed in the literature like,False,False,False,False,False,False
"RBQ, LVQ C4.5, CART, Bayesian Tree, ANN +",False,False,False,False,False,False
"FNN, HPM, Sim + F2, real coded GA, and FMM-",False,False,False,False,False,False
CART-RF to support diagnosis of diabetes as well,False,False,False,False,False,False
"as breast cancer disease (Seera & Lim, 2014).",False,False,False,False,False,False
"Various hybrid models (Kahramanli & Allahverdi,",False,False,False,False,False,False
"2008; llango & Ramaraj, 2010; Patil, Joshi, &",False,False,False,False,False,False
"Toshniwal, 2010) namely hybrid system consisting",False,False,False,False,False,False
of artificial neural network and fuzzy neural,False,False,False,False,False,False
"network, HPM consisting of K-means clustering",False,False,False,False,False,False
with J48 and  HPM with F-score consisting of,False,False,False,False,False,False
"feature selection using F-score, K-means clustering",False,False,False,False,False,False
and SVM are also proposed in the literature.,False,False,False,False,False,False
Researchers have developed a large,False,False,False,False,False,False
number of classification systems to improve,False,False,False,False,False,False
accuracy of predictive classification.  The,False,False,False,False,False,False
proposed model uses K-means clustering as a,False,False,False,False,False,False
" means to analyze 11 MVI techniques under studyand selects the best imputation method. This bestimputation method is applied on the data set beforepattern extraction and subsequently applyingprediction. Moreover, to the best of ourknowledge, none of the hybrid prediction modelshave combined use of K-means clustering andMLP for prediction.This paper makes a novel contribution byfirst analyzing 11 missing data imputationtechniques experimentally and finds the bestmethod for handling the missing values in the dataset using K-means clustering. Consequently, itimproves the quality of data. Moreover, it alsoaims at predictive classification using novel modelthat can classify records in the test data set usingtraining data set. Multi layer Perceptron (MLP) hasa capability to learn from examples and cangeneralize beyond the training data (Carpenter &Markuzon, 1998; Downs, Harrison, Kennedy, &Cross, 1996; Mukhopadhyay, Changhong, Huang,Mulong, & Palakal, 2002). Due to thesecharacteristics of neural networks, MLP withbackpropagation is investigated for developing avalid and useful prediction model. K-meansclustering is also used to develop proposed HybridPrediction Model with Missing Value Imputation(HPM-MI) to extract the patterns from data beforeapplying MLP for classification.Rest of the paper is grouped in five sections.Section-2 describes the study of data miningmethods namely imputation methods, K-meansclustering, MLP and review of prediction models.Then, section-3 depicts the proposed model.Evaluation of proposed model is done in section-4.Section-5 shows the results and its discussion.Finally, paper is concluded by section-6.2. Background StudyThis section reviews a few data mining methodsand predictive classification models.2.1 Data Mining MethodsAs the amount of data stored in medicaldatabases is increasing, there is growing need forefficient and effective techniques to extract theinformation. Previous researches have givenevidence that medical diagnosis and prognosis isamended by employing data mining techniques onclinical data (Hammer & Bonates, 2006;Saastamoinen & Ketola, 2006; Tsirogiannis, et al.,2004). This has been possible due to extensiveavailability of data mining techniques and tools fordata analysis. Predictive modeling requires that themedical informatics researchers and practitionersto select the most appropriate strategy to cope withclinical prediction problem (Bellazzi & Zupan,2008). This section discusses mining techniquesused to develop the proposed model.2.1.1 Missing Value Imputation2.1.1.1 IntroductionIn real-life databases, incomplete data orinformation as shown in Table 1 is frequent owingto the presence of missing values in the attributes.First row in Table 1 shows name of the variables inthe data set while other rows show the values ofthese variables. The values denoted by ‘?’ in Table1 represent the missing values. Missing values canoccur due to large number of reasons such as errorsin the manual data entry procedures, equipmenterrors or incorrect measurements. The presence ofmissing values (MVs) in data mining producesseveral problems in the knowledge extractionprocess such as loss of efficiency, complications inmanaging and analyzing data. It may also result inbias decisions due to differences between missingand complete data.Table 1Sample data showing missing values by ‘?’.AB C D E F G H class6 148 72 35 ? 33.6 0.62 50 yes1 85 66 29 ? 26.6 0.35 31 no8 183 64 ? ? 23.3 0.67 32 yes1 89 66 23 94 28.1 0.16 21 no? 137 40 35 168 43.1 2.28 33 yesIn order to solve these problems, twoapproaches are found in the literature. Firstapproach consists of missing data tolerationtechniques which integrate the techniques ofmissing values handling in specific data miningalgorithms such as in classification (David, 2007;Saar-Tsechansky M, 2007), clustering (Hathaway& Bezdek, 2002) and feature selection (Aussem &de Morais, 2008). Second type of approachconsists of missing data imputation techniques",False,False,False,True,False,False
" means to analyze 11 MVI techniques under studyand selects the best imputation method. This bestimputation method is applied on the data set beforepattern extraction and subsequently applyingprediction. Moreover, to the best of ourknowledge, none of the hybrid prediction modelshave combined use of K-means clustering andMLP for prediction.This paper makes a novel contribution byfirst analyzing 11 missing data imputationtechniques experimentally and finds the bestmethod for handling the missing values in the dataset using K-means clustering. Consequently, itimproves the quality of data. Moreover, it alsoaims at predictive classification using novel modelthat can classify records in the test data set usingtraining data set. Multi layer Perceptron (MLP) hasa capability to learn from examples and cangeneralize beyond the training data (Carpenter &Markuzon, 1998; Downs, Harrison, Kennedy, &Cross, 1996; Mukhopadhyay, Changhong, Huang,Mulong, & Palakal, 2002). Due to thesecharacteristics of neural networks, MLP withbackpropagation is investigated for developing avalid and useful prediction model. K-meansclustering is also used to develop proposed HybridPrediction Model with Missing Value Imputation(HPM-MI) to extract the patterns from data beforeapplying MLP for classification.Rest of the paper is grouped in five sections.Section-2 describes the study of data miningmethods namely imputation methods, K-meansclustering, MLP and review of prediction models.Then, section-3 depicts the proposed model.Evaluation of proposed model is done in section-4.Section-5 shows the results and its discussion.Finally, paper is concluded by section-6.2. Background StudyThis section reviews a few data mining methodsand predictive classification models.2.1 Data Mining MethodsAs the amount of data stored in medicaldatabases is increasing, there is growing need forefficient and effective techniques to extract theinformation. Previous researches have givenevidence that medical diagnosis and prognosis isamended by employing data mining techniques onclinical data (Hammer & Bonates, 2006;Saastamoinen & Ketola, 2006; Tsirogiannis, et al.,2004). This has been possible due to extensiveavailability of data mining techniques and tools fordata analysis. Predictive modeling requires that themedical informatics researchers and practitionersto select the most appropriate strategy to cope withclinical prediction problem (Bellazzi & Zupan,2008). This section discusses mining techniquesused to develop the proposed model.2.1.1 Missing Value Imputation2.1.1.1 IntroductionIn real-life databases, incomplete data orinformation as shown in Table 1 is frequent owingto the presence of missing values in the attributes.First row in Table 1 shows name of the variables inthe data set while other rows show the values ofthese variables. The values denoted by ‘?’ in Table1 represent the missing values. Missing values canoccur due to large number of reasons such as errorsin the manual data entry procedures, equipmenterrors or incorrect measurements. The presence ofmissing values (MVs) in data mining producesseveral problems in the knowledge extractionprocess such as loss of efficiency, complications inmanaging and analyzing data. It may also result inbias decisions due to differences between missingand complete data.Table 1Sample data showing missing values by ‘?’.AB C D E F G H class6 148 72 35 ? 33.6 0.62 50 yes1 85 66 29 ? 26.6 0.35 31 no8 183 64 ? ? 23.3 0.67 32 yes1 89 66 23 94 28.1 0.16 21 no? 137 40 35 168 43.1 2.28 33 yesIn order to solve these problems, twoapproaches are found in the literature. Firstapproach consists of missing data tolerationtechniques which integrate the techniques ofmissing values handling in specific data miningalgorithms such as in classification (David, 2007;Saar-Tsechansky M, 2007), clustering (Hathaway& Bezdek, 2002) and feature selection (Aussem &de Morais, 2008). Second type of approachconsists of missing data imputation techniques",False,False,False,True,False,False
 ,False,False,False,False,False,False
means to analyze 11 MVI techniques under study,False,False,False,False,False,False
and selects the best imputation method. This best,False,False,False,False,False,False
imputation method is applied on the data set before,False,False,False,False,False,False
pattern extraction and subsequently applying,False,False,False,False,False,False
"prediction. Moreover, to the best of our",False,False,False,False,False,False
"knowledge, none of the hybrid prediction models",False,False,False,False,False,False
have combined use of K-means clustering and,False,False,False,False,False,False
MLP for prediction.,False,False,False,False,False,False
This paper makes a novel contribution by,False,False,False,False,False,False
first analyzing 11 missing data imputation,False,False,False,False,False,False
techniques experimentally and finds the best,False,False,False,False,False,False
method for handling the missing values in the data,False,False,False,False,False,False
"set using K-means clustering. Consequently, it",False,False,False,False,False,False
"improves the quality of data. Moreover, it also",False,False,False,False,False,False
aims at predictive classification using novel model,False,False,False,False,False,False
that can classify records in the test data set using,False,False,False,False,False,False
training data set. Multi layer Perceptron (MLP) has,False,False,False,False,False,False
a capability to learn from examples and can,False,False,False,False,False,False
generalize beyond the training data (Carpenter &,False,False,False,False,False,False
"Markuzon, 1998; Downs, Harrison, Kennedy, &",False,False,False,False,False,False
"Cross, 1996; Mukhopadhyay, Changhong, Huang,",False,False,False,False,False,False
"Mulong, & Palakal, 2002). Due to these",False,False,False,False,False,False
"characteristics of neural networks, MLP with",False,False,False,False,False,False
backpropagation is investigated for developing a,False,False,False,False,False,False
valid and useful prediction model. K-means,False,False,False,False,False,False
clustering is also used to develop proposed Hybrid,False,False,False,False,False,False
Prediction Model with Missing Value Imputation,False,False,False,False,False,False
(HPM-MI) to extract the patterns from data before,False,False,False,False,False,False
applying MLP for classification.,False,False,False,False,False,False
Rest of the paper is grouped in five sections.,False,False,False,False,False,False
Section-2 describes the study of data mining,False,False,False,False,False,False
"methods namely imputation methods, K-means",False,False,False,False,False,False
"clustering, MLP and review of prediction models.",False,False,False,False,False,False
"Then, section-3 depicts the proposed model.",False,False,False,False,False,False
Evaluation of proposed model is done in section-4.,False,False,False,False,False,False
Section-5 shows the results and its discussion.,False,False,False,False,False,False
"Finally, paper is concluded by section-6.",False,False,False,False,False,False
2. Background Study,False,False,True,True,False,False
This section reviews a few data mining methods,False,False,False,False,False,False
and predictive classification models.,False,False,False,False,False,False
2.1 Data Mining Methods,False,False,False,False,False,False
As the amount of data stored in medical,False,False,False,False,False,False
"databases is increasing, there is growing need for",False,False,False,False,False,False
efficient and effective techniques to extract the,False,False,False,False,False,False
information. Previous researches have given,False,False,False,False,False,False
evidence that medical diagnosis and prognosis is,False,False,False,False,False,False
amended by employing data mining techniques on,False,False,False,False,False,False
"clinical data (Hammer & Bonates, 2006;",False,False,False,False,False,False
"Saastamoinen & Ketola, 2006; Tsirogiannis, et al.,",False,False,False,False,False,False
2004). This has been possible due to extensive,False,False,False,False,False,False
availability of data mining techniques and tools for,False,False,False,False,False,False
data analysis. Predictive modeling requires that the,False,False,False,False,False,False
medical informatics researchers and practitioners,False,False,False,False,False,False
to select the most appropriate strategy to cope with,False,False,False,False,False,False
"clinical prediction problem (Bellazzi & Zupan,",False,False,False,False,False,False
2008). This section discusses mining techniques,False,False,False,False,False,False
used to develop the proposed model.,False,False,False,False,False,False
2.1.1 Missing Value Imputation,False,False,False,False,False,False
2.1.1.1 Introduction,False,False,False,True,False,False
"In real-life databases, incomplete data or",False,False,False,False,False,False
information as shown in Table 1 is frequent owing,False,False,False,False,False,False
to the presence of missing values in the attributes.,False,False,False,False,False,False
First row in Table 1 shows name of the variables in,False,False,False,False,False,False
the data set while other rows show the values of,False,False,False,False,False,False
these variables. The values denoted by ‘?’ in Table,False,False,False,False,False,False
1 represent the missing values. Missing values can,False,False,False,False,False,False
occur due to large number of reasons such as errors,False,False,False,False,False,False
"in the manual data entry procedures, equipment",False,False,False,False,False,False
errors or incorrect measurements. The presence of,False,False,False,False,False,False
missing values (MVs) in data mining produces,False,False,False,False,False,False
several problems in the knowledge extraction,False,False,False,False,False,False
"process such as loss of efficiency, complications in",False,False,False,False,False,False
managing and analyzing data. It may also result in,False,False,False,False,False,False
bias decisions due to differences between missing,False,False,False,False,False,False
and complete data.,False,False,False,False,False,False
Sample data showing missing values by ‘?’.,False,False,False,False,False,False
A,True,False,False,True,False,False
B C D E F G H class,False,False,False,False,False,False
6 148 72 35 ? 33.6 0.62 50 yes,False,False,False,False,False,False
1 85 66 29 ? 26.6 0.35 31 no,False,False,False,False,False,False
8 183 64 ? ? 23.3 0.67 32 yes,False,False,False,False,False,False
1 89 66 23 94 28.1 0.16 21 no,False,False,False,False,False,False
? 137 40 35 168 43.1 2.28 33 yes,False,False,False,False,False,False
"In order to solve these problems, two",False,False,False,False,False,False
approaches are found in the literature. First,False,False,False,False,False,False
approach consists of missing data toleration,False,False,False,False,False,False
techniques which integrate the techniques of,False,False,False,False,False,False
missing values handling in specific data mining,False,False,False,False,False,False
"algorithms such as in classification (David, 2007;",False,False,False,False,False,False
"Saar-Tsechansky M, 2007), clustering (Hathaway",False,False,False,False,False,False
"& Bezdek, 2002) and feature selection (Aussem &",False,False,False,False,False,False
"de Morais, 2008). Second type of approach",False,False,False,False,False,False
consists of missing data imputation techniques,False,False,False,False,False,False
" which fill in missing values before using complete-data methods. One advantage of imputation is thatthe treatment of missing data is independent of thesucceeding mining algorithm, and people canselect a suitable learning algorithm afterimputation (Qin, Zhang, Zhu, Zhang, & Zhang,2007).In our proposed HPM-MI, we have usedbest proven MVI approach to fill the missing valuebefore pattern extraction and classification on thedata set. We have validated our model on threebenchmark data sets i.e. Pima Indian diabetesWisconsin breast cancer and Hepatitis data setfrom UCI repository (Newman, Hettich, Blake, &Merz, 2007) having 763, 16 and 167 missingvalues respectively and two complete data setsnamely 2D synthetic data set as well as WisconsinDiagnostics Breast cancer (WDBC) in whichmissing values were artificially induced.2.1.1.2 Imputation TechniquesIn order to analyse the impact of various MVItechniques (Luengo, García,  & Herrera, 2011;Koren, Bell, & Volinsky,2009; Takács,, Pilászy, &Németh, 2008), experimentation is done on thefollowing  techniques to choose the best possibleone to handle missing values present in the datasets under study.The following approaches havebeen empirically assessed  to find the missingvalues:· Case Deletion: The examples that haveany missing value in their attributes areremoved from the data set.· Most Common Method (MC): Missingvalues present in the data set is substitutedby mean value for numerical and mode fornominal attributes.· Concept Most Common (CMC): Thismethod calculates the missing valuessimilar to MC method but it considers onlythe same class in which MV is missing.· K-Nearest Neighbor (KNNI): Firstly, thismethod finds the k nearest neighbors andthen, the most common value among allneighbors is taken for nominal attributes,and the mean value is used for numericalattributes.· Weighted Imputation with K-NearestNeighbor (WKNN): This methodcalculates the distance of each missingvalue instances from its neighbors. Thisdistance is used to calculate the weight.MV is computed by weighted mean fornumerical attributes. For nominalattributes, imputed value is the categorywith highest weight.· K-means Clustering Imputation (KMI): Allthe instances are clustered using K-meansclustering. The instances in each clusterare considered nearest neighbors of eachother. The missing value is computed insimilar manner as in KNNI method.· Imputation with Fuzzy K-means Clustering(FKMI): After the fuzzy clustering of thedata set, missing values are computed asweighed sum of all centroids, using themembership function of each cluster as theweight.· Support Vector Machines Imputation(SVMI): SVM model is trained to predictmissing attributes from the completeinstances which do not have missingvalues. During testing,   missing values arepredicted using other attributes by settingmissing attribute as a class attribute whosevalue is intended to be predicted.· Singular Value Decomposition Imputation(SVDI): In this method, singular valuedecomposition is used to obtain a set ofmutually orthogonal expression patternsthat can be linearly combined toapproximate the values of all attributes inthe data set. In order to do that, first SVDIestimates the MVs within the expectedmaximization algorithm, and then itcomputes the singular valuedecomposition and obtains the eigenvalues. Now, SVDI can use the eigenvalues to apply a regression to thecomplete attributes of the instance and toobtain an estimation of the MV itself.· Local Least Squares Imputation (LLSI):This method identifies k similar instancesand fits a least squares model between theinstances and the known part of the recordwith missing values· Matrix Factorization: This method aims toget decompositions whose productapproximate the values of all attribute in",False,False,False,False,False,False
" which fill in missing values before using complete-data methods. One advantage of imputation is thatthe treatment of missing data is independent of thesucceeding mining algorithm, and people canselect a suitable learning algorithm afterimputation (Qin, Zhang, Zhu, Zhang, & Zhang,2007).In our proposed HPM-MI, we have usedbest proven MVI approach to fill the missing valuebefore pattern extraction and classification on thedata set. We have validated our model on threebenchmark data sets i.e. Pima Indian diabetesWisconsin breast cancer and Hepatitis data setfrom UCI repository (Newman, Hettich, Blake, &Merz, 2007) having 763, 16 and 167 missingvalues respectively and two complete data setsnamely 2D synthetic data set as well as WisconsinDiagnostics Breast cancer (WDBC) in whichmissing values were artificially induced.2.1.1.2 Imputation TechniquesIn order to analyse the impact of various MVItechniques (Luengo, García,  & Herrera, 2011;Koren, Bell, & Volinsky,2009; Takács,, Pilászy, &Németh, 2008), experimentation is done on thefollowing  techniques to choose the best possibleone to handle missing values present in the datasets under study.The following approaches havebeen empirically assessed  to find the missingvalues:· Case Deletion: The examples that haveany missing value in their attributes areremoved from the data set.· Most Common Method (MC): Missingvalues present in the data set is substitutedby mean value for numerical and mode fornominal attributes.· Concept Most Common (CMC): Thismethod calculates the missing valuessimilar to MC method but it considers onlythe same class in which MV is missing.· K-Nearest Neighbor (KNNI): Firstly, thismethod finds the k nearest neighbors andthen, the most common value among allneighbors is taken for nominal attributes,and the mean value is used for numericalattributes.· Weighted Imputation with K-NearestNeighbor (WKNN): This methodcalculates the distance of each missingvalue instances from its neighbors. Thisdistance is used to calculate the weight.MV is computed by weighted mean fornumerical attributes. For nominalattributes, imputed value is the categorywith highest weight.· K-means Clustering Imputation (KMI): Allthe instances are clustered using K-meansclustering. The instances in each clusterare considered nearest neighbors of eachother. The missing value is computed insimilar manner as in KNNI method.· Imputation with Fuzzy K-means Clustering(FKMI): After the fuzzy clustering of thedata set, missing values are computed asweighed sum of all centroids, using themembership function of each cluster as theweight.· Support Vector Machines Imputation(SVMI): SVM model is trained to predictmissing attributes from the completeinstances which do not have missingvalues. During testing,   missing values arepredicted using other attributes by settingmissing attribute as a class attribute whosevalue is intended to be predicted.· Singular Value Decomposition Imputation(SVDI): In this method, singular valuedecomposition is used to obtain a set ofmutually orthogonal expression patternsthat can be linearly combined toapproximate the values of all attributes inthe data set. In order to do that, first SVDIestimates the MVs within the expectedmaximization algorithm, and then itcomputes the singular valuedecomposition and obtains the eigenvalues. Now, SVDI can use the eigenvalues to apply a regression to thecomplete attributes of the instance and toobtain an estimation of the MV itself.· Local Least Squares Imputation (LLSI):This method identifies k similar instancesand fits a least squares model between theinstances and the known part of the recordwith missing values· Matrix Factorization: This method aims toget decompositions whose productapproximate the values of all attribute in",False,False,False,False,False,False
 ,False,False,False,False,False,False
which fill in missing values before using complete-,False,False,False,False,False,False
data methods. One advantage of imputation is that,False,False,False,False,False,False
the treatment of missing data is independent of the,False,False,False,False,False,False
"succeeding mining algorithm, and people can",False,False,False,False,False,False
select a suitable learning algorithm after,False,False,False,False,False,False
"imputation (Qin, Zhang, Zhu, Zhang, & Zhang,",False,False,False,False,False,False
2007).,False,False,False,False,False,False
"In our proposed HPM-MI, we have used",False,False,False,False,False,False
best proven MVI approach to fill the missing value,False,False,False,False,False,False
before pattern extraction and classification on the,False,False,False,False,False,False
data set. We have validated our model on three,False,False,False,False,False,False
benchmark data sets i.e. Pima Indian diabetes,False,False,False,False,False,False
Wisconsin breast cancer and Hepatitis data set,False,False,False,False,False,False
"from UCI repository (Newman, Hettich, Blake, &",False,False,False,False,False,False
"Merz, 2007) having 763, 16 and 167 missing",False,False,False,False,False,False
values respectively and two complete data sets,False,False,False,False,False,False
namely 2D synthetic data set as well as Wisconsin,False,False,False,False,False,False
Diagnostics Breast cancer (WDBC) in which,False,False,False,False,False,False
missing values were artificially induced.,False,False,False,False,False,False
2.1.1.2 Imputation Techniques,False,False,False,False,False,False
In order to analyse the impact of various MVI,False,False,False,False,False,False
"techniques (Luengo, García,  & Herrera, 2011;",False,False,False,False,False,False
"Koren, Bell, & Volinsky,2009; Takács,, Pilászy, &",False,False,False,False,False,False
"Németh, 2008), experimentation is done on the",False,False,False,False,False,False
following  techniques to choose the best possible,False,False,False,False,False,False
one to handle missing values present in the data,False,False,False,False,False,False
sets under study.The following approaches have,False,False,False,False,False,False
been empirically assessed  to find the missing,False,False,False,False,False,False
values:,False,False,False,False,False,False
· Case Deletion: The examples that have,False,False,False,False,False,False
any missing value in their attributes are,False,False,False,False,False,False
removed from the data set.,False,False,False,False,False,False
· Most Common Method (MC): Missing,False,False,False,False,False,False
values present in the data set is substituted,False,False,False,False,False,False
by mean value for numerical and mode for,False,False,False,False,False,False
nominal attributes.,False,False,False,False,False,False
· Concept Most Common (CMC): This,False,False,False,False,False,False
method calculates the missing values,False,False,False,False,False,False
similar to MC method but it considers only,False,False,False,False,False,False
the same class in which MV is missing.,False,False,False,False,False,False
"· K-Nearest Neighbor (KNNI): Firstly, this",False,False,False,False,False,False
method finds the k nearest neighbors and,False,False,False,False,False,False
"then, the most common value among all",False,False,False,False,False,False
"neighbors is taken for nominal attributes,",False,False,False,False,False,False
and the mean value is used for numerical,False,False,False,False,False,False
attributes.,False,False,False,False,False,False
· Weighted Imputation with K-Nearest,False,False,False,False,False,False
Neighbor (WKNN): This method,False,False,False,False,False,False
calculates the distance of each missing,False,False,False,False,False,False
value instances from its neighbors. This,False,False,False,False,False,False
distance is used to calculate the weight.,False,False,False,False,False,False
MV is computed by weighted mean for,False,False,False,False,False,False
numerical attributes. For nominal,False,False,False,False,False,False
"attributes, imputed value is the category",False,False,False,False,False,False
with highest weight.,False,False,False,False,False,False
· K-means Clustering Imputation (KMI): All,False,False,False,False,False,False
the instances are clustered using K-means,False,False,False,False,False,False
clustering. The instances in each cluster,False,False,False,False,False,False
are considered nearest neighbors of each,False,False,False,False,False,False
other. The missing value is computed in,False,False,False,False,False,False
similar manner as in KNNI method.,False,False,False,False,False,False
· Imputation with Fuzzy K-means Clustering,False,False,False,False,False,False
(FKMI): After the fuzzy clustering of the,False,False,False,False,False,False
"data set, missing values are computed as",False,False,False,False,False,False
"weighed sum of all centroids, using the",False,False,False,False,False,False
membership function of each cluster as the,False,False,False,False,False,False
weight.,False,False,False,False,False,False
· Support Vector Machines Imputation,False,False,False,False,False,False
(SVMI): SVM model is trained to predict,False,False,False,False,False,False
missing attributes from the complete,False,False,False,False,False,False
instances which do not have missing,False,False,False,False,False,False
"values. During testing,   missing values are",False,False,False,False,False,False
predicted using other attributes by setting,False,False,False,False,False,False
missing attribute as a class attribute whose,False,False,False,False,False,False
value is intended to be predicted.,False,False,False,False,False,False
· Singular Value Decomposition Imputation,False,False,False,False,False,False
"(SVDI): In this method, singular value",False,False,False,False,False,False
decomposition is used to obtain a set of,False,False,False,False,False,False
mutually orthogonal expression patterns,False,False,False,False,False,False
that can be linearly combined to,False,False,False,False,False,False
approximate the values of all attributes in,False,False,False,False,False,False
"the data set. In order to do that, first SVDI",False,False,False,False,False,False
estimates the MVs within the expected,False,False,False,False,False,False
"maximization algorithm, and then it",False,False,False,False,False,False
computes the singular value,False,False,False,False,False,False
decomposition and obtains the eigen,False,False,False,False,False,False
"values. Now, SVDI can use the eigen",False,False,False,False,False,False
values to apply a regression to the,False,False,False,False,False,False
complete attributes of the instance and to,False,False,False,False,False,False
obtain an estimation of the MV itself.,False,False,False,False,False,False
· Local Least Squares Imputation (LLSI):,False,False,False,False,False,False
This method identifies k similar instances,False,False,False,False,False,False
and fits a least squares model between the,False,False,False,False,False,False
instances and the known part of the record,False,False,False,False,False,False
with missing values,False,False,False,False,False,False
· Matrix Factorization: This method aims to,False,False,False,False,False,False
get decompositions whose product,False,False,False,False,False,False
approximate the values of all attribute in,False,False,False,False,False,False
" the data set. Then a missing value for agiven feature can be computed by the dotproduct of these vectors that correspondsto a given instance and feature.2.1.2 K-means ClusteringClustering methods partition data set intogroups so that the instances in one group aresimilar to each other, and as dissimilar as possiblefrom the objects in other groups. K-meansclustering is a partitioning algorithm that groupsthe n instances into k clusters (user defined inputparameter) so that intra-cluster similarity is highbut the inter-cluster similarity is low. Clustersimilarity is measured in regard to the mean valueof the instances in a cluster. K-means clusteringconsist of following steps (Kanungo, et al., 2002).Let  X = {x1,x2,x3,……..,xn} be the set of ninstances.1) Randomly select ‘k’ cluster centers as {v1,v2,…….,vk}.2) Calculate the distance between each instanceand cluster centers.3) Assign instance to the cluster center whosedistance from the cluster center is minimum of allthe cluster centers.4) Recalculate the new cluster center vi[1 ≤ i ≤ k ]for ith cluster using:=1where, ‘ci’ represents the number of data points inith cluster and xjis the data point in ith cluster [1 ≤ j≤ ci ]5) Recalculate the distance between each instanceand new obtained cluster centers.6) If no instance was reassigned then stop,otherwise repeat from step 3.In order to identify the correct patterns,clustering is applied from the given data set beforeclassification.2.1.3 Multilayer Perceptron (MLP) withbackpropogationMLP (Rumelhart, 1986) has been usedearlier for prediction of type-2 diabetes which isknown to be strong function approximation forprediction problems. MLP is a feed-forwardartificial neural network model that maps sets ofinput data onto a set of appropriate outputs. AMLP consists of multiple layers of nodes in adirected graph, with each layer fully connected tothe next one. Except for the input nodes, each nodeis a neuron (or processing element) with anonlinear activation function. MLP is amodification of the standard linear perceptron andcan distinguish data that are not linearly separable.It utilizes a supervised learning technique calledbackpropagation for training the network becauseit learns iteratively by processing data set oftraining examples, comparing the network’sprediction for each example with the actual knowntargets value known as class labels. For eachtraining example, the weights are modified so as tominimize the mean squared error between thenetwork prediction and the actual target value.These modifications are made in the backwarddirection.MLP has been used as a classifier in theproposed HPM-MI model for predictiveclassification due to its prominent characteristicssuch as high tolerance to noisy data as well as theirability to classify patterns on which they have notbeen trained (Han & Kamber, 2006).2.2 Review of Predictive classificationModelsIn present years, predictive classificationtechniques have been applied in medical diagnosissuccessfully. Over the last few years severalresearchers have shown the use of predictive datamining to infer clinically relevant models frompatient data and to provide decision support in themedical field. Various prediction models havebeen developed to support various medicaldecision making tasks such as prediction of breastcancer, diabetes,  liver and hepatitis disease(Luukka, 2011; Seera & Lim, 2014; Polat &G¨unes, 2006).Michie et. al. (Michie, 1994) have applied 22divergent algorithms to classify the diabeticpatients and accuracy of results varies from 67.6%to 77.7%. Adaptive Resonance Theory Map–instance counting (ARTMAP-IC)  produced 81%accuracy by testing 576 samples which were usedfor training dataset, and 192 samples which were           (1)",False,False,False,False,False,False
" the data set. Then a missing value for agiven feature can be computed by the dotproduct of these vectors that correspondsto a given instance and feature.2.1.2 K-means ClusteringClustering methods partition data set intogroups so that the instances in one group aresimilar to each other, and as dissimilar as possiblefrom the objects in other groups. K-meansclustering is a partitioning algorithm that groupsthe n instances into k clusters (user defined inputparameter) so that intra-cluster similarity is highbut the inter-cluster similarity is low. Clustersimilarity is measured in regard to the mean valueof the instances in a cluster. K-means clusteringconsist of following steps (Kanungo, et al., 2002).Let  X = {x1,x2,x3,……..,xn} be the set of ninstances.1) Randomly select ‘k’ cluster centers as {v1,v2,…….,vk}.2) Calculate the distance between each instanceand cluster centers.3) Assign instance to the cluster center whosedistance from the cluster center is minimum of allthe cluster centers.4) Recalculate the new cluster center vi[1 ≤ i ≤ k ]for ith cluster using:=1where, ‘ci’ represents the number of data points inith cluster and xjis the data point in ith cluster [1 ≤ j≤ ci ]5) Recalculate the distance between each instanceand new obtained cluster centers.6) If no instance was reassigned then stop,otherwise repeat from step 3.In order to identify the correct patterns,clustering is applied from the given data set beforeclassification.2.1.3 Multilayer Perceptron (MLP) withbackpropogationMLP (Rumelhart, 1986) has been usedearlier for prediction of type-2 diabetes which isknown to be strong function approximation forprediction problems. MLP is a feed-forwardartificial neural network model that maps sets ofinput data onto a set of appropriate outputs. AMLP consists of multiple layers of nodes in adirected graph, with each layer fully connected tothe next one. Except for the input nodes, each nodeis a neuron (or processing element) with anonlinear activation function. MLP is amodification of the standard linear perceptron andcan distinguish data that are not linearly separable.It utilizes a supervised learning technique calledbackpropagation for training the network becauseit learns iteratively by processing data set oftraining examples, comparing the network’sprediction for each example with the actual knowntargets value known as class labels. For eachtraining example, the weights are modified so as tominimize the mean squared error between thenetwork prediction and the actual target value.These modifications are made in the backwarddirection.MLP has been used as a classifier in theproposed HPM-MI model for predictiveclassification due to its prominent characteristicssuch as high tolerance to noisy data as well as theirability to classify patterns on which they have notbeen trained (Han & Kamber, 2006).2.2 Review of Predictive classificationModelsIn present years, predictive classificationtechniques have been applied in medical diagnosissuccessfully. Over the last few years severalresearchers have shown the use of predictive datamining to infer clinically relevant models frompatient data and to provide decision support in themedical field. Various prediction models havebeen developed to support various medicaldecision making tasks such as prediction of breastcancer, diabetes,  liver and hepatitis disease(Luukka, 2011; Seera & Lim, 2014; Polat &G¨unes, 2006).Michie et. al. (Michie, 1994) have applied 22divergent algorithms to classify the diabeticpatients and accuracy of results varies from 67.6%to 77.7%. Adaptive Resonance Theory Map–instance counting (ARTMAP-IC)  produced 81%accuracy by testing 576 samples which were usedfor training dataset, and 192 samples which were           (1)",False,False,False,False,False,False
 ,False,False,False,False,False,False
the data set. Then a missing value for a,False,False,False,False,False,False
given feature can be computed by the dot,False,False,False,False,False,False
product of these vectors that corresponds,False,False,False,False,False,False
to a given instance and feature.,False,False,False,False,False,False
2.1.2 K-means Clustering,False,False,False,False,False,False
Clustering methods partition data set into,False,False,False,False,False,False
groups so that the instances in one group are,False,False,False,False,False,False
"similar to each other, and as dissimilar as possible",False,False,False,False,False,False
from the objects in other groups. K-means,False,False,False,False,False,False
clustering is a partitioning algorithm that groups,False,False,False,False,False,False
the n instances into k clusters (user defined input,False,False,False,False,False,False
parameter) so that intra-cluster similarity is high,False,False,False,False,False,False
but the inter-cluster similarity is low. Cluster,False,False,False,False,False,False
similarity is measured in regard to the mean value,False,False,False,False,False,False
of the instances in a cluster. K-means clustering,False,False,False,False,False,False
"consist of following steps (Kanungo, et al., 2002).",False,False,False,False,False,False
Let  X = {x,False,False,False,False,False,False
1,False,False,False,False,False,False
",x",False,False,False,False,False,False
2,False,False,False,False,False,False
",x",False,False,False,False,False,False
3,False,False,False,False,False,False
",……..,x",False,False,False,False,False,False
n,False,False,False,False,False,False
} be the set of n,False,False,False,False,False,False
instances.,False,False,False,False,False,False
1) Randomly select ‘k’ cluster centers as {v,False,False,False,False,False,False
1,False,False,False,False,False,False
",",False,False,False,False,False,False
v,False,False,False,False,False,False
2,False,False,False,False,False,False
",…….,v",False,False,False,False,False,False
k,False,False,False,False,False,False
}.,False,False,False,False,False,False
2) Calculate the distance between each instance,False,False,False,False,False,False
and cluster centers.,False,False,False,False,False,False
3) Assign instance to the cluster center whose,False,False,False,False,False,False
distance from the cluster center is minimum of all,False,False,False,False,False,False
the cluster centers.,False,False,False,False,False,False
4) Recalculate the new cluster center v,False,False,False,False,False,False
i,False,False,False,False,False,False
[1 ≤ i ≤ k ],False,False,False,False,False,False
for i,False,False,False,False,False,False
th,False,False,False,False,False,False
 cluster using:,False,False,False,False,False,False
,False,False,False,False,False,False
,False,False,False,False,False,False
,False,False,False,False,False,False
,False,False,False,False,False,False
"where, ‘c",False,False,False,False,False,False
i,False,False,False,False,False,False
’ represents the number of data points in,False,False,False,False,False,False
i,False,False,False,False,False,False
th,False,False,False,False,False,False
 cluster and x,False,False,False,False,False,False
j,False,False,False,False,False,False
is the data point in i,False,False,False,False,False,False
th,False,False,False,False,False,False
 cluster [1 ≤ j,False,False,False,False,False,False
≤ c,False,False,False,False,False,False
i,False,False,False,False,False,False
 ],False,False,False,False,False,False
5) Recalculate the distance between each instance,False,False,False,False,False,False
and new obtained cluster centers.,False,False,False,False,False,False
"6) If no instance was reassigned then stop,",False,False,False,False,False,False
otherwise repeat from step 3.,False,False,False,False,False,False
"In order to identify the correct patterns,",False,False,False,False,False,False
clustering is applied from the given data set before,False,False,False,False,False,False
classification.,False,False,False,False,False,False
2.1.3 Multilayer Perceptron (MLP) with,False,False,False,False,False,False
backpropogation,False,False,False,False,False,False
"MLP (Rumelhart, 1986) has been used",False,False,False,False,False,False
earlier for prediction of type-2 diabetes which is,False,False,False,False,False,False
known to be strong function approximation for,False,False,False,False,False,False
prediction problems. MLP is a feed-forward,False,False,False,False,False,False
artificial neural network model that maps sets of,False,False,False,False,False,False
input data onto a set of appropriate outputs. A,False,False,False,False,False,False
MLP consists of multiple layers of nodes in a,False,False,False,False,False,False
"directed graph, with each layer fully connected to",False,False,False,False,False,False
"the next one. Except for the input nodes, each node",False,False,False,False,False,False
is a neuron (or processing element) with a,False,False,False,False,False,False
nonlinear activation function. MLP is a,False,False,False,False,False,False
modification of the standard linear perceptron and,False,False,False,False,False,False
can distinguish data that are not linearly separable.,False,False,False,False,False,False
It utilizes a supervised learning technique called,False,False,False,False,False,False
backpropagation for training the network because,False,False,False,False,False,False
it learns iteratively by processing data set of,False,False,False,False,False,False
"training examples, comparing the network’s",False,False,False,False,False,False
prediction for each example with the actual known,False,False,False,False,False,False
targets value known as class labels. For each,False,False,False,False,False,False
"training example, the weights are modified so as to",False,False,False,False,False,False
minimize the mean squared error between the,False,False,False,False,False,False
network prediction and the actual target value.,False,False,False,False,False,False
These modifications are made in the backward,False,False,False,False,False,False
direction.,False,False,False,False,False,False
MLP has been used as a classifier in the,False,False,False,False,False,False
proposed HPM-MI model for predictive,False,False,False,False,False,False
classification due to its prominent characteristics,False,False,False,False,False,False
such as high tolerance to noisy data as well as their,False,False,False,False,False,False
ability to classify patterns on which they have not,False,False,False,False,False,False
"been trained (Han & Kamber, 2006).",False,False,False,False,False,False
2.2 Review of Predictive classification,False,False,False,False,False,False
Models,False,False,False,False,False,False
"In present years, predictive classification",False,False,False,False,False,False
techniques have been applied in medical diagnosis,False,False,False,False,False,False
successfully. Over the last few years several,False,False,False,False,False,False
researchers have shown the use of predictive data,False,False,False,False,False,False
mining to infer clinically relevant models from,False,False,False,False,False,False
patient data and to provide decision support in the,False,False,False,False,False,False
medical field. Various prediction models have,False,False,False,False,False,False
been developed to support various medical,False,False,False,False,False,False
decision making tasks such as prediction of breast,False,False,False,False,False,False
"cancer, diabetes,  liver and hepatitis disease",False,False,False,False,False,False
"(Luukka, 2011; Seera & Lim, 2014; Polat &",False,False,False,False,False,False
"G¨unes, 2006).",False,False,False,False,False,False
"Michie et. al. (Michie, 1994) have applied 22",False,False,False,False,False,False
divergent algorithms to classify the diabetic,False,False,False,False,False,False
patients and accuracy of results varies from 67.6%,False,False,False,False,False,False
to 77.7%. Adaptive Resonance Theory Map–,False,False,False,False,False,False
instance counting (ARTMAP-IC)  produced 81%,False,False,False,False,False,False
accuracy by testing 576 samples which were used,False,False,False,False,False,False
"for training dataset, and 192 samples which were",False,False,False,False,False,False
" used as testing dataset (Carpenter & Markuzon,1998). Bioach et al.  removed the tuples where theattributes Plasma-Glucose level and Body massindex of patients were recorded as having zerovalue in their study (Bioch, 1996). They got anaccuracy of 75.4% and 79.5% using neuralnetwork and Bayesian approach respectively.Further, hybrid prediction models (HPM)(Kahramanli & Allahverdi, 2008; llango &Ramaraj, 2010; Patil, et al., 2010) by Patil et. al.,Kahramanli and Illango et al gave an accuracy of84.5%, 92.38% and 98.84%. Kahramanlideveloped hybrid system consisting of neuralnetwork and fuzzy neural network. The modelproposed by Patil et al. (Patil, et al., 2010) used thecombination of K-means clustering with decisiontree classifier. Illango had further increased theaccuracy by using F-Score feature selectionmethod, K-means clustering and support vectormachine (SVM). Recently Seera et. Al. proposed ahybrid intelligent system (Seera & Lim, 2014) thatconsists of Fuzzy Min max neural network, theclassification & regression tree, and RandomForest and compared their model with Lukka(Luukka, 2011) and Orkcu (Örkcü & Bal, 2011).Table 2 shows that Serra et al got the accuracy rateof 78.39 % and Lukka and Orkcu got accuracy of75.97 % and 77.60 % in their models respectively.Table 2 The values of accuracy of classification made on PimaIndian diabetes illness data ( Seera & Lim, 2014.MethodAccuracy(%)Sim75.29Sim + F175.84Sim + F275.97Binary-coded GA74.80BP73.80Real-coded GA77.60FMM69.28FMM-CART71.35FMM-CART-RF78.39A lot of classification techniques have also beenpropounded for the diagnosis of Wisconsin BreastCancer Data Set. Among these, C4.5 by Quinlan(Quinlan, 1996), LDA by dobinkar (Ster  &Dobinkar, 1996)  and SVM by (Bennett & Blue,1998)  were proposed with accuracy of 94.74%,96.8% and 97.2% respectively in the late nineties.Other developed models were NEFCLASS, FuzzyGA, LVQ, ANFIS, and PSO-SVM (Chen, & et al,2012). The most recent study has been done bySeera et. al. They have proposed FMM-CART-RFmodel and compared their model with othermethods (Seera & Lim, 2014). Results obtainedfrom their study are listed in Table 3. Accuracyrate shows that none of them have achievedaccuracy higher than 98.84 % for breast cancerdata set.Table 3The values of accuracy of classification made on BreastCancer data  (Seera & Lim, 2014).MethodAccuracy (%)BC FRPCA198.19BC Original97.49BC PCA97.72Sim 97.49Sim + F197.10Sim + F297.18Binary-coded GA94.00BP93.10Real-coded GA96.50AIRS97.20Fuzzy-AIRS98.51Cooperative96.69coevolutionDecompositìonal95.93Pedagogical97.07SVMs96.50FMM95.26FMM-CART94.86FMM-CART-RF98.84Many hybrid methods have also been proposedto deal with the automated diagnosis of hepatitisdisease problem. Polat and G¨unes (Polat &G¨unes, 2006) proposed a new diagnostic methodbased on a hybrid feature selection (FS)methodand artificial immune recognition system (AIRS)using fuzzy resource allocation mechanism. Theobtained classification accuracy of the proposedsystem was 92.59%. In Polat and Gunes (2007a,2007b), an artificial immune recognition system(AIRS) based on principal component analysis(PCA) was used for classification, the reportedaccuracy was up to 94.12%. In Dogantekin et al.(2009), an adaptive network based on fuzzyinference system combining with linear",False,False,False,False,False,False
" used as testing dataset (Carpenter & Markuzon,1998). Bioach et al.  removed the tuples where theattributes Plasma-Glucose level and Body massindex of patients were recorded as having zerovalue in their study (Bioch, 1996). They got anaccuracy of 75.4% and 79.5% using neuralnetwork and Bayesian approach respectively.Further, hybrid prediction models (HPM)(Kahramanli & Allahverdi, 2008; llango &Ramaraj, 2010; Patil, et al., 2010) by Patil et. al.,Kahramanli and Illango et al gave an accuracy of84.5%, 92.38% and 98.84%. Kahramanlideveloped hybrid system consisting of neuralnetwork and fuzzy neural network. The modelproposed by Patil et al. (Patil, et al., 2010) used thecombination of K-means clustering with decisiontree classifier. Illango had further increased theaccuracy by using F-Score feature selectionmethod, K-means clustering and support vectormachine (SVM). Recently Seera et. Al. proposed ahybrid intelligent system (Seera & Lim, 2014) thatconsists of Fuzzy Min max neural network, theclassification & regression tree, and RandomForest and compared their model with Lukka(Luukka, 2011) and Orkcu (Örkcü & Bal, 2011).Table 2 shows that Serra et al got the accuracy rateof 78.39 % and Lukka and Orkcu got accuracy of75.97 % and 77.60 % in their models respectively.Table 2 The values of accuracy of classification made on PimaIndian diabetes illness data ( Seera & Lim, 2014.MethodAccuracy(%)Sim75.29Sim + F175.84Sim + F275.97Binary-coded GA74.80BP73.80Real-coded GA77.60FMM69.28FMM-CART71.35FMM-CART-RF78.39A lot of classification techniques have also beenpropounded for the diagnosis of Wisconsin BreastCancer Data Set. Among these, C4.5 by Quinlan(Quinlan, 1996), LDA by dobinkar (Ster  &Dobinkar, 1996)  and SVM by (Bennett & Blue,1998)  were proposed with accuracy of 94.74%,96.8% and 97.2% respectively in the late nineties.Other developed models were NEFCLASS, FuzzyGA, LVQ, ANFIS, and PSO-SVM (Chen, & et al,2012). The most recent study has been done bySeera et. al. They have proposed FMM-CART-RFmodel and compared their model with othermethods (Seera & Lim, 2014). Results obtainedfrom their study are listed in Table 3. Accuracyrate shows that none of them have achievedaccuracy higher than 98.84 % for breast cancerdata set.Table 3The values of accuracy of classification made on BreastCancer data  (Seera & Lim, 2014).MethodAccuracy (%)BC FRPCA198.19BC Original97.49BC PCA97.72Sim 97.49Sim + F197.10Sim + F297.18Binary-coded GA94.00BP93.10Real-coded GA96.50AIRS97.20Fuzzy-AIRS98.51Cooperative96.69coevolutionDecompositìonal95.93Pedagogical97.07SVMs96.50FMM95.26FMM-CART94.86FMM-CART-RF98.84Many hybrid methods have also been proposedto deal with the automated diagnosis of hepatitisdisease problem. Polat and G¨unes (Polat &G¨unes, 2006) proposed a new diagnostic methodbased on a hybrid feature selection (FS)methodand artificial immune recognition system (AIRS)using fuzzy resource allocation mechanism. Theobtained classification accuracy of the proposedsystem was 92.59%. In Polat and Gunes (2007a,2007b), an artificial immune recognition system(AIRS) based on principal component analysis(PCA) was used for classification, the reportedaccuracy was up to 94.12%. In Dogantekin et al.(2009), an adaptive network based on fuzzyinference system combining with linear",False,False,False,False,False,False
 ,False,False,False,False,False,False
"used as testing dataset (Carpenter & Markuzon,",False,False,False,False,False,False
1998). Bioach et al.  removed the tuples where the,False,False,False,False,False,False
attributes Plasma-Glucose level and Body mass,False,False,False,False,False,False
index of patients were recorded as having zero,False,False,False,False,False,False
"value in their study (Bioch, 1996). They got an",False,False,False,False,False,False
accuracy of 75.4% and 79.5% using neural,False,False,False,False,False,False
network and Bayesian approach respectively.,False,False,False,False,False,False
"Further, hybrid prediction models (HPM)",False,False,False,False,False,False
"(Kahramanli & Allahverdi, 2008; llango &",False,False,False,False,False,False
"Ramaraj, 2010; Patil, et al., 2010) by Patil et. al.,",False,False,False,False,False,False
Kahramanli and Illango et al gave an accuracy of,False,False,False,False,False,False
"84.5%, 92.38% and 98.84%. Kahramanli",False,False,False,False,False,False
developed hybrid system consisting of neural,False,False,False,False,False,False
network and fuzzy neural network. The model,False,False,False,False,False,False
"proposed by Patil et al. (Patil, et al., 2010) used the",False,False,False,False,False,False
combination of K-means clustering with decision,False,False,False,False,False,False
tree classifier. Illango had further increased the,False,False,False,False,False,False
accuracy by using F-Score feature selection,False,False,False,False,False,False
"method, K-means clustering and support vector",False,False,False,False,False,False
machine (SVM). Recently Seera et. Al. proposed a,False,False,False,False,False,False
"hybrid intelligent system (Seera & Lim, 2014) that",False,False,False,False,False,False
"consists of Fuzzy Min max neural network, the",False,False,False,False,False,False
"classification & regression tree, and Random",False,False,False,False,False,False
Forest and compared their model with Lukka,False,False,False,False,False,False
"(Luukka, 2011) and Orkcu (Örkcü & Bal, 2011).",False,False,False,False,False,False
Table 2 shows that Serra et al got the accuracy rate,False,False,False,False,False,True
of 78.39 % and Lukka and Orkcu got accuracy of,False,False,False,False,False,False
75.97 % and 77.60 % in their models respectively.,False,False,False,False,False,False
 The values of accuracy of classification made on Pima,False,False,False,False,False,False
"Indian diabetes illness data ( Seera & Lim, 2014.",False,False,False,False,False,False
Method,False,False,False,False,False,False
Accuracy,False,False,False,False,False,False
(%),False,False,False,False,False,False
Sim,False,False,False,False,False,False
75.29,False,False,False,False,False,False
Sim + F1,False,False,False,False,False,False
75.84,False,False,False,False,False,False
Sim + F2,False,False,False,False,False,False
75.97,False,False,False,False,False,False
Binary,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
74.80,False,False,False,False,False,False
BP,True,False,False,True,False,False
73.80,False,False,False,False,False,False
Real,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
77.60,False,False,False,False,False,False
FMM,True,False,False,True,False,False
69.28,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
71.35,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
-,False,False,False,False,False,False
RF,True,False,False,True,False,False
78.39,False,False,False,False,False,False
A lot of classification techniques have also been,False,False,False,False,False,False
propounded for the diagnosis of Wisconsin Breast,False,False,False,False,False,False
"Cancer Data Set. Among these, C4.5 by Quinlan",False,False,False,False,False,False
"(Quinlan, 1996), LDA by dobinkar (Ster  &",False,False,False,False,False,False
"Dobinkar, 1996)  and SVM by (Bennett & Blue,",False,False,False,False,False,False
"1998)  were proposed with accuracy of 94.74%,",False,False,False,False,False,False
96.8% and 97.2% respectively in the late nineties.,False,False,False,False,False,False
"Other developed models were NEFCLASS, Fuzzy",False,False,False,False,False,False
"GA, LVQ, ANFIS, and PSO-SVM (Chen, & et al,",False,False,False,False,False,False
2012). The most recent study has been done by,False,False,False,False,False,False
Seera et. al. They have proposed FMM-CART-RF,False,False,False,False,False,False
model and compared their model with other,False,False,False,False,False,False
"methods (Seera & Lim, 2014). Results obtained",False,False,False,False,False,False
from their study are listed in Table 3. Accuracy,False,False,False,False,False,False
rate shows that none of them have achieved,False,False,False,False,False,False
accuracy higher than 98.84 % for breast cancer,False,False,False,False,False,False
data set.,False,False,False,False,False,False
The values of accuracy of classification made on Breast,False,False,False,False,False,False
"Cancer data  (Seera & Lim, 2014).",False,False,False,False,False,False
Method,False,False,False,False,False,False
Accuracy (%),False,False,False,False,False,False
BC FRPCA1,True,False,False,True,False,False
98.19,False,False,False,False,False,False
BC Original,False,False,False,False,False,False
97.49,False,False,False,False,False,False
BC PCA,True,False,False,True,False,False
97.72,False,False,False,False,False,False
Sim 97.49,False,False,False,False,False,False
Sim + F1,False,False,False,False,False,False
97.10,False,False,False,False,False,False
Sim + F2,False,False,False,False,False,False
97.18,False,False,False,False,False,False
Binary,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
94.00,False,False,False,False,False,False
BP,True,False,False,True,False,False
93.10,False,False,False,False,False,False
Real,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
96.50,False,False,False,False,False,False
AIRS,True,False,False,True,False,False
97.20,False,False,False,False,False,False
Fuzzy,False,False,False,False,False,False
-,False,False,False,False,False,False
AIRS,True,False,False,True,False,False
98.51,False,False,False,False,False,False
Cooperative,False,False,False,False,False,False
96.69,False,False,False,False,False,False
coevolution,False,False,False,False,False,False
Decompositìonal,False,False,False,False,False,False
95.93,False,False,False,False,False,False
Pedagogical,False,False,False,False,False,False
97.07,False,False,False,False,False,False
SVMs,False,False,False,False,False,False
96.50,False,False,False,False,False,False
FMM,True,False,False,True,False,False
95.26,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
94.86,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
-,False,False,False,False,False,False
RF,True,False,False,True,False,False
98.84,False,False,False,False,False,False
Many hybrid methods have also been proposed,False,False,False,False,False,False
to deal with the automated diagnosis of hepatitis,False,False,False,False,False,False
disease problem. Polat and G¨unes (Polat &,False,False,False,False,False,False
"G¨unes, 2006) proposed a new diagnostic method",False,False,False,False,False,False
based on a hybrid feature selection (FS)method,False,False,False,False,False,False
and artificial immune recognition system (AIRS),False,False,False,False,False,False
using fuzzy resource allocation mechanism. The,False,False,False,False,False,False
obtained classification accuracy of the proposed,False,False,False,False,False,False
"system was 92.59%. In Polat and Gunes (2007a,",False,False,False,False,False,False
"2007b), an artificial immune recognition system",False,False,False,False,False,False
(AIRS) based on principal component analysis,False,False,False,False,False,False
"(PCA) was used for classification, the reported",False,False,False,False,False,False
accuracy was up to 94.12%. In Dogantekin et al.,False,False,False,False,False,False
"(2009), an adaptive network based on fuzzy",False,False,False,False,False,False
inference system combining with linear,False,False,False,False,False,False
" discriminant analysis (LDA-ANFIS) was appliedfor automatic hepatitis diagnosis, and an accuracyof 94.16% was obtained. Chen et. al. (2011)developed a hybrid model using local fisherdiscriminant analysis (LFDA) and support vectormachine and achieved an accuracy of 96.77%.Recently, Zangooei et. al. have developed SupportVector Regression(SVR) based classificationmodel (Zangooei, Habibi, & Alizadehsani, 2014)where its parameter values were optimized byNon-dominated Sorting Genetic Algorithm-II(NSGA-II) and   compared with other modelsResults obtained from their study are listed inTable 4. Accuracy rate shows that none of themhave achieved accuracy higher than 98.52% forhepatitis data set.Table 4The values of accuracy of classification made onhepatitis data ( Zangooei , Habibi, & lizadehsani,2014).MethodAccuracy(%)C4.583.60BNND90.00Weighted9NN92.90FSM without rotations88.40LDA86.40FS-AIRS with fuzzy res92.50FS-fuzzy-AIRS94.10PCA–LSSVM95.00GA-SVM89.60LDA-ANFIS94.10J4885.67MLP90.52SVR NSGA-II98.52Although all these work have demonstratedpromising prediction accurately, majority of papershave concentrated on how to increase accuracy oftheir models .Presence of missing values thataffects the data quality and accuracy of miningresults have not been addressed properly in theirwork.Patil, et. al. (2010) used case deletion andSeera & Lim (2014) have taken the default methodto handle the missing values in the data set.Research shows that other imputation techniquesperform better as compared to case deletion anddefault methods, if the missing values are large innumber. Hence this paper proposes a novel HPM-MI with the goal of improvisation data quality aswell as accuracy. Further, it compares results ofproposed model with other results reviewed inTable 2, Table 3 and Table 4.3. Proposed SystemIn this paper, a Hybrid Prediction Model withMissing value Imputation (HPM-MI) is developedwhich is shown in Fig. 1 to deal with predictiveclassification problem of medical patients. Thismodel comprises of three stages namely analysisand selection of Imputation method using K-meansFig. 1 Hybrid Prediction Model with Missing ValueImputationData SetApply 11 Missing valueImputation MethodsUnsupervised learning usingsimple K-meansclusteringEvaluation of MV approachesusing cluster validation andchoose the best imputed data setSupervised learning(Multilayer Perceptron)Pattern ExtractionPerformance evaluationand conclusionCluster 1 Cluster 2",False,False,False,False,False,False
" discriminant analysis (LDA-ANFIS) was appliedfor automatic hepatitis diagnosis, and an accuracyof 94.16% was obtained. Chen et. al. (2011)developed a hybrid model using local fisherdiscriminant analysis (LFDA) and support vectormachine and achieved an accuracy of 96.77%.Recently, Zangooei et. al. have developed SupportVector Regression(SVR) based classificationmodel (Zangooei, Habibi, & Alizadehsani, 2014)where its parameter values were optimized byNon-dominated Sorting Genetic Algorithm-II(NSGA-II) and   compared with other modelsResults obtained from their study are listed inTable 4. Accuracy rate shows that none of themhave achieved accuracy higher than 98.52% forhepatitis data set.Table 4The values of accuracy of classification made onhepatitis data ( Zangooei , Habibi, & lizadehsani,2014).MethodAccuracy(%)C4.583.60BNND90.00Weighted9NN92.90FSM without rotations88.40LDA86.40FS-AIRS with fuzzy res92.50FS-fuzzy-AIRS94.10PCA–LSSVM95.00GA-SVM89.60LDA-ANFIS94.10J4885.67MLP90.52SVR NSGA-II98.52Although all these work have demonstratedpromising prediction accurately, majority of papershave concentrated on how to increase accuracy oftheir models .Presence of missing values thataffects the data quality and accuracy of miningresults have not been addressed properly in theirwork.Patil, et. al. (2010) used case deletion andSeera & Lim (2014) have taken the default methodto handle the missing values in the data set.Research shows that other imputation techniquesperform better as compared to case deletion anddefault methods, if the missing values are large innumber. Hence this paper proposes a novel HPM-MI with the goal of improvisation data quality aswell as accuracy. Further, it compares results ofproposed model with other results reviewed inTable 2, Table 3 and Table 4.3. Proposed SystemIn this paper, a Hybrid Prediction Model withMissing value Imputation (HPM-MI) is developedwhich is shown in Fig. 1 to deal with predictiveclassification problem of medical patients. Thismodel comprises of three stages namely analysisand selection of Imputation method using K-meansFig. 1 Hybrid Prediction Model with Missing ValueImputationData SetApply 11 Missing valueImputation MethodsUnsupervised learning usingsimple K-meansclusteringEvaluation of MV approachesusing cluster validation andchoose the best imputed data setSupervised learning(Multilayer Perceptron)Pattern ExtractionPerformance evaluationand conclusionCluster 1 Cluster 2",False,False,False,False,False,False
 ,False,False,False,False,False,False
discriminant analysis (LDA-ANFIS) was applied,False,False,False,False,False,False
"for automatic hepatitis diagnosis, and an accuracy",False,False,False,False,False,False
of 94.16% was obtained. Chen et. al. (2011),False,False,False,False,False,False
developed a hybrid model using local fisher,False,False,False,False,False,False
discriminant analysis (LFDA) and support vector,False,False,False,False,False,False
machine and achieved an accuracy of 96.77%.,False,False,False,False,False,False
"Recently, Zangooei et. al. have developed Support",False,False,False,False,False,False
Vector Regression(SVR) based classification,False,False,False,False,False,False
"model (Zangooei, Habibi, & Alizadehsani, 2014)",False,False,False,False,False,False
where its parameter values were optimized by,False,False,False,False,False,False
Non-dominated Sorting Genetic Algorithm-II,False,False,False,False,False,False
(NSGA-II) and   compared with other models,False,False,False,False,False,False
Results obtained from their study are listed in,False,False,False,False,False,False
Table 4. Accuracy rate shows that none of them,False,False,False,False,False,True
have achieved accuracy higher than 98.52% for,False,False,False,False,False,False
hepatitis data set.,False,False,False,False,False,False
The values of accuracy of classification made on,False,False,False,False,False,False
2014).,False,False,False,False,False,False
Method,False,False,False,False,False,False
Accuracy,False,False,False,False,False,False
(%),False,False,False,False,False,False
C4.5,True,False,False,True,False,False
83.60,False,False,False,False,False,False
BNND,True,False,False,True,False,False
90.00,False,False,False,False,False,False
Weighted9NN,False,False,False,False,False,False
92.90,False,False,False,False,False,False
FSM without rotations,False,False,False,False,False,False
88.40,False,False,False,False,False,False
LDA,True,False,False,True,False,False
86.40,False,False,False,False,False,False
FS,True,False,False,True,False,False
-,False,False,False,False,False,False
AIRS with fuzzy res,False,False,False,False,False,False
92.50,False,False,False,False,False,False
FS,True,False,False,True,False,False
-,False,False,False,False,False,False
fuzzy,False,False,False,False,False,False
-,False,False,False,False,False,False
AIRS,True,False,False,True,False,False
94.10,False,False,False,False,False,False
PCA,True,False,False,True,False,False
–,False,False,False,False,False,False
LSSVM,True,False,False,True,False,False
95.00,False,False,False,False,False,False
GA,True,False,False,True,False,False
-,False,False,False,False,False,False
SVM,True,False,False,True,False,False
89.60,False,False,False,False,False,False
LDA,True,False,False,True,False,False
-,False,False,False,False,False,False
ANFIS,True,False,False,True,False,False
94.10,False,False,False,False,False,False
J48,True,False,False,True,False,False
85.67,False,False,False,False,False,False
MLP,True,False,False,True,False,False
90.52,False,False,False,False,False,False
SVR NSGA,True,False,False,True,False,False
-,False,False,False,False,False,False
II,True,False,False,True,False,False
98.52,False,False,False,False,False,False
Although all these work have demonstrated,False,False,False,False,False,False
"promising prediction accurately, majority of papers",False,False,False,False,False,False
have concentrated on how to increase accuracy of,False,False,False,False,False,False
their models .Presence of missing values that,False,False,False,False,False,False
affects the data quality and accuracy of mining,False,False,False,False,False,False
results have not been addressed properly in their,False,False,False,False,False,False
"work.Patil, et. al. (2010) used case deletion and",False,False,False,False,False,False
Seera & Lim (2014) have taken the default method,False,False,False,False,False,False
to handle the missing values in the data set.,False,False,False,False,False,False
Research shows that other imputation techniques,False,False,False,False,False,False
perform better as compared to case deletion and,False,False,False,False,False,False
"default methods, if the missing values are large in",False,False,False,False,False,False
number. Hence this paper proposes a novel HPM-,False,False,False,False,False,False
MI with the goal of improvisation data quality as,False,False,False,False,False,False
"well as accuracy. Further, it compares results of",False,False,False,False,False,False
proposed model with other results reviewed in,False,False,False,False,False,False
"Table 2, Table 3 and Table 4.",False,False,False,False,False,True
3. Proposed System,False,False,True,True,False,False
"In this paper, a Hybrid Prediction Model with",False,False,False,False,False,False
Missing value Imputation (HPM-MI) is developed,False,False,False,False,False,False
which is shown in Fig. 1 to deal with predictive,False,False,False,False,False,False
classification problem of medical patients. This,False,False,False,False,False,False
model comprises of three stages namely analysis,False,False,False,False,False,False
and selection of Imputation method using K-means,False,False,False,False,False,False
Imputation,False,False,False,False,False,False
" clustering, pattern extraction and MultilayerPerceptron with back propagation as trainingalgorithm. Fig 2 shows procedure of the proposedmodel and following subsections describe thedetailed study.3.1 Missing Value ImputationIn this paper, we have analyzed 11approaches to fill missing values in incompletedata set in order to improve the quality of data.Detailed discussion of these approaches is alreadyprovided in section 2.1.1MVI is a crucial step incorporated in ourproposed (HPM-MI) model .The objective of thisstep is to find the best imputation technique withrespect to incomplete data set under study. As nogeneralization can be made regarding bestimputation method. So we need to analyzeempirically all of 11 MVI approaches under studyfor a given data set. Selection of best MVI methodfor incomplete data set is based on accuracyachieved (ground truth) after applying mining task.For our proposed model, we have chosenclustering as basis for selection of best imputationtechnique. Imputation technique which hasproduced more compact clusters is chosen toimpute missing values in the data set. This imputeddata set is used for pattern extraction while othersare discarded.Simple K-means clustering algorithm isapplied on each imputed data set. Clusteringresults produced by each imputed data set arevalidated through their actual classes which areknown. As a result, performance of imputationtechniques applied on data sets under study ismeasured using incorrectly classified instancesproduced by each imputed data set shown in Table5, Table 6 and Table 7 for diabetes, breast cancer,and hepatitis data sets respectively.Concept Most Common (CMC) methodhas given minimum number of incorrectlyclassified instances in case of diabetes data as wellas hepatitis data set. Case Deletion has performedbest in case of Wisconsin Breast Cancer data.Therefore, CMC has been chosen to handle themissing values for diabetes data set as well ashepatitis data set and case deletion is selected forbreast Cancer data set.Table 5.Clustering results with 11 imputation methods for PimaIndian diabetes data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC25.781.For a given data set D (having missing values) under study.2. Obtain the imputed data sets i.e. D1, D2, D3….. , and D11after employing 11 missing valuesimputation approaches on data set D.3. Use simple K-means clustering on D1, D2, D3….. , and D11obtained from step 2.4. Validate the clustering results obtained from 11 experiments using their actual classes.5. Choose imputed data set that has produced least number of incorrectly classified instances anddiscard other 10 imputed data sets.6. Extract the instances from chosen imputed data set using correctly classified instances as a resultof K-means clustering.7. Classify the extracted data using Multilayer Perceptron Model.8. Evaluate the Performance of above model using accuracy, specificity, sensitivity, kappa and ROC.9. Compare this model with previous models.Fig. 2 Steps for developing HPM-MI model",False,False,False,False,False,False
" clustering, pattern extraction and MultilayerPerceptron with back propagation as trainingalgorithm. Fig 2 shows procedure of the proposedmodel and following subsections describe thedetailed study.3.1 Missing Value ImputationIn this paper, we have analyzed 11approaches to fill missing values in incompletedata set in order to improve the quality of data.Detailed discussion of these approaches is alreadyprovided in section 2.1.1MVI is a crucial step incorporated in ourproposed (HPM-MI) model .The objective of thisstep is to find the best imputation technique withrespect to incomplete data set under study. As nogeneralization can be made regarding bestimputation method. So we need to analyzeempirically all of 11 MVI approaches under studyfor a given data set. Selection of best MVI methodfor incomplete data set is based on accuracyachieved (ground truth) after applying mining task.For our proposed model, we have chosenclustering as basis for selection of best imputationtechnique. Imputation technique which hasproduced more compact clusters is chosen toimpute missing values in the data set. This imputeddata set is used for pattern extraction while othersare discarded.Simple K-means clustering algorithm isapplied on each imputed data set. Clusteringresults produced by each imputed data set arevalidated through their actual classes which areknown. As a result, performance of imputationtechniques applied on data sets under study ismeasured using incorrectly classified instancesproduced by each imputed data set shown in Table5, Table 6 and Table 7 for diabetes, breast cancer,and hepatitis data sets respectively.Concept Most Common (CMC) methodhas given minimum number of incorrectlyclassified instances in case of diabetes data as wellas hepatitis data set. Case Deletion has performedbest in case of Wisconsin Breast Cancer data.Therefore, CMC has been chosen to handle themissing values for diabetes data set as well ashepatitis data set and case deletion is selected forbreast Cancer data set.Table 5.Clustering results with 11 imputation methods for PimaIndian diabetes data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC25.781.For a given data set D (having missing values) under study.2. Obtain the imputed data sets i.e. D1, D2, D3….. , and D11after employing 11 missing valuesimputation approaches on data set D.3. Use simple K-means clustering on D1, D2, D3….. , and D11obtained from step 2.4. Validate the clustering results obtained from 11 experiments using their actual classes.5. Choose imputed data set that has produced least number of incorrectly classified instances anddiscard other 10 imputed data sets.6. Extract the instances from chosen imputed data set using correctly classified instances as a resultof K-means clustering.7. Classify the extracted data using Multilayer Perceptron Model.8. Evaluate the Performance of above model using accuracy, specificity, sensitivity, kappa and ROC.9. Compare this model with previous models.Fig. 2 Steps for developing HPM-MI model",False,False,False,False,False,False
 ,False,False,False,False,False,False
"clustering, pattern extraction and Multilayer",False,False,False,False,False,False
Perceptron with back propagation as training,False,False,False,False,False,False
algorithm. Fig 2 shows procedure of the proposed,False,False,False,False,True,False
model and following subsections describe the,False,False,False,False,False,False
detailed study.,False,False,False,False,False,False
3.1 Missing Value Imputation,False,False,False,False,False,False
"In this paper, we have analyzed 11",False,False,False,False,False,False
approaches to fill missing values in incomplete,False,False,False,False,False,False
data set in order to improve the quality of data.,False,False,False,False,False,False
Detailed discussion of these approaches is already,False,False,False,False,False,False
provided in section 2.1.1,False,False,False,False,False,False
MVI is a crucial step incorporated in our,False,False,False,False,False,False
proposed (HPM-MI) model .The objective of this,False,False,False,False,False,False
step is to find the best imputation technique with,False,False,False,False,False,False
respect to incomplete data set under study. As no,False,False,False,False,False,False
generalization can be made regarding best,False,False,False,False,False,False
imputation method. So we need to analyze,False,False,False,False,False,False
empirically all of 11 MVI approaches under study,False,False,False,False,False,False
for a given data set. Selection of best MVI method,False,False,False,False,False,False
for incomplete data set is based on accuracy,False,False,False,False,False,False
achieved (ground truth) after applying mining task.,False,False,False,False,False,False
"For our proposed model, we have chosen",False,False,False,False,False,False
clustering as basis for selection of best imputation,False,False,False,False,False,False
technique. Imputation technique which has,False,False,False,False,False,False
produced more compact clusters is chosen to,False,False,False,False,False,False
impute missing values in the data set. This imputed,False,False,False,False,False,False
data set is used for pattern extraction while others,False,False,False,False,False,False
are discarded.,False,False,False,False,False,False
Simple K-means clustering algorithm is,False,False,False,False,False,False
applied on each imputed data set. Clustering,False,False,False,False,False,False
results produced by each imputed data set are,False,False,False,False,False,False
validated through their actual classes which are,False,False,False,False,False,False
"known. As a result, performance of imputation",False,False,False,False,False,False
techniques applied on data sets under study is,False,False,False,False,False,False
measured using incorrectly classified instances,False,False,False,False,False,False
produced by each imputed data set shown in Table,False,False,False,False,False,False
"5, Table 6 and Table 7 for diabetes, breast cancer,",False,False,False,False,False,True
and hepatitis data sets respectively.,False,False,False,False,False,False
Concept Most Common (CMC) method,False,False,False,False,False,False
has given minimum number of incorrectly,False,False,False,False,False,False
classified instances in case of diabetes data as well,False,False,False,False,False,False
as hepatitis data set. Case Deletion has performed,False,False,False,False,False,False
best in case of Wisconsin Breast Cancer data.,False,False,False,False,False,False
"Therefore, CMC has been chosen to handle the",False,False,False,False,False,False
missing values for diabetes data set as well as,False,False,False,False,False,False
hepatitis data set and case deletion is selected for,False,False,False,False,False,False
Clustering results with 11 imputation methods for Pima,False,False,False,False,False,False
Indian diabetes data.,False,False,False,False,False,False
1,False,False,False,False,False,False
For a given data set D (having missing values) under study.,False,False,False,False,False,False
2. Obtain the imputed data sets i.e. D,False,False,True,True,False,False
 D,True,False,False,True,False,False
 D,True,False,False,True,False,False
" , and D",False,False,False,False,False,False
after employing 11 missing values,False,False,False,False,False,False
imputation approaches on data set D.,False,False,False,False,False,False
3. Use simple K-means clustering on D,False,False,True,True,False,False
 D,True,False,False,True,False,False
 D,True,False,False,True,False,False
" , and D",False,False,False,False,False,False
obtained from step 2.,False,False,False,False,False,False
4. Validate the clustering results obtained from 11 experiments using their actual classes.,False,False,True,True,False,False
5. Choose imputed data set that has produced least number of incorrectly classified instances and,False,False,True,True,False,False
discard other 10 imputed data sets.,False,False,False,False,False,False
of K-means clustering.,False,False,False,False,False,False
7. Classify the extracted data using Multilayer Perceptron Model.,False,False,True,True,False,False
"8. Evaluate the Performance of above model using accuracy, specificity, sensitivity, kappa and ROC.",False,False,True,True,False,False
9. Compare this model with previous models.,False,False,True,True,False,False
" 2FKMI31.773KMI26.564KNNI27.995LSSI31.646MC32.427SVDI44.798SVMI31.519WKNN27.7310Case Deletion30.7211Matrix Factorization27.60Table 6 Clustering results with 11 imputation methods forWisconsin Breast Cancer Data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC4.002FKMI4.003KMI4.004KNNI4.005LSSI4.006MC4.157SVDI4.298SVMI4.009WKNN4.0010Case Deletion3.9511Matrix Factorization4.15Table 7Clustering results with 11 imputation methods forhepatitis data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC29.682F KMI30.323KMI30.324KNNI29.685LSSI29.686MC49.037SVDI30.978SVMI29.689WKNN29.6810Case Deletion37.5011MatrixFactorization33.553.2 Pattern ExtractionWe have obtained wrongly classifiedinstances, as shown in Table 8 for the data setsunder study after cluster validation done inprevious step. These instances were eliminatedfrom best imputed data set chosen for diabetes,breast Cancer, and hepatitis data sets beforeapplying classification.3.3 Prediction using MLP (MultilayerPerceptron)It has been proved that MLP with singlehidden layer is able to accurately approximatecontinuous functions (Cibenko, 1989). MLPnetwork architecture built by Weka as shown inFig. 3 is used in my proposed model as a classifier.It consists of an input layer, one hidden layer, andan output layer. Each layer is made up of neurons.The number of neurons in input layers is same asthe number of attributes or features present in eachtraining example. The attributes are fedsimultaneously into the neurons making up theinput layer. The output of input layer is calculatedusing sigmoidal function and in turn, fedsimultaneously to a second layer known as hiddenlayer. The outputs of the hidden layer are the inputto output layer. This output layer shows thenetwork’s prediction for given instances of data setas “yes” and “no” for binary classification problemMLP uses backpropagation algorithm to train thenetwork.Pima diabetes data setBreastCancer data setHepatitis data setClusterattribute(clusters)SamplesIncorrectlyclassifiedClusterattribute(clusters)SamplesIncorrectlyclassifiedClusterattribute(clusters)SamplesIncorrectly classifiedCluster 1( yes)Cluster 0( no )768198Cluster 1(benign)Cluster 0(malignant)68327Cluster 1(die)Cluster 0(live)15546Table 8Clustered Instances",False,False,False,False,False,False
" 2FKMI31.773KMI26.564KNNI27.995LSSI31.646MC32.427SVDI44.798SVMI31.519WKNN27.7310Case Deletion30.7211Matrix Factorization27.60Table 6 Clustering results with 11 imputation methods forWisconsin Breast Cancer Data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC4.002FKMI4.003KMI4.004KNNI4.005LSSI4.006MC4.157SVDI4.298SVMI4.009WKNN4.0010Case Deletion3.9511Matrix Factorization4.15Table 7Clustering results with 11 imputation methods forhepatitis data.S.NoImputationMethodIncorrectly classifiedinstances (%)1CMC29.682F KMI30.323KMI30.324KNNI29.685LSSI29.686MC49.037SVDI30.978SVMI29.689WKNN29.6810Case Deletion37.5011MatrixFactorization33.553.2 Pattern ExtractionWe have obtained wrongly classifiedinstances, as shown in Table 8 for the data setsunder study after cluster validation done inprevious step. These instances were eliminatedfrom best imputed data set chosen for diabetes,breast Cancer, and hepatitis data sets beforeapplying classification.3.3 Prediction using MLP (MultilayerPerceptron)It has been proved that MLP with singlehidden layer is able to accurately approximatecontinuous functions (Cibenko, 1989). MLPnetwork architecture built by Weka as shown inFig. 3 is used in my proposed model as a classifier.It consists of an input layer, one hidden layer, andan output layer. Each layer is made up of neurons.The number of neurons in input layers is same asthe number of attributes or features present in eachtraining example. The attributes are fedsimultaneously into the neurons making up theinput layer. The output of input layer is calculatedusing sigmoidal function and in turn, fedsimultaneously to a second layer known as hiddenlayer. The outputs of the hidden layer are the inputto output layer. This output layer shows thenetwork’s prediction for given instances of data setas “yes” and “no” for binary classification problemMLP uses backpropagation algorithm to train thenetwork.Pima diabetes data setBreastCancer data setHepatitis data setClusterattribute(clusters)SamplesIncorrectlyclassifiedClusterattribute(clusters)SamplesIncorrectlyclassifiedClusterattribute(clusters)SamplesIncorrectly classifiedCluster 1( yes)Cluster 0( no )768198Cluster 1(benign)Cluster 0(malignant)68327Cluster 1(die)Cluster 0(live)15546Table 8Clustered Instances",False,False,False,False,False,False
 ,False,False,False,False,False,False
2,False,False,False,False,False,False
FKMI,True,False,False,True,False,False
31.77,False,False,False,False,False,False
3,False,False,False,False,False,False
KMI,True,False,False,True,False,False
26.56,False,False,False,False,False,False
4,False,False,False,False,False,False
KNNI,True,False,False,True,False,False
27.99,False,False,False,False,False,False
5,False,False,False,False,False,False
LSSI,True,False,False,True,False,False
31.64,False,False,False,False,False,False
6,False,False,False,False,False,False
MC,True,False,False,True,False,False
32.42,False,False,False,False,False,False
7,False,False,False,False,False,False
SVDI,True,False,False,True,False,False
44.79,False,False,False,False,False,False
8,False,False,False,False,False,False
SVMI,True,False,False,True,False,False
31.51,False,False,False,False,False,False
9,False,False,False,False,False,False
WKNN,True,False,False,True,False,False
27.73,False,False,False,False,False,False
10,False,False,False,False,False,False
Case Deletion,False,False,False,False,False,False
30.72,False,False,False,False,False,False
11,False,False,False,False,False,False
Matrix Factorization,False,False,False,False,False,False
27.60,False,False,False,False,False,False
 Clustering results with 11 imputation methods for,False,False,False,False,False,False
Wisconsin Breast Cancer Data.,False,False,False,False,False,False
1,False,False,False,False,False,False
CMC,True,False,False,True,False,False
4.00,False,False,False,False,False,False
2,False,False,False,False,False,False
FKMI,True,False,False,True,False,False
4.00,False,False,False,False,False,False
3,False,False,False,False,False,False
KMI,True,False,False,True,False,False
4.00,False,False,False,False,False,False
4,False,False,False,False,False,False
KNNI,True,False,False,True,False,False
4.00,False,False,False,False,False,False
5,False,False,False,False,False,False
LSSI,True,False,False,True,False,False
4.00,False,False,False,False,False,False
6,False,False,False,False,False,False
MC,True,False,False,True,False,False
4.15,False,False,False,False,False,False
7,False,False,False,False,False,False
SVDI,True,False,False,True,False,False
4.29,False,False,False,False,False,False
8,False,False,False,False,False,False
SVMI,True,False,False,True,False,False
4.00,False,False,False,False,False,False
9,False,False,False,False,False,False
WKNN,True,False,False,True,False,False
4.00,False,False,False,False,False,False
10,False,False,False,False,False,False
11,False,False,False,False,False,False
Matrix Factorization,False,False,False,False,False,False
4.15,False,False,False,False,False,False
Clustering results with 11 imputation methods for,False,False,False,False,False,False
1,False,False,False,False,False,False
2,False,False,False,False,False,False
F KMI,True,False,False,True,False,False
30.32,False,False,False,False,False,False
3,False,False,False,False,False,False
KMI,True,False,False,True,False,False
30.32,False,False,False,False,False,False
4,False,False,False,False,False,False
KNNI,True,False,False,True,False,False
29.68,False,False,False,False,False,False
5,False,False,False,False,False,False
LSSI,True,False,False,True,False,False
29.68,False,False,False,False,False,False
6,False,False,False,False,False,False
MC,True,False,False,True,False,False
49.03,False,False,False,False,False,False
7,False,False,False,False,False,False
SVDI,True,False,False,True,False,False
30.97,False,False,False,False,False,False
8,False,False,False,False,False,False
SVMI,True,False,False,True,False,False
29.68,False,False,False,False,False,False
9,False,False,False,False,False,False
WKNN,True,False,False,True,False,False
29.68,False,False,False,False,False,False
10,False,False,False,False,False,False
Case Deletion,False,False,False,False,False,False
37.50,False,False,False,False,False,False
11,False,False,False,False,False,False
Matrix,False,False,False,False,False,False
Factorization,False,False,False,False,False,False
33.55,False,False,False,False,False,False
We have obtained wrongly classified,False,False,False,False,False,False
"instances, as shown in Table 8 for the data sets",False,False,False,False,False,False
under study after cluster validation done in,False,False,False,False,False,False
previous step. These instances were eliminated,False,False,False,False,False,False
"from best imputed data set chosen for diabetes,",False,False,False,False,False,False
"breast Cancer, and hepatitis data sets before",False,False,False,False,False,False
applying classification.,False,False,False,False,False,False
3.3 Prediction using MLP (Multilayer,False,False,False,False,False,False
Perceptron),False,False,False,False,False,False
It has been proved that MLP with single,False,False,False,False,False,False
hidden layer is able to accurately approximate,False,False,False,False,False,False
"continuous functions (Cibenko, 1989). MLP",False,False,False,False,False,False
network architecture built by Weka as shown in,False,False,False,False,False,False
Fig. 3 is used in my proposed model as a classifier.,False,False,False,False,True,False
"It consists of an input layer, one hidden layer, and",False,False,False,False,False,False
an output layer. Each layer is made up of neurons.,False,False,False,False,False,False
The number of neurons in input layers is same as,False,False,False,False,False,False
the number of attributes or features present in each,False,False,False,False,False,False
training example. The attributes are fed,False,False,False,False,False,False
simultaneously into the neurons making up the,False,False,False,False,False,False
input layer. The output of input layer is calculated,False,False,False,False,False,False
"using sigmoidal function and in turn, fed",False,False,False,False,False,False
simultaneously to a second layer known as hidden,False,False,False,False,False,False
layer. The outputs of the hidden layer are the input,False,False,False,False,False,False
to output layer. This output layer shows the,False,False,False,False,False,False
network’s prediction for given instances of data set,False,False,False,False,False,False
as “yes” and “no” for binary classification problem,False,False,False,False,False,False
MLP uses backpropagation algorithm to train the,False,False,False,False,False,False
network.,False,False,False,False,False,False
Cluster 1,False,False,False,False,False,False
( yes),False,False,False,False,False,False
Cluster 0,False,False,False,False,False,False
( no ),False,False,False,False,False,False
768,False,False,False,False,False,False
198,False,False,False,False,False,False
Cluster 1,False,False,False,False,False,False
(benign),False,False,False,False,False,False
Cluster 0,False,False,False,False,False,False
(malignant),False,False,False,False,False,False
683,False,False,False,False,False,False
27,False,False,False,False,False,False
Cluster 1,False,False,False,False,False,False
(die),False,False,False,False,False,False
Cluster 0,False,False,False,False,False,False
(live),False,False,False,False,False,False
155,False,False,False,False,False,False
46,False,False,False,False,False,False
Clustered Instances,False,False,False,False,False,False
" Fig. 3 MLP Architecture built in Weka4. Evaluation of Proposed System4.1 ExperimentsIn this section, proposed model isvalidated on three publicly available data setsnamely Pima Indians Diabetes, WisconsinBreast Cancer Data Set and Hepatitis data setfrom the UCI machine learning data repository(Newman, Hettich, Blake, & Merz, 2007).We have taken two more data sets whichdo not have missing values namely 2D syntheticdata set and Wisconsin Diagnostic BreastCancer Data Set (WDBC) from the UCImachine learning data repository (Newman,Hettich, Blake, & Merz, 2007). We haveartificially induced missing values in these datasets to observe the robustness of our proposedmodel as a function of missing rate.4.1.1 Pima Indian Diabetes data SetThis data set includes a total of 768instances depicted by 8 attributes and apredictive class. Out of 768 instances , 268instances belong to class ‘1’ which indicate thatdiabetic cases  and 500 instances belong to class‘0’ means non diabetic cases i.e. they are healthypersons. Most of the cases contain missingvalues. Number of missing values correspondingto each attribute in the data set is shown in Table9.4.1.2 Wisconsin Breast Cancer Data SetThe data set contains total 699 instancesdescribed by 9 attributes and a predictive class.The attribute values for all 9 attributes lie from 1to 10.  The class attribute has only twocategories namely benign and malignant. Class‘malignant’ has 241 instances and class ‘benign’consist of 458 instances. This data set consists ofmissing values .The number of missing valueswith their attribute name is mentioned in Table9.Table 9Numbers of missing values in each variable of  datasets.Pima diabetes data set(763 missing values in768 instances)Breast Cancer Data Set(16 missing values in 699instances)VariableNumberof missingvaluesVariableNumberof missingvaluesPregnant 111 ClumpthicknessNil*Plasmaglucose5Uniformityof cell sizeNil*Diastolic BP35Uniformityof cell shapeNil*TricepsSFT  227MarginaladhesionNil*Serum-Insulin374SingleepitheticalsizeNil*BMI  11Bare nuclei16DPF  Nil*BlandchromatinNil*Age Nil*NormalnucleiNil*Class Nil*Mitoses Nil*Class Nil*Nil*indicates no missing value with respect to agiven feature.",False,False,False,False,True,False
" Fig. 3 MLP Architecture built in Weka4. Evaluation of Proposed System4.1 ExperimentsIn this section, proposed model isvalidated on three publicly available data setsnamely Pima Indians Diabetes, WisconsinBreast Cancer Data Set and Hepatitis data setfrom the UCI machine learning data repository(Newman, Hettich, Blake, & Merz, 2007).We have taken two more data sets whichdo not have missing values namely 2D syntheticdata set and Wisconsin Diagnostic BreastCancer Data Set (WDBC) from the UCImachine learning data repository (Newman,Hettich, Blake, & Merz, 2007). We haveartificially induced missing values in these datasets to observe the robustness of our proposedmodel as a function of missing rate.4.1.1 Pima Indian Diabetes data SetThis data set includes a total of 768instances depicted by 8 attributes and apredictive class. Out of 768 instances , 268instances belong to class ‘1’ which indicate thatdiabetic cases  and 500 instances belong to class‘0’ means non diabetic cases i.e. they are healthypersons. Most of the cases contain missingvalues. Number of missing values correspondingto each attribute in the data set is shown in Table9.4.1.2 Wisconsin Breast Cancer Data SetThe data set contains total 699 instancesdescribed by 9 attributes and a predictive class.The attribute values for all 9 attributes lie from 1to 10.  The class attribute has only twocategories namely benign and malignant. Class‘malignant’ has 241 instances and class ‘benign’consist of 458 instances. This data set consists ofmissing values .The number of missing valueswith their attribute name is mentioned in Table9.Table 9Numbers of missing values in each variable of  datasets.Pima diabetes data set(763 missing values in768 instances)Breast Cancer Data Set(16 missing values in 699instances)VariableNumberof missingvaluesVariableNumberof missingvaluesPregnant 111 ClumpthicknessNil*Plasmaglucose5Uniformityof cell sizeNil*Diastolic BP35Uniformityof cell shapeNil*TricepsSFT  227MarginaladhesionNil*Serum-Insulin374SingleepitheticalsizeNil*BMI  11Bare nuclei16DPF  Nil*BlandchromatinNil*Age Nil*NormalnucleiNil*Class Nil*Mitoses Nil*Class Nil*Nil*indicates no missing value with respect to agiven feature.",False,False,False,False,True,False
 ,False,False,False,False,False,False
4. Evaluation of Proposed System,False,False,True,True,False,False
4.1 Experiments,False,False,False,False,False,False
"In this section, proposed model is",False,False,False,False,False,False
validated on three publicly available data sets,False,False,False,False,False,False
"namely Pima Indians Diabetes, Wisconsin",False,False,False,False,False,False
Breast Cancer Data Set and Hepatitis data set,False,False,False,False,False,False
from the UCI machine learning data repository,False,False,False,False,False,False
"(Newman, Hettich, Blake, & Merz, 2007).",False,False,False,False,False,False
We have taken two more data sets which,False,False,False,False,False,False
do not have missing values namely 2D synthetic,False,False,False,False,False,False
data set and Wisconsin Diagnostic Breast,False,False,False,False,False,False
Cancer Data Set (WDBC) from the UCI,False,False,False,False,False,False
"machine learning data repository (Newman,",False,False,False,False,False,False
"Hettich, Blake, & Merz, 2007). We have",False,False,False,False,False,False
artificially induced missing values in these data,False,False,False,False,False,False
sets to observe the robustness of our proposed,False,False,False,False,False,False
model as a function of missing rate.,False,False,False,False,False,False
4.1.1 Pima Indian Diabetes data Set,False,False,False,False,False,False
This data set includes a total of 768,False,False,False,False,False,False
instances depicted by 8 attributes and a,False,False,False,False,False,False
"predictive class. Out of 768 instances , 268",False,False,False,False,False,False
instances belong to class ‘1’ which indicate that,False,False,False,False,False,False
diabetic cases  and 500 instances belong to class,False,False,False,False,False,False
‘0’ means non diabetic cases i.e. they are healthy,False,False,False,False,False,False
persons. Most of the cases contain missing,False,False,False,False,False,False
values. Number of missing values corresponding,False,False,False,False,False,False
to each attribute in the data set is shown in Table,False,False,False,False,False,False
9.,False,False,False,False,False,False
4.1.2 Wisconsin Breast Cancer Data Set,False,False,False,False,False,False
The data set contains total 699 instances,False,False,False,False,False,False
described by 9 attributes and a predictive class.,False,False,False,False,False,False
The attribute values for all 9 attributes lie from 1,False,False,False,False,False,False
to 10.  The class attribute has only two,False,False,False,False,False,False
categories namely benign and malignant. Class,False,False,False,False,False,False
‘malignant’ has 241 instances and class ‘benign’,False,False,False,False,False,False
consist of 458 instances. This data set consists of,False,False,False,False,False,False
missing values .The number of missing values,False,False,False,False,False,False
with their attribute name is mentioned in Table,False,False,False,False,False,False
9.,False,False,False,False,False,False
Numbers of missing values in each variable of  data,False,False,False,False,False,False
sets.,False,False,False,False,False,False
Pregnant 111 Clump,False,False,False,False,False,False
thickness,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Plasma,False,False,False,False,False,False
glucose,False,False,False,False,False,False
5,False,False,False,False,False,False
Uniformity,False,False,False,False,False,False
of cell size,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Diastolic,False,False,False,False,False,False
 BP,True,False,False,True,False,False
35,False,False,False,False,False,False
Uniformity,False,False,False,False,False,False
of cell shape,False,False,False,False,False,False
Nil,False,False,False,False,False,False
TricepsSFT  227,False,False,False,False,False,False
Marginal,False,False,False,False,False,False
adhesion,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Serum-,False,False,False,False,False,False
Insulin,False,False,False,False,False,False
374,False,False,False,False,False,False
Single,False,False,False,False,False,False
epithetical,False,False,False,False,False,False
size,False,False,False,False,False,False
Nil,False,False,False,False,False,False
BMI  11,True,False,False,True,False,False
Bare nuclei,False,False,False,False,False,False
16,False,False,False,False,False,False
DPF  Nil,False,False,False,False,False,False
Bland,False,False,False,False,False,False
chromatin,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Age Nil,False,False,False,False,False,False
Normal,False,False,False,False,False,False
nuclei,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Class Nil,False,False,False,False,False,False
Mitoses Nil,False,False,False,False,False,False
Class Nil,False,False,False,False,False,False
Nil,False,False,False,False,False,False
*,False,False,False,False,False,False
indicates no missing value with respect to a,False,False,False,False,False,False
given feature.,False,False,False,False,False,False
"  4.1.3 Hepatitis Data SetThe data set contains total 155 instancesdescribed by 19 attributes and a predictive class.The class attribute has only two categoriesnamely live and die. Class ‘live’ has 123instances and class ‘die’ consist of 32 instances.Many instances in the data set consist of missingvalues .The number of missing values with theirattribute name is mentioned in Table 10.Table10Numbers of missing values in each variable of theHepatitis data set (167 missing values in 155instances).VariableNumberof missing valuesAge Nil*SexNil*Steroid 1Antivirals Nil*Fatigue1Malaise1Anorexia1Liver Big10Liver Firm11Spleen Palpable5Spiders5Ascites5Varices 5Bilirubin6Alk Phosphate29Sgot4Albumin16Protime 67Histology Nil*Class Nil*Nil*indicates no missing value with respect to agiven feature.4.1.4 2D Synthetic Data SetWe created 2D data set shown in Fig. 4.It is described by three attributes namely x, yand a class. Both the attributes x and y havenumerical values and a class attribute has  twocategories namely  “yes” and “no” Out of 200instances, 97 instances belong to class “yes” and103 instances belong to class “no. This data setdoes not have any missing values. We haveartificially created missing values to observe theperformance of our model at four missing rate(the amount of data missing) such as 20%,40 %,60% and 80 %.Fig. 4.  2D data set4.1.5 Wisconsin Diagnostic Breast CancerData SetWisconsin Diagnostic Breast CancerData Set (WDBC) contains total 569 instancesdescribed by 30 attributes and a predictive class.The class attribute has only two categoriesnamely B (benign) and M (malignant). Class ‘B’has 357 instances. This data set does not haveany missing values. We have artificially createdmissing values to observe the performance ofour model as a function of missing rate (theamount of data missing) such as 25%, 50% and75 %.4.1.6 Experimental FrameworkMV techniques except matrixfactorization (implemented in Python) understudy are implemented using Knowledgeextraction based on evolutionary learning(KEEL) (Alcalá-Fdez, et al., 2009). Then wehave applied simple K-mean clustering usingWaikato Environment for Knowledge Analysis(Weka) toolkit (Witten & Frank, 2000) on 11imputed data sets. Clusters produced by K-means are evaluated using their classes (",False,False,False,False,False,False
"  4.1.3 Hepatitis Data SetThe data set contains total 155 instancesdescribed by 19 attributes and a predictive class.The class attribute has only two categoriesnamely live and die. Class ‘live’ has 123instances and class ‘die’ consist of 32 instances.Many instances in the data set consist of missingvalues .The number of missing values with theirattribute name is mentioned in Table 10.Table10Numbers of missing values in each variable of theHepatitis data set (167 missing values in 155instances).VariableNumberof missing valuesAge Nil*SexNil*Steroid 1Antivirals Nil*Fatigue1Malaise1Anorexia1Liver Big10Liver Firm11Spleen Palpable5Spiders5Ascites5Varices 5Bilirubin6Alk Phosphate29Sgot4Albumin16Protime 67Histology Nil*Class Nil*Nil*indicates no missing value with respect to agiven feature.4.1.4 2D Synthetic Data SetWe created 2D data set shown in Fig. 4.It is described by three attributes namely x, yand a class. Both the attributes x and y havenumerical values and a class attribute has  twocategories namely  “yes” and “no” Out of 200instances, 97 instances belong to class “yes” and103 instances belong to class “no. This data setdoes not have any missing values. We haveartificially created missing values to observe theperformance of our model at four missing rate(the amount of data missing) such as 20%,40 %,60% and 80 %.Fig. 4.  2D data set4.1.5 Wisconsin Diagnostic Breast CancerData SetWisconsin Diagnostic Breast CancerData Set (WDBC) contains total 569 instancesdescribed by 30 attributes and a predictive class.The class attribute has only two categoriesnamely B (benign) and M (malignant). Class ‘B’has 357 instances. This data set does not haveany missing values. We have artificially createdmissing values to observe the performance ofour model as a function of missing rate (theamount of data missing) such as 25%, 50% and75 %.4.1.6 Experimental FrameworkMV techniques except matrixfactorization (implemented in Python) understudy are implemented using Knowledgeextraction based on evolutionary learning(KEEL) (Alcalá-Fdez, et al., 2009). Then wehave applied simple K-mean clustering usingWaikato Environment for Knowledge Analysis(Weka) toolkit (Witten & Frank, 2000) on 11imputed data sets. Clusters produced by K-means are evaluated using their classes (",False,False,False,False,False,False
  ,False,False,False,False,False,False
4.1.3 Hepatitis Data Set,False,False,False,False,False,False
The data set contains total 155 instances,False,False,False,False,False,False
described by 19 attributes and a predictive class.,False,False,False,False,False,False
The class attribute has only two categories,False,False,False,False,False,False
namely live and die. Class ‘live’ has 123,False,False,False,False,False,False
instances and class ‘die’ consist of 32 instances.,False,False,False,False,False,False
Many instances in the data set consist of missing,False,False,False,False,False,False
values .The number of missing values with their,False,False,False,False,False,False
attribute name is mentioned in Table 10.,False,False,False,False,False,False
Numbers of missing values in each variable of the,False,False,False,False,False,False
Hepatitis data set (167 missing values in 155,False,False,False,False,False,False
instances).,False,False,False,False,False,False
Age Nil,False,False,False,False,False,False
Sex,False,False,False,False,False,False
Nil,False,False,False,False,False,False
Steroid 1,False,False,False,False,False,False
Antivirals Nil,False,False,False,False,False,False
Fatigue,False,False,False,False,False,False
1,False,False,False,False,False,False
Malaise,False,False,False,False,False,False
1,False,False,False,False,False,False
Anorexia,False,False,False,False,False,False
1,False,False,False,False,False,False
Liver Big,False,False,False,False,False,False
10,False,False,False,False,False,False
Liver Firm,False,False,False,False,False,False
11,False,False,False,False,False,False
Spleen Palpable,False,False,False,False,False,False
5,False,False,False,False,False,False
Spiders,False,False,False,False,False,False
5,False,False,False,False,False,False
Ascites,False,False,False,False,False,False
5,False,False,False,False,False,False
Varices 5,False,False,False,False,False,False
Bilirubin,False,False,False,False,False,False
6,False,False,False,False,False,False
Alk Phosphate,False,False,False,False,False,False
29,False,False,False,False,False,False
Sgot,False,False,False,False,False,False
4,False,False,False,False,False,False
Albumin,False,False,False,False,False,False
16,False,False,False,False,False,False
Protime 67,False,False,False,False,False,False
Histology Nil,False,False,False,False,False,False
Class Nil,False,False,False,False,False,False
Nil,False,False,False,False,False,False
*,False,False,False,False,False,False
indicates no missing value with respect to a,False,False,False,False,False,False
given feature.,False,False,False,False,False,False
4.1.4 2D Synthetic Data Set,False,False,False,False,False,False
We created 2D data set shown in Fig. 4.,False,False,False,False,False,False
"It is described by three attributes namely x, y",False,False,False,False,False,False
and a class. Both the attributes x and y have,False,False,False,False,False,False
numerical values and a class attribute has  two,False,False,False,False,False,False
categories namely  “yes” and “no” Out of 200,False,False,False,False,False,False
"instances, 97 instances belong to class “yes” and",False,False,False,False,False,False
103 instances belong to class “no. This data set,False,False,False,False,False,False
does not have any missing values. We have,False,False,False,False,False,False
artificially created missing values to observe the,False,False,False,False,False,False
performance of our model at four missing rate,False,False,False,False,False,False
"(the amount of data missing) such as 20%,40 %,",False,False,False,False,False,False
60% and 80 %.,False,False,False,False,False,False
Fig. 4.  2D data set,False,False,False,False,True,False
4.1.5 Wisconsin Diagnostic Breast Cancer,False,False,False,False,False,False
Data Set,False,False,False,False,False,False
Wisconsin Diagnostic Breast Cancer,False,False,False,False,False,False
Data Set (WDBC) contains total 569 instances,False,False,False,False,False,False
described by 30 attributes and a predictive class.,False,False,False,False,False,False
The class attribute has only two categories,False,False,False,False,False,False
namely B (benign) and M (malignant). Class ‘B’,False,False,False,False,False,False
has 357 instances. This data set does not have,False,False,False,False,False,False
any missing values. We have artificially created,False,False,False,False,False,False
missing values to observe the performance of,False,False,False,False,False,False
our model as a function of missing rate (the,False,False,False,False,False,False
"amount of data missing) such as 25%, 50% and",False,False,False,False,False,False
75 %.,False,False,False,False,False,False
4.1.6 Experimental Framework,False,False,False,False,False,False
MV techniques except matrix,False,False,False,False,False,False
factorization (implemented in Python) under,False,False,False,False,False,False
study are implemented using Knowledge,False,False,False,False,False,False
extraction based on evolutionary learning,False,False,False,False,False,False
"(KEEL) (Alcalá-Fdez, et al., 2009). Then we",False,False,False,False,False,False
have applied simple K-mean clustering using,False,False,False,False,False,False
Waikato Environment for Knowledge Analysis,False,False,False,False,False,False
"(Weka) toolkit (Witten & Frank, 2000) on 11",False,False,False,False,False,False
imputed data sets. Clusters produced by K-,False,False,False,False,False,False
" ground truth)  for 11 experiments .CMC andcase deletion are chosen as the best imputationapproaches for diabetes dataset as well ashepatitis data sets and case deletion for breastcancer dataset respectively. Correctly classifiedinstances were extracted from the respectivedata sets under study. These resulting data setswere classified using MLP classifier. Resultswere collected and evaluated using performancemetrics discussed in section 4.2.4.2 Performance metricsThis section   describes measures thatwere used to evaluate the performance ofclassifiers.(a) Accuracy, sensitivity and specificity.Accuracy is popular metric that refers to theability of the model to correctly predict the classlabel of new or unseen data (Han & Kamber,2006). In addition to this, sensitivity andspecificity are also used to assess how wellclassifier can recognize true examples as well asfalse examples. These measures are calculatedas. =                  (2) =                           (3) =                          (4)where,True positives (TP) = no. of correctclassifications predicted as yes (or positive).True negatives (TN) = no. of correctclassifications predicted as no (or negative)False positive (FP) = no. of examples that areincorrectly predicted as yes (or positive) when itis actually no (negative).False negative (FN) = no. of examples that areincorrectly predicted as no when it is actuallyyes.(b) Kappa statistics. This measure evaluates thepair wise agreement between two differentobservers, corrected for an expected chanceagreement (Thora,  Ebba,  Helgi, & Sven, 2008).Kappa value 0 indicates chance agreement and 1shows prefect agreement between classifier andthe ground truth (true classes). Kappa valueusing proposed model is 0.996, 0.986 and 0.978for Pima Indians Diabetes, Wisconsin BreastCancer and Hepatitis data set respectively. It iscalculated as =[()()][()]                   (5)where,  p(a) and p(e) can been found fromEq.(6) and Eq.(7) respectively.()=                     (6)()=[()∗()∗()]       (7)where N is the total number of instances used.(c) Area under ROC Curve. ROC curve is thegraph between True positive rate (TPR) andFalse positive rate (FPR). Accuracy of aclassifier can also be measured by the area underthe ROC curve (AUC). Most of the classifiershave AUC between 0.5 and 1. AUC is 1 forperfect classifiers. An area under the ROC curveof 0.8, for example, means that a randomlyselected case from the group with the targetequals 1 has a score larger than that for arandomly chosen case from the group with thetarget equals 0 in 80% of the time. When aclassifier cannot distinguish between the twogroups, the area will be equal to 0.5.(d) Confusion matrix. A confusion matrix iscalculated for the classifier to interpret theresults as shown in Table 11, Table 12 and Table13. Table11 shows 187 and 382 correctlyclassified instances in class ‘yes’ and class ‘no’respectively for diabetes data set. Similarly, 432are correctly classified instances in class“benign” and 220 are correct instances in class“malignant” for breast cancer data se, as shownin Table 12. Table 13 shows 80, correctlyclassified instances in class “live” and 28,correct instances in class “die” for Hepatitis dataset. The diagonal elements of the matrix showthe incorrect classification made by classifier.",False,False,False,False,False,False
" ground truth)  for 11 experiments .CMC andcase deletion are chosen as the best imputationapproaches for diabetes dataset as well ashepatitis data sets and case deletion for breastcancer dataset respectively. Correctly classifiedinstances were extracted from the respectivedata sets under study. These resulting data setswere classified using MLP classifier. Resultswere collected and evaluated using performancemetrics discussed in section 4.2.4.2 Performance metricsThis section   describes measures thatwere used to evaluate the performance ofclassifiers.(a) Accuracy, sensitivity and specificity.Accuracy is popular metric that refers to theability of the model to correctly predict the classlabel of new or unseen data (Han & Kamber,2006). In addition to this, sensitivity andspecificity are also used to assess how wellclassifier can recognize true examples as well asfalse examples. These measures are calculatedas. =                  (2) =                           (3) =                          (4)where,True positives (TP) = no. of correctclassifications predicted as yes (or positive).True negatives (TN) = no. of correctclassifications predicted as no (or negative)False positive (FP) = no. of examples that areincorrectly predicted as yes (or positive) when itis actually no (negative).False negative (FN) = no. of examples that areincorrectly predicted as no when it is actuallyyes.(b) Kappa statistics. This measure evaluates thepair wise agreement between two differentobservers, corrected for an expected chanceagreement (Thora,  Ebba,  Helgi, & Sven, 2008).Kappa value 0 indicates chance agreement and 1shows prefect agreement between classifier andthe ground truth (true classes). Kappa valueusing proposed model is 0.996, 0.986 and 0.978for Pima Indians Diabetes, Wisconsin BreastCancer and Hepatitis data set respectively. It iscalculated as =[()()][()]                   (5)where,  p(a) and p(e) can been found fromEq.(6) and Eq.(7) respectively.()=                     (6)()=[()∗()∗()]       (7)where N is the total number of instances used.(c) Area under ROC Curve. ROC curve is thegraph between True positive rate (TPR) andFalse positive rate (FPR). Accuracy of aclassifier can also be measured by the area underthe ROC curve (AUC). Most of the classifiershave AUC between 0.5 and 1. AUC is 1 forperfect classifiers. An area under the ROC curveof 0.8, for example, means that a randomlyselected case from the group with the targetequals 1 has a score larger than that for arandomly chosen case from the group with thetarget equals 0 in 80% of the time. When aclassifier cannot distinguish between the twogroups, the area will be equal to 0.5.(d) Confusion matrix. A confusion matrix iscalculated for the classifier to interpret theresults as shown in Table 11, Table 12 and Table13. Table11 shows 187 and 382 correctlyclassified instances in class ‘yes’ and class ‘no’respectively for diabetes data set. Similarly, 432are correctly classified instances in class“benign” and 220 are correct instances in class“malignant” for breast cancer data se, as shownin Table 12. Table 13 shows 80, correctlyclassified instances in class “live” and 28,correct instances in class “die” for Hepatitis dataset. The diagonal elements of the matrix showthe incorrect classification made by classifier.",False,False,False,False,False,False
 ,False,False,False,False,False,False
ground truth)  for 11 experiments .CMC and,False,False,False,False,False,False
case deletion are chosen as the best imputation,False,False,False,False,False,False
approaches for diabetes dataset as well as,False,False,False,False,False,False
hepatitis data sets and case deletion for breast,False,False,False,False,False,False
cancer dataset respectively. Correctly classified,False,False,False,False,False,False
instances were extracted from the respective,False,False,False,False,False,False
data sets under study. These resulting data sets,False,False,False,False,False,False
were classified using MLP classifier. Results,False,False,False,False,False,False
were collected and evaluated using performance,False,False,False,False,False,False
metrics discussed in section 4.2.,False,False,False,False,False,False
4.2 Performance metrics,False,False,False,False,False,False
This section   describes measures that,False,False,False,False,False,False
were used to evaluate the performance of,False,False,False,False,False,False
classifiers.,False,False,False,False,False,False
"(a) Accuracy, sensitivity and specificity.",False,False,False,False,False,False
Accuracy is popular metric that refers to the,False,False,False,False,False,False
ability of the model to correctly predict the class,False,False,False,False,False,False
"label of new or unseen data (Han & Kamber,",False,False,False,False,False,False
"2006). In addition to this, sensitivity and",False,False,False,False,False,False
specificity are also used to assess how well,False,False,False,False,False,False
classifier can recognize true examples as well as,False,False,False,False,False,False
false examples. These measures are calculated,False,False,False,False,False,False
as.,False,False,False,False,False,False
 =,False,False,False,False,False,False
                  (2),False,False,False,False,False,False
 =,False,False,False,False,False,False
                           (3),False,False,False,False,False,False
 =,False,False,False,False,False,False
                          (4),False,False,False,False,False,False
"where,",False,False,False,False,False,False
True positives (TP) = no. of correct,False,False,False,False,False,False
classifications predicted as yes (or positive).,False,False,False,False,False,False
True negatives (TN) = no. of correct,False,False,False,False,False,False
classifications predicted as no (or negative),False,False,False,False,False,False
False positive (FP) = no. of examples that are,False,False,False,False,False,False
incorrectly predicted as yes (or positive) when it,False,False,False,False,False,False
is actually no (negative).,False,False,False,False,False,False
False negative (FN) = no. of examples that are,False,False,False,False,False,False
incorrectly predicted as no when it is actually,False,False,False,False,False,False
yes.,False,False,False,False,False,False
(b) Kappa statistics. This measure evaluates the,False,False,False,False,False,False
pair wise agreement between two different,False,False,False,False,False,False
"observers, corrected for an expected chance",False,False,False,False,False,False
"agreement (Thora,  Ebba,  Helgi, & Sven, 2008).",False,False,False,False,False,False
Kappa value 0 indicates chance agreement and 1,False,False,False,False,False,False
shows prefect agreement between classifier and,False,False,False,False,False,False
the ground truth (true classes). Kappa value,False,False,False,False,False,False
"using proposed model is 0.996, 0.986 and 0.978",False,False,False,False,False,False
"for Pima Indians Diabetes, Wisconsin Breast",False,False,False,False,False,False
Cancer and Hepatitis data set respectively. It is,False,False,False,False,False,False
calculated as,False,False,False,False,False,False
 =,False,False,False,False,False,False
                   (5),False,False,False,False,False,False
"where,  p(a) and p(e) can been found from",False,False,False,False,False,False
Eq.(6) and Eq.(7) respectively.,False,False,False,False,False,False
()=,False,False,False,False,False,False
                     (6),False,False,False,False,False,False
()=,False,False,False,False,False,False
       (7),False,False,False,False,False,False
where N is the total number of instances used.,False,False,False,False,False,False
(c) Area under ROC Curve. ROC curve is the,False,False,False,False,False,False
graph between True positive rate (TPR) and,False,False,False,False,False,False
False positive rate (FPR). Accuracy of a,False,False,False,False,False,False
classifier can also be measured by the area under,False,False,False,False,False,False
the ROC curve (AUC). Most of the classifiers,False,False,False,False,False,False
have AUC between 0.5 and 1. AUC is 1 for,False,False,False,False,False,False
perfect classifiers. An area under the ROC curve,False,False,False,False,False,False
"of 0.8, for example, means that a randomly",False,False,False,False,False,False
selected case from the group with the target,False,False,False,False,False,False
equals 1 has a score larger than that for a,False,False,False,False,False,False
randomly chosen case from the group with the,False,False,False,False,False,False
target equals 0 in 80% of the time. When a,False,False,False,False,False,False
classifier cannot distinguish between the two,False,False,False,False,False,False
"groups, the area will be equal to 0.5.",False,False,False,False,False,False
(d) Confusion matrix. A confusion matrix is,False,False,False,False,False,False
calculated for the classifier to interpret the,False,False,False,False,False,False
"results as shown in Table 11, Table 12 and Table",False,False,False,False,False,False
13. Table11 shows 187 and 382 correctly,False,False,True,True,False,False
classified instances in class ‘yes’ and class ‘no,False,False,False,False,False,False
"’respectively for diabetes data set. Similarly, 432",False,False,False,False,False,False
are correctly classified instances in class,False,False,False,False,False,False
“benign” and 220 are correct instances in class,False,False,False,False,False,False
"“malignant” for breast cancer data se, as shown",False,False,False,False,False,False
"in Table 12. Table 13 shows 80, correctly",False,False,False,False,False,True
"classified instances in class “live” and 28,",False,False,False,False,False,False
correct instances in class “die” for Hepatitis data,False,False,False,False,False,False
set. The diagonal elements of the matrix show,False,False,False,False,False,False
the incorrect classification made by classifier.,False,False,False,False,False,False
" Table 11Confusion matrix. (570 instances) after patternextraction for Pima diabetes data set.abClassified as187(TruePositive) 0(False Negative) a=yes1(FalsePositive) 382(True Negative) b=noTable 12Confusion matrix. (656 instances) after patternextraction for Wisconsin Breast cancer data set.abClassified as432(TruePositive)3(FalseNegative) a=benign1(FalsePositive)220(TrueNegative) b=malignantTable 13 Confusion matrix. (109 instances) after patternextraction for Hepatitis  data set.abClassified as80(TruePositive)0(FalseNegative) a=live1(FalsePositive)28(TrueNegative) b=die4.3 k-fold cross-validationCross-Validation (Delen, Walker &Kadam, 2005) is a statistical method ofevaluating and comparing learning algorithmsby dividing data into two segments. One is usedto learn or train a model and the other is used tovalidate the model. The basic form of cross-validation is k-fold cross-validation. In thisform, data set is divided into k sub sets. Eachsub set is tested via classifier constructed fromthe remaining (k-1) sub sets. Thus the k differenttest results are obtained for each train–test pair.The average result gives the test accuracy of thealgorithm. We used 10 fold cross-validations inour proposed HPM-MI since it reduces the biasassociated with random sampling.5. Results and AnalysisThe results obtained from the proposedmodel are tabulated in Table 14 for diabetes,Wisconsin breast cancer and Hepatitis data sets.The following subsections give the detaileddiscussion of results obtained for data sets usedunder experimental work.Table14Obtained classification accuracy, sensitivity andspecificity, Kappa value and ROC .PerformanceMetricPimaIndiandiabetesData SetWisconsinBreastCancerData SetHepatitisdatasetAccuracy(%)99.82 99.39 99.08Sensitivity (%)10099.31100Specificity (%)99.7 99.5496.55Kappa0.996 0.9860.978ROC1 10.995.1 Discussion for Pima Indian DiabetesData SetThe accuracy of proposed HPM-MI iscomputed as 99.82% .We have also comparedwith existing methods (Seera & Lim, 2014;Kahramanli & Allahverdi, 2008; N., 2010; Patil,Joshi, & Toshniwal, 2010)  in the literatureshown in Table 15. It is clearly evident fromTable 15 that none of the studies had the successrates higher than 98.92 % for the mentionedalgorithms on Indian Pima Diabetes data set.Table 15Classification accuracies of proposed model andother classifiers for the Pima Indian diabetes.MethodAccuracy(%)ReferenceHPM-MI99.82Our ProposedModelFMM-CART-RF78.39(Seera & Lim,2014)FMM-CART71.35(Seera & Lim,2014)FMM69.28(Seera & Lim,2014)Sim + F275.29(Luukka,2011)Sim + F175.84(Luukka,2011)Sim75.29(Luukka,2011)",False,False,False,False,False,True
" Table 11Confusion matrix. (570 instances) after patternextraction for Pima diabetes data set.abClassified as187(TruePositive) 0(False Negative) a=yes1(FalsePositive) 382(True Negative) b=noTable 12Confusion matrix. (656 instances) after patternextraction for Wisconsin Breast cancer data set.abClassified as432(TruePositive)3(FalseNegative) a=benign1(FalsePositive)220(TrueNegative) b=malignantTable 13 Confusion matrix. (109 instances) after patternextraction for Hepatitis  data set.abClassified as80(TruePositive)0(FalseNegative) a=live1(FalsePositive)28(TrueNegative) b=die4.3 k-fold cross-validationCross-Validation (Delen, Walker &Kadam, 2005) is a statistical method ofevaluating and comparing learning algorithmsby dividing data into two segments. One is usedto learn or train a model and the other is used tovalidate the model. The basic form of cross-validation is k-fold cross-validation. In thisform, data set is divided into k sub sets. Eachsub set is tested via classifier constructed fromthe remaining (k-1) sub sets. Thus the k differenttest results are obtained for each train–test pair.The average result gives the test accuracy of thealgorithm. We used 10 fold cross-validations inour proposed HPM-MI since it reduces the biasassociated with random sampling.5. Results and AnalysisThe results obtained from the proposedmodel are tabulated in Table 14 for diabetes,Wisconsin breast cancer and Hepatitis data sets.The following subsections give the detaileddiscussion of results obtained for data sets usedunder experimental work.Table14Obtained classification accuracy, sensitivity andspecificity, Kappa value and ROC .PerformanceMetricPimaIndiandiabetesData SetWisconsinBreastCancerData SetHepatitisdatasetAccuracy(%)99.82 99.39 99.08Sensitivity (%)10099.31100Specificity (%)99.7 99.5496.55Kappa0.996 0.9860.978ROC1 10.995.1 Discussion for Pima Indian DiabetesData SetThe accuracy of proposed HPM-MI iscomputed as 99.82% .We have also comparedwith existing methods (Seera & Lim, 2014;Kahramanli & Allahverdi, 2008; N., 2010; Patil,Joshi, & Toshniwal, 2010)  in the literatureshown in Table 15. It is clearly evident fromTable 15 that none of the studies had the successrates higher than 98.92 % for the mentionedalgorithms on Indian Pima Diabetes data set.Table 15Classification accuracies of proposed model andother classifiers for the Pima Indian diabetes.MethodAccuracy(%)ReferenceHPM-MI99.82Our ProposedModelFMM-CART-RF78.39(Seera & Lim,2014)FMM-CART71.35(Seera & Lim,2014)FMM69.28(Seera & Lim,2014)Sim + F275.29(Luukka,2011)Sim + F175.84(Luukka,2011)Sim75.29(Luukka,2011)",False,False,False,False,False,True
 ,False,False,False,False,False,False
Confusion matrix. (570 instances) after pattern,False,False,False,False,False,False
extraction for Pima diabetes data set.,False,False,False,False,False,False
187(True,False,False,False,False,False,False
Positive) 0(False Negative) a=yes,False,False,False,False,False,False
1(False,False,False,False,False,False,False
Positive) 382(True Negative) b=no,False,False,False,False,False,False
Confusion matrix. (656 instances) after pattern,False,False,False,False,False,False
extraction for Wisconsin Breast cancer data set.,False,False,False,False,False,False
432(True,False,False,False,False,False,False
Positive),False,False,False,False,False,False
3(False,False,False,False,False,False,False
Negative) a=benign,False,False,False,False,False,False
1(False,False,False,False,False,False,False
Positive),False,False,False,False,False,False
220(True,False,False,False,False,False,False
Negative) b=malignant,False,False,False,False,False,False
 Confusion matrix. (109 instances) after pattern,False,False,False,False,False,False
extraction for Hepatitis  data set.,False,False,False,False,False,False
80(True,False,False,False,False,False,False
Positive),False,False,False,False,False,False
0(False,False,False,False,False,False,False
Negative) a=live,False,False,False,False,False,False
1(False,False,False,False,False,False,False
Positive),False,False,False,False,False,False
28(True,False,False,False,False,False,False
Negative) b=die,False,False,False,False,False,False
4.3 k-fold cross-validation,False,False,False,False,False,False
"Cross-Validation (Delen, Walker &",False,False,False,False,False,False
"Kadam, 2005) is a statistical method of",False,False,False,False,False,False
evaluating and comparing learning algorithms,False,False,False,False,False,False
by dividing data into two segments. One is used,False,False,False,False,False,False
to learn or train a model and the other is used to,False,False,False,False,False,False
validate the model. The basic form of cross-,False,False,False,False,False,False
validation is k-fold cross-validation. In this,False,False,False,False,False,False
"form, data set is divided into k sub sets. Each",False,False,False,False,False,False
sub set is tested via classifier constructed from,False,False,False,False,False,False
the remaining (k-1) sub sets. Thus the k different,False,False,False,False,False,False
test results are obtained for each train–test pair.,False,False,False,False,False,False
The average result gives the test accuracy of the,False,False,False,False,False,False
algorithm. We used 10 fold cross-validations in,False,False,False,False,False,False
our proposed HPM-MI since it reduces the bias,False,False,False,False,False,False
associated with random sampling.,False,False,False,False,False,False
5. Results and Analysis,False,False,True,True,False,False
The results obtained from the proposed,False,False,False,False,False,False
"model are tabulated in Table 14 for diabetes,",False,False,False,False,False,False
Wisconsin breast cancer and Hepatitis data sets.,False,False,False,False,False,False
The following subsections give the detailed,False,False,False,False,False,False
discussion of results obtained for data sets used,False,False,False,False,False,False
under experimental work.,False,False,False,False,False,False
"Obtained classification accuracy, sensitivity and",False,False,False,False,False,False
"specificity, Kappa value and ROC .",False,False,False,False,False,False
99.82 99.39 99.08,False,False,False,False,False,False
100,False,False,False,False,False,False
99.31,False,False,False,False,False,False
100,False,False,False,False,False,False
99.7 99.54,False,False,False,False,False,False
96.55,False,False,False,False,False,False
0.996 0.986,False,False,False,False,False,False
0.978,False,False,False,False,False,False
1 1,False,False,False,False,False,False
0.99,False,False,False,False,False,False
5.1 Discussion for Pima Indian Diabetes,False,False,False,False,False,False
Data Set,False,False,False,False,False,False
The accuracy of proposed HPM-MI is,False,False,False,False,False,False
computed as 99.82% .We have also compared,False,False,False,False,False,False
"with existing methods (Seera & Lim, 2014;",False,False,False,False,False,False
"Kahramanli & Allahverdi, 2008; N., 2010; Patil,",False,False,False,False,False,False
"Joshi, & Toshniwal, 2010)  in the literature",False,False,False,False,False,False
shown in Table 15. It is clearly evident from,False,False,False,False,False,True
Table 15 that none of the studies had the success,False,False,False,False,False,True
rates higher than 98.92 % for the mentioned,False,False,False,False,False,False
algorithms on Indian Pima Diabetes data set.,False,False,False,False,False,False
Classification accuracies of proposed model and,False,False,False,False,False,False
other classifiers for the Pima Indian diabetes.,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
-,False,False,False,False,False,False
RF,True,False,False,True,False,False
78.39,False,False,False,False,False,False
"(Seera & Lim,",False,False,False,False,False,False
2014),False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
71.35,False,False,False,False,False,False
"(Seera & Lim,",False,False,False,False,False,False
2014),False,False,False,False,False,False
FMM,True,False,False,True,False,False
69.28,False,False,False,False,False,False
"(Seera & Lim,",False,False,False,False,False,False
2014),False,False,False,False,False,False
Sim + F2,False,False,False,False,False,False
75.29,False,False,False,False,False,False
"(Luukka,",False,False,False,False,False,False
2011),False,False,False,False,False,False
Sim + F1,False,False,False,False,False,False
75.84,False,False,False,False,False,False
"(Luukka,",False,False,False,False,False,False
2011),False,False,False,False,False,False
Sim,False,False,False,False,False,False
75.29,False,False,False,False,False,False
"(Luukka,",False,False,False,False,False,False
2011),False,False,False,False,False,False
" Binary-coded GA74.80(Örkcü & Bal,2011)BP73.80(Örkcü & Bal,2011)Real-coded GA77.60(Örkcü & Bal,2011)Hybrid PredictionModel with F-score98.92(llango &Ramaraj, 2010)Hybrid PredictionModel92.38(Patil et al, 2010)Hybrid model84.5(Kahramanli &Allahverdi,2008) In addition to accuracy, specificity aswell as sensitivity is also computed andcompared with three publications shown in Fig.5. HPM-MI produced best results of sensitivity& specificity as 100% and 99.74%. (Kahramanli& Allahverdi, 2008) has reported sensitivity &specificity of 80.3% & 87.2%. HPM   proposedby (Patil et al, 2010) used resulted sensitivity &specificity of 90.4% & 93.4%, (llango &Ramaraj, 2010) produced gave sensitivity &specificity as 99.3% & 98.7%.Fig. 5 Comparison of sensitivity & specificity withhybrid system , HPM , HPM with F- Score  andProposed Model for Pima diabetes data setAnother important metric ROC is alsocalculated. (Lukka, 2011) and (Seera & Lim,2014) has reported ROC score for theirprediction models in their publications. Hence,Fig. 6 shows the comparisons of the areas underROC curve of HPM-MI with their models andHPM-MI has ranked highest as compared torecent methods proposed by (Luukka, 2011) aswell as (Seera & Lim, 2014). Comparing theresults produced by proposed model, weconclude that the proposed Hybrid PredictionModel (HPM) using missing value imputationobtains very promising results in classifying thepossible diabetes patients. The proposed systemcan be very helpful to the physicians for theirfinal decision on their patients as by using suchan efficient model they can make very accuratedecisions.Fig. 6 ROC score for Pima Indian diabetes data set5.2 Discussion for Wisconsin BreastCancer Data SetThe accuracy of HPM-MI is computedas 99.39%. We have also compared withexisting methods as shown in Table 16. It isclearly evident from Table 16 that none of thestudies had the success rates higher than 98.84%for the mentioned algorithms on WisconsinBrest Cancer Diabetes data set.In addition to accuracy, specificity aswell as sensitivity is also computed .The valuesof sensitivity & specificity are 99.39% and99.54% respectively. The publications shown inTable 16 have not mentioned sensitivity &specificity rates; hence no comparison is madefor these metrics.Another important metric ROC was alsocalculated. Area under ROC for HPM-MI is 1 ofproposed model. (Lukka, 2011) and (Seera &Lim, 2014) has reported ROC score for theirprediction models in their publications. Hence,",False,False,False,False,False,False
" Binary-coded GA74.80(Örkcü & Bal,2011)BP73.80(Örkcü & Bal,2011)Real-coded GA77.60(Örkcü & Bal,2011)Hybrid PredictionModel with F-score98.92(llango &Ramaraj, 2010)Hybrid PredictionModel92.38(Patil et al, 2010)Hybrid model84.5(Kahramanli &Allahverdi,2008) In addition to accuracy, specificity aswell as sensitivity is also computed andcompared with three publications shown in Fig.5. HPM-MI produced best results of sensitivity& specificity as 100% and 99.74%. (Kahramanli& Allahverdi, 2008) has reported sensitivity &specificity of 80.3% & 87.2%. HPM   proposedby (Patil et al, 2010) used resulted sensitivity &specificity of 90.4% & 93.4%, (llango &Ramaraj, 2010) produced gave sensitivity &specificity as 99.3% & 98.7%.Fig. 5 Comparison of sensitivity & specificity withhybrid system , HPM , HPM with F- Score  andProposed Model for Pima diabetes data setAnother important metric ROC is alsocalculated. (Lukka, 2011) and (Seera & Lim,2014) has reported ROC score for theirprediction models in their publications. Hence,Fig. 6 shows the comparisons of the areas underROC curve of HPM-MI with their models andHPM-MI has ranked highest as compared torecent methods proposed by (Luukka, 2011) aswell as (Seera & Lim, 2014). Comparing theresults produced by proposed model, weconclude that the proposed Hybrid PredictionModel (HPM) using missing value imputationobtains very promising results in classifying thepossible diabetes patients. The proposed systemcan be very helpful to the physicians for theirfinal decision on their patients as by using suchan efficient model they can make very accuratedecisions.Fig. 6 ROC score for Pima Indian diabetes data set5.2 Discussion for Wisconsin BreastCancer Data SetThe accuracy of HPM-MI is computedas 99.39%. We have also compared withexisting methods as shown in Table 16. It isclearly evident from Table 16 that none of thestudies had the success rates higher than 98.84%for the mentioned algorithms on WisconsinBrest Cancer Diabetes data set.In addition to accuracy, specificity aswell as sensitivity is also computed .The valuesof sensitivity & specificity are 99.39% and99.54% respectively. The publications shown inTable 16 have not mentioned sensitivity &specificity rates; hence no comparison is madefor these metrics.Another important metric ROC was alsocalculated. Area under ROC for HPM-MI is 1 ofproposed model. (Lukka, 2011) and (Seera &Lim, 2014) has reported ROC score for theirprediction models in their publications. Hence,",False,False,False,False,False,False
 ,False,False,False,False,False,False
Binary,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
74.80,False,False,False,False,False,False
"(Örkcü & Bal,",False,False,False,False,False,False
2011),False,False,False,False,False,False
BP,True,False,False,True,False,False
73.80,False,False,False,False,False,False
"(Örkcü & Bal,",False,False,False,False,False,False
2011),False,False,False,False,False,False
Real,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
77.60,False,False,False,False,False,False
"(Örkcü & Bal,",False,False,False,False,False,False
2011),False,False,False,False,False,False
Hybrid Prediction,False,False,False,False,False,False
Model with F-score,False,False,False,False,False,False
98.92,False,False,False,False,False,False
(llango &,False,False,False,False,False,False
"Ramaraj, 2010)",False,False,False,False,False,False
Hybrid Prediction,False,False,False,False,False,False
Model,False,False,False,False,False,False
92.38,False,False,False,False,False,False
"(Patil et al, 2010)",False,False,False,False,False,False
Hybrid model,False,False,False,False,False,False
84.5,False,False,False,False,False,False
(Kahramanli &,False,False,False,False,False,False
"Allahverdi,",False,False,False,False,False,False
2008),False,False,False,False,False,False
" In addition to accuracy, specificity as",False,False,False,False,False,False
well as sensitivity is also computed and,False,False,False,False,False,False
compared with three publications shown in Fig.,False,False,False,False,False,False
5. HPM-MI produced best results of sensitivity,False,False,True,True,False,False
& specificity as 100% and 99.74%. (Kahramanli,False,False,False,False,False,False
"& Allahverdi, 2008) has reported sensitivity &",False,False,False,False,False,False
specificity of 80.3% & 87.2%. HPM   proposed,False,False,False,False,False,False
"by (Patil et al, 2010) used resulted sensitivity &",False,False,False,False,False,False
"specificity of 90.4% & 93.4%, (llango &",False,False,False,False,False,False
"Ramaraj, 2010) produced gave sensitivity &",False,False,False,False,False,False
specificity as 99.3% & 98.7%.,False,False,False,False,False,False
"hybrid system , HPM , HPM with F- Score  and",False,False,False,False,False,False
Proposed Model for Pima diabetes data set,False,False,False,False,False,False
Another important metric ROC is also,False,False,False,False,False,False
"calculated. (Lukka, 2011) and (Seera & Lim,",False,False,False,False,False,False
2014) has reported ROC score for their,False,False,False,False,False,False
"prediction models in their publications. Hence,",False,False,False,False,False,False
Fig. 6 shows the comparisons of the areas under,False,False,False,False,True,False
ROC curve of HPM-MI with their models and,False,False,False,False,False,False
HPM-MI has ranked highest as compared to,False,False,False,False,False,False
"recent methods proposed by (Luukka, 2011) as",False,False,False,False,False,False
"well as (Seera & Lim, 2014). Comparing the",False,False,False,False,False,False
"results produced by proposed model, we",False,False,False,False,False,False
conclude that the proposed Hybrid Prediction,False,False,False,False,False,False
Model (HPM) using missing value imputation,False,False,False,False,False,False
obtains very promising results in classifying the,False,False,False,False,False,False
possible diabetes patients. The proposed system,False,False,False,False,False,False
can be very helpful to the physicians for their,False,False,False,False,False,False
final decision on their patients as by using such,False,False,False,False,False,False
an efficient model they can make very accurate,False,False,False,False,False,False
decisions.,False,False,False,False,False,False
5.2 Discussion for Wisconsin Breast,False,False,False,False,False,False
Cancer Data Set,False,False,False,False,False,False
The accuracy of HPM-MI is computed,False,False,False,False,False,False
as 99.39%. We have also compared with,False,False,False,False,False,False
existing methods as shown in Table 16. It is,False,False,False,False,False,False
clearly evident from Table 16 that none of the,False,False,False,False,False,False
studies had the success rates higher than 98.84%,False,False,False,False,False,False
for the mentioned algorithms on Wisconsin,False,False,False,False,False,False
Brest Cancer Diabetes data set.,False,False,False,False,False,False
"In addition to accuracy, specificity as",False,False,False,False,False,False
well as sensitivity is also computed .The values,False,False,False,False,False,False
of sensitivity & specificity are 99.39% and,False,False,False,False,False,False
99.54% respectively. The publications shown in,False,False,False,False,False,False
Table 16 have not mentioned sensitivity &,False,False,False,False,False,True
specificity rates; hence no comparison is made,False,False,False,False,False,False
for these metrics.,False,False,False,False,False,False
Another important metric ROC was also,False,False,False,False,False,False
calculated. Area under ROC for HPM-MI is 1 of,False,False,False,False,False,False
"proposed model. (Lukka, 2011) and (Seera &",False,False,False,False,False,False
"Lim, 2014) has reported ROC score for their",False,False,False,False,False,False
"prediction models in their publications. Hence,",False,False,False,False,False,False
"  Fig. 7 shows the comparisons of the areas underROC curve of HPM-MI with their models andHPM-MI has ranked highest as compared torecent methods proposed by (Lukka, 2011) aswell as (Seera and Lim, 2014). Comparing theresults produced by proposed model, weconclude that the proposed Hybrid PredictionModel (HPM) using missing value imputationobtains very promising results in classifying thepossible diabetes patients. The proposed systemcan be very helpful to the physicians for theirfinal decision on their patients as by using suchan efficient model they can make very accuratedecisions.Table 16The values of accuracy of classification made onWisconsin Breast Cancer  dataMethodAccuracy(%)ReferenceHPM-MI99.39Proposed ModelFMM-CART-RF98.84(Seera & Lim,2014)FMM-CART94.86(Seera & Lim,2014)FMM95.26(Seera & Lim,2014)Pedagogical97.07(Stoean &Stoean,2013)Cooperativecoevolution96.69(Stoean & Stoean,2013)SVMs96.50(Stoean & Stoean,2013)Decompositìonal95.93(Stoean & Stoean,2013)Sim97.49(Luukka, 2011)Sim + F197.10(Luukka, 2011)Sim + F297.18(Luukka, 2011)Binary-coded GA94.00(Örkcü & Bal, 2011)BP93.10(Örkcü & Bal, 2011)BC FRPCA198.19(Luukka,2009)BC FRPCA298.13(Luukka, 2009)BC FRPCA398.16(Luukka, 2009)BC Original97.49(Luukka, 2009)BC PCA97.72(Örkcü & Bal, 2011)Fuzzy-AIRS98.51(Polat et al., 2007)AIRS97.20(Polat et al., 2007)5.3 Discussion for Hepatitis Data SetThe accuracy of HPM-MI is computedas 99.08%. We have also compared withexisting methods as shown in Table 17. It isclearly evident from Table 18 that none of thestudies had the success rates higher than 98.52% for the mentioned algorithms on Hepatitisdata set.Fig. 7. ROC score for Wisconsin breast cancerdata set.Table 17 The values of accuracy of classification made onHepatitis dataset.MethodAccuracy(%)ReferenceHPM-MI99.08Proposed ModelSVR NSGAII98.52(Zangooei ,Habibi, &lizadehsani, 2014)LFDM SVM96.77(Chen et al , 2011)LDA-ANFIS94.16(Dogantekin et al. ,2009)PCA AIRS94.12(Polat and Gunes ,2007 b)FS–AIRSwith fuzzyresource92.59(Polat and Gunes ,2006)5. 4 Robustness testing of proposed modelTo show the robustness of our proposedmodel at different missing rate, we made theexperiments on 2D Synthetic Data Set atdifferent missing rate as 20 %, 40 %, 60 % and80 %. Results are produced in Table 18. Table18 shows the incorrectly classified instancesproduced by 11 imputation techniques. Athighest missing rate SVM was chosen as bestimputation method and for other missing rate,CMC was selected to impute the missing valuesin the data set. Moreover, proposed model hasproduced good results at different missing rates.Table 19 summarizes accuracy, specificity,",False,False,False,False,True,False
"  Fig. 7 shows the comparisons of the areas underROC curve of HPM-MI with their models andHPM-MI has ranked highest as compared torecent methods proposed by (Lukka, 2011) aswell as (Seera and Lim, 2014). Comparing theresults produced by proposed model, weconclude that the proposed Hybrid PredictionModel (HPM) using missing value imputationobtains very promising results in classifying thepossible diabetes patients. The proposed systemcan be very helpful to the physicians for theirfinal decision on their patients as by using suchan efficient model they can make very accuratedecisions.Table 16The values of accuracy of classification made onWisconsin Breast Cancer  dataMethodAccuracy(%)ReferenceHPM-MI99.39Proposed ModelFMM-CART-RF98.84(Seera & Lim,2014)FMM-CART94.86(Seera & Lim,2014)FMM95.26(Seera & Lim,2014)Pedagogical97.07(Stoean &Stoean,2013)Cooperativecoevolution96.69(Stoean & Stoean,2013)SVMs96.50(Stoean & Stoean,2013)Decompositìonal95.93(Stoean & Stoean,2013)Sim97.49(Luukka, 2011)Sim + F197.10(Luukka, 2011)Sim + F297.18(Luukka, 2011)Binary-coded GA94.00(Örkcü & Bal, 2011)BP93.10(Örkcü & Bal, 2011)BC FRPCA198.19(Luukka,2009)BC FRPCA298.13(Luukka, 2009)BC FRPCA398.16(Luukka, 2009)BC Original97.49(Luukka, 2009)BC PCA97.72(Örkcü & Bal, 2011)Fuzzy-AIRS98.51(Polat et al., 2007)AIRS97.20(Polat et al., 2007)5.3 Discussion for Hepatitis Data SetThe accuracy of HPM-MI is computedas 99.08%. We have also compared withexisting methods as shown in Table 17. It isclearly evident from Table 18 that none of thestudies had the success rates higher than 98.52% for the mentioned algorithms on Hepatitisdata set.Fig. 7. ROC score for Wisconsin breast cancerdata set.Table 17 The values of accuracy of classification made onHepatitis dataset.MethodAccuracy(%)ReferenceHPM-MI99.08Proposed ModelSVR NSGAII98.52(Zangooei ,Habibi, &lizadehsani, 2014)LFDM SVM96.77(Chen et al , 2011)LDA-ANFIS94.16(Dogantekin et al. ,2009)PCA AIRS94.12(Polat and Gunes ,2007 b)FS–AIRSwith fuzzyresource92.59(Polat and Gunes ,2006)5. 4 Robustness testing of proposed modelTo show the robustness of our proposedmodel at different missing rate, we made theexperiments on 2D Synthetic Data Set atdifferent missing rate as 20 %, 40 %, 60 % and80 %. Results are produced in Table 18. Table18 shows the incorrectly classified instancesproduced by 11 imputation techniques. Athighest missing rate SVM was chosen as bestimputation method and for other missing rate,CMC was selected to impute the missing valuesin the data set. Moreover, proposed model hasproduced good results at different missing rates.Table 19 summarizes accuracy, specificity,",False,False,False,False,True,False
  ,False,False,False,False,False,False
Fig. 7 shows the comparisons of the areas under,False,False,False,False,True,False
ROC curve of HPM-MI with their models and,False,False,False,False,False,False
HPM-MI has ranked highest as compared to,False,False,False,False,False,False
"recent methods proposed by (Lukka, 2011) as",False,False,False,False,False,False
"well as (Seera and Lim, 2014). Comparing the",False,False,False,False,False,False
"results produced by proposed model, we",False,False,False,False,False,False
conclude that the proposed Hybrid Prediction,False,False,False,False,False,False
Model (HPM) using missing value imputation,False,False,False,False,False,False
obtains very promising results in classifying the,False,False,False,False,False,False
possible diabetes patients. The proposed system,False,False,False,False,False,False
can be very helpful to the physicians for their,False,False,False,False,False,False
final decision on their patients as by using such,False,False,False,False,False,False
an efficient model they can make very accurate,False,False,False,False,False,False
decisions.,False,False,False,False,False,False
The values of accuracy of classification made on,False,False,False,False,False,False
Wisconsin Breast Cancer  data,False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
-,False,False,False,False,False,False
RF,True,False,False,True,False,False
98.84,False,False,False,False,False,False
"(Seera & Lim,2014)",False,False,False,False,False,False
FMM,True,False,False,True,False,False
-,False,False,False,False,False,False
CART,True,False,False,True,False,False
94.86,False,False,False,False,False,False
"(Seera & Lim,2014)",False,False,False,False,False,False
FMM,True,False,False,True,False,False
95.26,False,False,False,False,False,False
"(Seera & Lim,2014)",False,False,False,False,False,False
Pedagogical,False,False,False,False,False,False
97.07,False,False,False,False,False,False
(Stoean &,False,False,False,False,False,False
"Stoean,2013)",False,False,False,False,False,False
Cooperative,False,False,False,False,False,False
coevolution,False,False,False,False,False,False
96.69,False,False,False,False,False,False
"(Stoean & Stoean,",False,False,False,False,False,False
2013),False,False,False,False,False,False
SVMs,False,False,False,False,False,False
96.50,False,False,False,False,False,False
"(Stoean & Stoean,",False,False,False,False,False,False
2013),False,False,False,False,False,False
Decompositìonal,False,False,False,False,False,False
95.93,False,False,False,False,False,False
"(Stoean & Stoean,",False,False,False,False,False,False
2013),False,False,False,False,False,False
Sim,False,False,False,False,False,False
97.49,False,False,False,False,False,False
"(Luukka, 2011)",False,False,False,False,False,False
Sim + F1,False,False,False,False,False,False
97.10,False,False,False,False,False,False
"(Luukka, 2011)",False,False,False,False,False,False
Sim + F2,False,False,False,False,False,False
97.18,False,False,False,False,False,False
"(Luukka, 2011)",False,False,False,False,False,False
Binary,False,False,False,False,False,False
-,False,False,False,False,False,False
coded GA,False,False,False,False,False,False
94.00,False,False,False,False,False,False
"(Örkcü & Bal, 2011)",False,False,False,False,False,False
BP,True,False,False,True,False,False
93.10,False,False,False,False,False,False
"(Örkcü & Bal, 2011)",False,False,False,False,False,False
BC FRPCA1,True,False,False,True,False,False
98.19,False,False,False,False,False,False
"(Luukka,",False,False,False,False,False,False
2009),False,False,False,False,False,False
BC FRPCA2,True,False,False,True,False,False
98.13,False,False,False,False,False,False
"(Luukka, 2009)",False,False,False,False,False,False
BC FRPCA3,True,False,False,True,False,False
98.16,False,False,False,False,False,False
"(Luukka, 2009)",False,False,False,False,False,False
BC Original,False,False,False,False,False,False
97.49,False,False,False,False,False,False
"(Luukka, 2009)",False,False,False,False,False,False
BC PCA,True,False,False,True,False,False
97.72,False,False,False,False,False,False
"(Örkcü & Bal, 2011)",False,False,False,False,False,False
Fuzzy,False,False,False,False,False,False
-,False,False,False,False,False,False
AIRS,True,False,False,True,False,False
98.51,False,False,False,False,False,False
"(Polat et al., 2007)",False,False,False,False,False,False
AIRS,True,False,False,True,False,False
97.20,False,False,False,False,False,False
"(Polat et al., 2007)",False,False,False,False,False,False
5.3 Discussion for Hepatitis Data Set,False,False,False,False,False,False
The accuracy of HPM-MI is computed,False,False,False,False,False,False
as 99.08%. We have also compared with,False,False,False,False,False,False
existing methods as shown in Table 17. It is,False,False,False,False,False,False
clearly evident from Table 18 that none of the,False,False,False,False,False,False
studies had the success rates higher than 98.52,False,False,False,False,False,False
% for the mentioned algorithms on Hepatitis,False,False,False,False,False,False
data set.,False,False,False,False,False,False
 The values of accuracy of classification made on,False,False,False,False,False,False
Hepatitis dataset.,False,False,False,False,False,False
SVR NSGA,True,False,False,True,False,False
II,True,False,False,True,False,False
98.52,False,False,False,False,False,False
"(Zangooei ,",False,False,False,False,False,False
"Habibi, &",False,False,False,False,False,False
"lizadehsani, 2014)",False,False,False,False,False,False
LFDM SVM,True,False,False,True,False,False
96.77,False,False,False,False,False,False
"(Chen et al , 2011)",False,False,False,False,False,False
LDA,True,False,False,True,False,False
-,False,False,False,False,False,False
ANFIS,True,False,False,True,False,False
94.16,False,False,False,False,False,False
(,False,False,False,False,False,False
"Dogantekin et al. ,",False,False,False,False,False,False
2009),False,False,False,False,False,False
PCA AIRS,True,False,False,True,False,False
94.12,False,False,False,False,False,False
(,False,False,False,False,False,False
"Polat and Gunes ,",False,False,False,False,False,False
2007 b),False,False,False,False,False,False
FS,True,False,False,True,False,False
–,False,False,False,False,False,False
AIRS,True,False,False,True,False,False
with fuzzy,False,False,False,False,False,False
resource,False,False,False,False,False,False
92.59,False,False,False,False,False,False
(,False,False,False,False,False,False
"Polat and Gunes ,",False,False,False,False,False,False
2006),False,False,False,False,False,False
5. 4 Robustness testing of proposed model,False,False,True,True,False,False
To show the robustness of our proposed,False,False,False,False,False,False
"model at different missing rate, we made the",False,False,False,False,False,False
experiments on 2D Synthetic Data Set at,False,False,False,False,False,False
"different missing rate as 20 %, 40 %, 60 % and",False,False,False,False,False,False
80 %. Results are produced in Table 18. Table,False,False,False,False,False,False
18 shows the incorrectly classified instances,False,False,False,False,False,False
produced by 11 imputation techniques. At,False,False,False,False,False,False
highest missing rate SVM was chosen as best,False,False,False,False,False,False
"imputation method and for other missing rate,",False,False,False,False,False,False
CMC was selected to impute the missing values,False,False,False,False,False,False
"in the data set. Moreover, proposed model has",False,False,False,False,False,False
produced good results at different missing rates.,False,False,False,False,False,False
"Table 19 summarizes accuracy, specificity,",False,False,False,False,False,True
" sensitivity, Kappa and ROC value achieved atdifferent missing rates.Table 18Clustering results with 11 imputation methods for 2Dsynthetic Data Set at different missing rate.ImputationMethodIncorrectly classified instances( %)20 %missing40 %missing60 %missing80 %missingCMC8108.58FKMI1622.519.519KMI141721.516.5KNNI1618.516.515LSSI12.522.529.512.5MC16.527.53641SVDI15.5263131SVMI810.59.55WKNN1618.516.515CaseDeletion11.3816.6824.7718.60MatrixFactorization16.524.53429Table 19Obtained classification accuracy, sensitivity andspecificity in percentage for 2D synthetic data set.MissingRate (%) 20 40 60 80Accuracy (%)10010099.45100Sensitivity (%)100100100 100Specificity (%)100 10098.86 100Kappa1 10.98 1ROC1 11 1We have also analyzed the performanceof our model as a function of missing rate andtrain-test ratio. We experimented HPM-MI onWDBC data set similar to a missing tolerantalgorithm namely logistic regression (David,2007)  on different missing rate at 25 %, 50 %and 75 % .At each missing rate, accuracy as wellas ROC for HPM-MI was calculated  bydifferent train-test ratio as shown in Table20.Accuracy varies between 98% to 100 % in allthe experiments and area under ROC varies from0.99 to 1 which is better than missing tolerantapproach proposed by David et al. (David, 2007)as AUC produced by their approach varies from0.94 to 0.99 for similar experiments.Table 20Obtained classification accuracy and ROC ondifferent missing rate and different train test ratio forWDBC Data Set.Train–TestRatio25 % Missingrate50 %Missing rate75 % MissingrateAccuracy(%)ROC Accuracy(%)ROC Accuracy(%)ROC10-9099.58 1 99.37 1 99.16 120-8098.82 1 98.82 1 99.06 130-7099.19 1 99.19 1 99.19 .9940-6099.35 1 99.37 1 99.06 0.9950-5099.24 1 99.24 1 98.86 160-4098.59 1 98.58 1 99.53 170-3098.74 1 98.11 1 99.37 180-2098.11 .99 98.11 .99 100 190-1096.23 .99 96.23 .99 100 1Table 21Obtained classification accuracy of HPM-MI,Naivebayes,NaiveBayes (MVI) Bayesnet and J48 inpercentage for WDBC synthetic data set at differentmissing rate.Missing Rate(%)255075Naïve Bayes93.1492.9793.14NaïveBayes(MVI)93.1493.4994.02BayesNet94.5594.3794.20J4894.7293.3293.32ProposedModel99.4399.2499.06Moreover, we have also comparedclassification accuracy achieved by HPM-MI onWDBC data set with missing value toleranttechniques such as naïve bayes, bayesnet andJ48 using WEKA. Table 21 shows that accuracyobtained from proposed model is 99.43%,99.24% and 99.06 at missing rate of 25%, 50%and 75% respectively that is best in comparisonto others. Table 21 also shows that comparisonbetween naïve bayes classifier (that handles thevalues by omitting the probability whilecomputing the likelihoods of membership of",False,False,False,False,False,False
" sensitivity, Kappa and ROC value achieved atdifferent missing rates.Table 18Clustering results with 11 imputation methods for 2Dsynthetic Data Set at different missing rate.ImputationMethodIncorrectly classified instances( %)20 %missing40 %missing60 %missing80 %missingCMC8108.58FKMI1622.519.519KMI141721.516.5KNNI1618.516.515LSSI12.522.529.512.5MC16.527.53641SVDI15.5263131SVMI810.59.55WKNN1618.516.515CaseDeletion11.3816.6824.7718.60MatrixFactorization16.524.53429Table 19Obtained classification accuracy, sensitivity andspecificity in percentage for 2D synthetic data set.MissingRate (%) 20 40 60 80Accuracy (%)10010099.45100Sensitivity (%)100100100 100Specificity (%)100 10098.86 100Kappa1 10.98 1ROC1 11 1We have also analyzed the performanceof our model as a function of missing rate andtrain-test ratio. We experimented HPM-MI onWDBC data set similar to a missing tolerantalgorithm namely logistic regression (David,2007)  on different missing rate at 25 %, 50 %and 75 % .At each missing rate, accuracy as wellas ROC for HPM-MI was calculated  bydifferent train-test ratio as shown in Table20.Accuracy varies between 98% to 100 % in allthe experiments and area under ROC varies from0.99 to 1 which is better than missing tolerantapproach proposed by David et al. (David, 2007)as AUC produced by their approach varies from0.94 to 0.99 for similar experiments.Table 20Obtained classification accuracy and ROC ondifferent missing rate and different train test ratio forWDBC Data Set.Train–TestRatio25 % Missingrate50 %Missing rate75 % MissingrateAccuracy(%)ROC Accuracy(%)ROC Accuracy(%)ROC10-9099.58 1 99.37 1 99.16 120-8098.82 1 98.82 1 99.06 130-7099.19 1 99.19 1 99.19 .9940-6099.35 1 99.37 1 99.06 0.9950-5099.24 1 99.24 1 98.86 160-4098.59 1 98.58 1 99.53 170-3098.74 1 98.11 1 99.37 180-2098.11 .99 98.11 .99 100 190-1096.23 .99 96.23 .99 100 1Table 21Obtained classification accuracy of HPM-MI,Naivebayes,NaiveBayes (MVI) Bayesnet and J48 inpercentage for WDBC synthetic data set at differentmissing rate.Missing Rate(%)255075Naïve Bayes93.1492.9793.14NaïveBayes(MVI)93.1493.4994.02BayesNet94.5594.3794.20J4894.7293.3293.32ProposedModel99.4399.2499.06Moreover, we have also comparedclassification accuracy achieved by HPM-MI onWDBC data set with missing value toleranttechniques such as naïve bayes, bayesnet andJ48 using WEKA. Table 21 shows that accuracyobtained from proposed model is 99.43%,99.24% and 99.06 at missing rate of 25%, 50%and 75% respectively that is best in comparisonto others. Table 21 also shows that comparisonbetween naïve bayes classifier (that handles thevalues by omitting the probability whilecomputing the likelihoods of membership of",False,False,False,False,False,False
 ,False,False,False,False,False,False
"sensitivity, Kappa and ROC value achieved at",False,False,False,False,False,False
different missing rates.,False,False,False,False,False,False
Clustering results with 11 imputation methods for 2D,False,False,False,False,False,False
synthetic Data Set at different missing rate.,False,False,False,False,False,False
CMC,True,False,False,True,False,False
8,False,False,False,False,False,False
FKMI,True,False,False,True,False,False
16,False,False,False,False,False,False
22.5,False,False,False,False,False,False
19.5,False,False,False,False,False,False
19,False,False,False,False,False,False
KMI,True,False,False,True,False,False
14,False,False,False,False,False,False
17,False,False,False,False,False,False
21.5,False,False,False,False,False,False
16.5,False,False,False,False,False,False
KNNI,True,False,False,True,False,False
16,False,False,False,False,False,False
18.5,False,False,False,False,False,False
16.5,False,False,False,False,False,False
15,False,False,False,False,False,False
LSSI,True,False,False,True,False,False
12.5,False,False,False,False,False,False
22.5,False,False,False,False,False,False
29.5,False,False,False,False,False,False
12.5,False,False,False,False,False,False
MC,True,False,False,True,False,False
16.5,False,False,False,False,False,False
27.5,False,False,False,False,False,False
36,False,False,False,False,False,False
41,False,False,False,False,False,False
SVDI,True,False,False,True,False,False
15.5,False,False,False,False,False,False
26,False,False,False,False,False,False
31,False,False,False,False,False,False
31,False,False,False,False,False,False
SVMI,True,False,False,True,False,False
8,False,False,False,False,False,False
10.5,False,False,False,False,False,False
9.5,False,False,False,False,False,False
WKNN,True,False,False,True,False,False
16,False,False,False,False,False,False
18.5,False,False,False,False,False,False
16.5,False,False,False,False,False,False
15,False,False,False,False,False,False
Case,False,False,False,False,False,False
Deletion,False,False,False,False,False,False
11.38,False,False,False,False,False,False
16.68,False,False,False,False,False,False
24.77,False,False,False,False,False,False
18.60,False,False,False,False,False,False
Matrix,False,False,False,False,False,False
Factoriz,False,False,False,False,False,False
ation,False,False,False,False,False,False
16.5,False,False,False,False,False,False
24.5,False,False,False,False,False,False
34,False,False,False,False,False,False
29,False,False,False,False,False,False
"Obtained classification accuracy, sensitivity and",False,False,False,False,False,False
specificity in percentage for 2D synthetic data set.,False,False,False,False,False,False
100,False,False,False,False,False,False
100,False,False,False,False,False,False
99.45,False,False,False,False,False,False
100,False,False,False,False,False,False
100,False,False,False,False,False,False
100,False,False,False,False,False,False
100 100,False,False,False,False,False,False
100 100,False,False,False,False,False,False
98.86 100,False,False,False,False,False,False
1 1,False,False,False,False,False,False
0.98 1,False,False,False,False,False,False
1 1,False,False,False,False,False,False
1 1,False,False,False,False,False,False
We have also analyzed the performance,False,False,False,False,False,False
of our model as a function of missing rate and,False,False,False,False,False,False
train-test ratio. We experimented HPM-MI on,False,False,False,False,False,False
WDBC data set similar to a missing tolerant,False,False,False,False,False,False
"algorithm namely logistic regression (David,",False,False,False,False,False,False
"2007)  on different missing rate at 25 %, 50 %",False,False,False,False,False,False
"and 75 % .At each missing rate, accuracy as well",False,False,False,False,False,False
as ROC for HPM-MI was calculated  by,False,False,False,False,False,False
different train-test ratio as shown in Table,False,False,False,False,False,False
20.Accuracy varies between 98% to 100 % in all,False,False,False,False,False,False
the experiments and area under ROC varies from,False,False,False,False,False,False
0.99 to 1 which is better than missing tolerant,False,False,False,False,False,False
"approach proposed by David et al. (David, 2007)",False,False,False,False,False,False
as AUC produced by their approach varies from,False,False,False,False,False,False
0.94 to 0.99 for similar experiments.,False,False,False,False,False,False
Obtained classification accuracy and ROC on,False,False,False,False,False,False
different missing rate and different train test ratio for,False,False,False,False,False,False
WDBC Data Set.,False,False,False,False,False,False
"Obtained classification accuracy of HPM-MI,",False,False,False,False,False,False
"Naivebayes,NaiveBayes (MVI) Bayesnet and J48 in",False,False,False,False,False,False
percentage for WDBC synthetic data set at different,False,False,False,False,False,False
missing rate.,False,False,False,False,False,False
93.14,False,False,False,False,False,False
92.97,False,False,False,False,False,False
93.14,False,False,False,False,False,False
93.14,False,False,False,False,False,False
93.49,False,False,False,False,False,False
94.02,False,False,False,False,False,False
94.55,False,False,False,False,False,False
94.37,False,False,False,False,False,False
94.20,False,False,False,False,False,False
94.72,False,False,False,False,False,False
93.32,False,False,False,False,False,False
93.32,False,False,False,False,False,False
"Moreover, we have also compared",False,False,False,False,False,False
classification accuracy achieved by HPM-MI on,False,False,False,False,False,False
WDBC data set with missing value tolerant,False,False,False,False,False,False
"techniques such as naïve bayes, bayesnet and",False,False,False,False,False,False
J48 using WEKA. Table 21 shows that accuracy,False,False,False,False,False,False
"obtained from proposed model is 99.43%,",False,False,False,False,False,False
"99.24% and 99.06 at missing rate of 25%, 50%",False,False,False,False,False,False
and 75% respectively that is best in comparison,False,False,False,False,False,False
to others. Table 21 also shows that comparison,False,False,False,False,False,True
between naïve bayes classifier (that handles the,False,False,False,False,False,False
values by omitting the probability while,False,False,False,False,False,False
computing the likelihoods of membership of,False,False,False,False,False,False
" each class) and naïve bayes with best missingvalue imputation approach selected from ouranalysis and results are better for naïve bayeswith imputation at higher missing rates as 50%and 75%.6. Conclusion and Future WorkThis has paper has proposed a hybridprediction model with missing value imputation,i.e., HPM-MI to support medical decisions. Thismodel comprises of analysis and selection ofimputation method using K-mean clustering,pattern extraction and MLP. Proposed model hasemphasized on the analysis and selection ofmissing value imputation techniques beforemaking any prediction as no generalization canbe made for the best imputation method relativeto incomplete data set. K-means clustering isalso incorporated to extract the correct patternsbefore applying MLP for classification. A lot ofexperiments have been performed to validate theproposed model using three medical data setsnamely Pima Indian diabetes data set, breastcancer data set, and hepatitis data set from UCIrepository. Results obtained from theexperiments shows that none of the studies haveachieved accuracy of 99.82% for Pima Indiandiabetes data set. Further, accuracy obtained forWisconsin breast cancer as well as hepatitis dataset are 99.30% and 99.08% respectively andcomparable with other models reported in theliterature. Moreover, proposed model isvalidated on two other data sets namely 2Dsynthetic and WDBC data sets at differentmissing rates. Results achieved for these twodata sets are also consistent with previous workin the literature.Future work will focus on the testingand improving the model for multi-classimbalanced classification problems. Imbalancedclassification problem arises due to imbalanceddistribution of instances among multiple classespresent in the data set. Further we would alsolike to incorporate more MVI approaches suchas BPPCA, non-linear PCA in the set of MVIapproaches under study (Scholz, Kaplan, Guy,Kopka, & Selbig, 2005). On the other hand, realworld data may consist of noise in addition tomissing values that has been neglected in ourproposed model. Therefore, it is advantageous toequip HPM-MI to deal with noisy data.  Finally,it would be nice to examine the efficiency ofHPM-MI in other domains in addition tomedical domain.ReferencesAlcalá-Fdez, J., Sánchez, L., García, S., del Jesus, M.J., Ventura, S., Garrell, J. M., Otero, J.,Romero, C., Bacardit, J., Rivas, V. M.,Fernández, J. C., & Herrera, F. (2009).KEEL: a software tool to assessevolutionary algorithms for data miningproblems. Soft Computing, 13, 307-318.Aussem, A., & de Morais, S. R. (2008). AConservative Feature Subset SelectionAlgorithm with Missing Data. In DataMining, 2008. ICDM '08. Eighth IEEEInternational Conference on (pp. 725-730).Bellazzi, R., & Zupan, B. (2008). Predictive datamining in clinical medicine: Current issuesand guidelines. International Journal ofMedical Informatics, 77, 81-97.Bennett, K. P., & Blue, J. A. (1998). A support vectormachine approach to decision trees. InNeural Networks Proceedings, 1998. IEEEWorld Congress on ComputationalIntelligence. The 1998 IEEE InternationalJoint Conference on (Vol. 3, pp. 2396-2401vol.2393).Bioch, J. C., Meer, O., & Potharst, R. . (1996).Classification using Bayesian neural nets. InInternational conference on neural networks(pp. 1488–1493).Carpenter, G. A., & Markuzon, N. (1998).ARTMAP-IC and medical diagnosis:Instance counting and inconsistent cases.Neural Networks, 11, 323-336.Chen, H.,Liu D., Yang B., Liu J., & Wang G.(2011),A new hybrid method based on local fisherdiscriminant analysis and support vectormachines for hepatitis disease diagnosis,Expert Systems with Applications, 38(9),11796-11803.Chen, H., Yang, B., Wang, G., Wang, S.,  Liu, J., &Liu, D. (2012) , Support Vector MachineBased Diagnostic System for Breast CancerUsing Swarm Intelligence. Journal ofMedical Systems, 2505-2519Cibenko, G.( 1989),Approximation by superpositionsof a sigmoidal function,"" Mathematics ofControl, Signal and systems,  2(4), 303-314.",False,False,False,True,False,False
" each class) and naïve bayes with best missingvalue imputation approach selected from ouranalysis and results are better for naïve bayeswith imputation at higher missing rates as 50%and 75%.6. Conclusion and Future WorkThis has paper has proposed a hybridprediction model with missing value imputation,i.e., HPM-MI to support medical decisions. Thismodel comprises of analysis and selection ofimputation method using K-mean clustering,pattern extraction and MLP. Proposed model hasemphasized on the analysis and selection ofmissing value imputation techniques beforemaking any prediction as no generalization canbe made for the best imputation method relativeto incomplete data set. K-means clustering isalso incorporated to extract the correct patternsbefore applying MLP for classification. A lot ofexperiments have been performed to validate theproposed model using three medical data setsnamely Pima Indian diabetes data set, breastcancer data set, and hepatitis data set from UCIrepository. Results obtained from theexperiments shows that none of the studies haveachieved accuracy of 99.82% for Pima Indiandiabetes data set. Further, accuracy obtained forWisconsin breast cancer as well as hepatitis dataset are 99.30% and 99.08% respectively andcomparable with other models reported in theliterature. Moreover, proposed model isvalidated on two other data sets namely 2Dsynthetic and WDBC data sets at differentmissing rates. Results achieved for these twodata sets are also consistent with previous workin the literature.Future work will focus on the testingand improving the model for multi-classimbalanced classification problems. Imbalancedclassification problem arises due to imbalanceddistribution of instances among multiple classespresent in the data set. Further we would alsolike to incorporate more MVI approaches suchas BPPCA, non-linear PCA in the set of MVIapproaches under study (Scholz, Kaplan, Guy,Kopka, & Selbig, 2005). On the other hand, realworld data may consist of noise in addition tomissing values that has been neglected in ourproposed model. Therefore, it is advantageous toequip HPM-MI to deal with noisy data.  Finally,it would be nice to examine the efficiency ofHPM-MI in other domains in addition tomedical domain.ReferencesAlcalá-Fdez, J., Sánchez, L., García, S., del Jesus, M.J., Ventura, S., Garrell, J. M., Otero, J.,Romero, C., Bacardit, J., Rivas, V. M.,Fernández, J. C., & Herrera, F. (2009).KEEL: a software tool to assessevolutionary algorithms for data miningproblems. Soft Computing, 13, 307-318.Aussem, A., & de Morais, S. R. (2008). AConservative Feature Subset SelectionAlgorithm with Missing Data. In DataMining, 2008. ICDM '08. Eighth IEEEInternational Conference on (pp. 725-730).Bellazzi, R., & Zupan, B. (2008). Predictive datamining in clinical medicine: Current issuesand guidelines. International Journal ofMedical Informatics, 77, 81-97.Bennett, K. P., & Blue, J. A. (1998). A support vectormachine approach to decision trees. InNeural Networks Proceedings, 1998. IEEEWorld Congress on ComputationalIntelligence. The 1998 IEEE InternationalJoint Conference on (Vol. 3, pp. 2396-2401vol.2393).Bioch, J. C., Meer, O., & Potharst, R. . (1996).Classification using Bayesian neural nets. InInternational conference on neural networks(pp. 1488–1493).Carpenter, G. A., & Markuzon, N. (1998).ARTMAP-IC and medical diagnosis:Instance counting and inconsistent cases.Neural Networks, 11, 323-336.Chen, H.,Liu D., Yang B., Liu J., & Wang G.(2011),A new hybrid method based on local fisherdiscriminant analysis and support vectormachines for hepatitis disease diagnosis,Expert Systems with Applications, 38(9),11796-11803.Chen, H., Yang, B., Wang, G., Wang, S.,  Liu, J., &Liu, D. (2012) , Support Vector MachineBased Diagnostic System for Breast CancerUsing Swarm Intelligence. Journal ofMedical Systems, 2505-2519Cibenko, G.( 1989),Approximation by superpositionsof a sigmoidal function,"" Mathematics ofControl, Signal and systems,  2(4), 303-314.",False,False,False,True,False,False
 ,False,False,False,False,False,False
each class) and naïve bayes with best missing,False,False,False,False,False,False
value imputation approach selected from our,False,False,False,False,False,False
analysis and results are better for naïve bayes,False,False,False,False,False,False
with imputation at higher missing rates as 50%,False,False,False,False,False,False
and 75%.,False,False,False,False,False,False
6. Conclusion and Future Work,False,False,True,True,False,False
This has paper has proposed a hybrid,False,False,False,False,False,False
"prediction model with missing value imputation,",False,False,False,False,False,False
"i.e., HPM-MI to support medical decisions. This",False,False,False,False,False,False
model comprises of analysis and selection of,False,False,False,False,False,False
"imputation method using K-mean clustering,",False,False,False,False,False,False
pattern extraction and MLP. Proposed model has,False,False,False,False,False,False
emphasized on the analysis and selection of,False,False,False,False,False,False
missing value imputation techniques before,False,False,False,False,False,False
making any prediction as no generalization can,False,False,False,False,False,False
be made for the best imputation method relative,False,False,False,False,False,False
to incomplete data set. K-means clustering is,False,False,False,False,False,False
also incorporated to extract the correct patterns,False,False,False,False,False,False
before applying MLP for classification. A lot of,False,False,False,False,False,False
experiments have been performed to validate the,False,False,False,False,False,False
proposed model using three medical data sets,False,False,False,False,False,False
"namely Pima Indian diabetes data set, breast",False,False,False,False,False,False
"cancer data set, and hepatitis data set from UCI",False,False,False,False,False,False
repository. Results obtained from the,False,False,False,False,False,False
experiments shows that none of the studies have,False,False,False,False,False,False
achieved accuracy of 99.82% for Pima Indian,False,False,False,False,False,False
"diabetes data set. Further, accuracy obtained for",False,False,False,False,False,False
Wisconsin breast cancer as well as hepatitis data,False,False,False,False,False,False
set are 99.30% and 99.08% respectively and,False,False,False,False,False,False
comparable with other models reported in the,False,False,False,False,False,False
"literature. Moreover, proposed model is",False,False,False,False,False,False
validated on two other data sets namely 2D,False,False,False,False,False,False
synthetic and WDBC data sets at different,False,False,False,False,False,False
missing rates. Results achieved for these two,False,False,False,False,False,False
data sets are also consistent with previous work,False,False,False,False,False,False
in the literature.,False,False,False,False,False,False
Future work will focus on the testing,False,False,False,False,False,False
and improving the model for multi-class,False,False,False,False,False,False
imbalanced classification problems. Imbalanced,False,False,False,False,False,False
classification problem arises due to imbalanced,False,False,False,False,False,False
distribution of instances among multiple classes,False,False,False,False,False,False
present in the data set. Further we would also,False,False,False,False,False,False
like to incorporate more MVI approaches such,False,False,False,False,False,False
"as BPPCA, non-linear PCA in the set of MVI",False,False,False,False,False,False
"approaches under study (Scholz, Kaplan, Guy,",False,False,False,False,False,False
"Kopka, & Selbig, 2005). On the other hand, real",False,False,False,False,False,False
world data may consist of noise in addition to,False,False,False,False,False,False
missing values that has been neglected in our,False,False,False,False,False,False
"proposed model. Therefore, it is advantageous to",False,False,False,False,False,False
"equip HPM-MI to deal with noisy data.  Finally,",False,False,False,False,False,False
it would be nice to examine the efficiency of,False,False,False,False,False,False
HPM-MI in other domains in addition to,False,False,False,False,False,False
medical domain.,False,False,False,False,False,False
References,False,False,False,True,False,False
"Alcalá-Fdez, J., Sánchez, L., García, S., del Jesus, M.",False,False,False,False,False,False
"J., Ventura, S., Garrell, J. M., Otero, J.,",False,False,False,False,False,False
"Romero, C., Bacardit, J., Rivas, V. M.,",False,False,False,False,False,False
"Fernández, J. C., & Herrera, F. (2009).",False,False,False,False,False,False
KEEL: a software tool to assess,False,False,False,False,False,False
evolutionary algorithms for data mining,False,False,False,False,False,False
"problems. Soft Computing, 13, 307-318.",False,False,False,False,False,False
"Aussem, A., & de Morais, S. R. (2008). A",False,False,False,False,False,False
Conservative Feature Subset Selection,False,False,False,False,False,False
Algorithm with Missing Data. In Data,False,False,False,False,False,False
"Mining, 2008. ICDM '08. Eighth IEEE",False,False,False,False,False,False
International Conference on (pp. 725-730).,False,False,False,False,False,False
"Bellazzi, R., & Zupan, B. (2008). Predictive data",False,False,False,False,False,False
mining in clinical medicine: Current issues,False,False,False,False,False,False
and guidelines. International Journal of,False,False,False,False,False,False
"Medical Informatics, 77, 81-97.",False,False,False,False,False,False
"Bennett, K. P., & Blue, J. A. (1998). A support vector",False,False,False,False,False,False
machine approach to decision trees. In,False,False,False,False,False,False
"Neural Networks Proceedings, 1998. IEEE",False,False,False,False,False,False
World Congress on Computational,False,False,False,False,False,False
Intelligence. The 1998 IEEE International,False,False,False,False,False,False
"Joint Conference on (Vol. 3, pp. 2396-2401",False,False,False,False,False,False
vol.2393).,False,False,False,False,False,False
"Bioch, J. C., Meer, O., & Potharst, R. . (1996).",False,False,False,False,False,False
Classification using Bayesian neural nets. In,False,False,False,False,False,False
International conference on neural networks,False,False,False,False,False,False
(pp. 1488–1493).,False,False,False,False,False,False
"Carpenter, G. A., & Markuzon, N. (1998).",False,False,False,False,False,False
ARTMAP-IC and medical diagnosis:,False,False,False,False,False,False
Instance counting and inconsistent cases.,False,False,False,False,False,False
"Neural Networks, 11, 323-336.",False,False,False,False,False,False
"Chen, H.,Liu D., Yang B., Liu J., & Wang G.(2011),",False,False,False,False,False,False
A new hybrid method based on local fisher,False,False,False,False,False,False
discriminant analysis and support vector,False,False,False,False,False,False
"machines for hepatitis disease diagnosis,",False,False,False,False,False,False
"Expert Systems with Applications, 38(9),",False,False,False,False,False,False
11796-11803.,False,False,False,False,False,False
"Chen, H., Yang, B., Wang, G., Wang, S.,  Liu, J., &",False,False,False,False,False,False
"Liu, D. (2012) , Support Vector Machine",False,False,False,False,False,False
Based Diagnostic System for Breast Cancer,False,False,False,False,False,False
Using Swarm Intelligence. Journal of,False,False,False,False,False,False
"Medical Systems, 2505-2519",False,False,False,False,False,False
"Cibenko, G.( 1989),Approximation by superpositions",False,False,False,False,False,False
"of a sigmoidal function,"" Mathematics of",False,False,False,False,False,False
"Control, Signal and systems,  2(4), 303-314.",False,False,False,False,False,False
" David, W. (2007). On Classification with IncompleteData. IEEE Transactions on PatternAnalysis and Machine Intelligence, 29, 427-436.Delen, D., Walker, G., & Kadam, A. (2005).Predicting breast cancer survivability: Acomparison of three data mining methods.Artificial Intelligence in Medicine, 34(2),113–127.Dogantekin, E., Dogantekin, A., & Avci, D.(2009). Automatic hepatitis diagnosissystem based on linear discriminant analysisand adaptive network based on fuzzyinference system. Expert Systems withApplications, 36(8), 11282–11286.Downs, J., Harrison, R. F., Kennedy, R. L., & Cross,S. S. (1996). Application of the fuzzyARTMAP neural network model to medicalpattern classification tasks. ArtificialIntelligence in Medicine, 8, 403-428.Hammer, P., & Bonates, T. (2006). Logical analysisof data—An overview: From combinatorialoptimization to medical applications. Annalsof Operations Research, 148, 203-225.Han, J., & Kamber, M. (2006). Data mining:Concepts and techniques (2nd ed.). MorganKaufmann Publishers.Hathaway, R. J., & Bezdek, J. C. (2002). Clusteringincomplete relational data using the non-Euclidean relational fuzzy c-meansalgorithm. Pattern Recognition Letters, 23,151-160.llango, S. B.,  & Ramaraj, N. (2010). A hybridprediction model with F-score featureselection for type II Diabetes databases. In1st Amrita ACM-W Celebration on Womenin Computing in India (pp. 1-4).Coimbatore, India.Kahramanli, H., & Allahverdi, N. (2008). Design of ahybrid system for the diabetes and heartdiseases. Expert Systems with Applications,35, 82-89.Kanungo, T., Mount, D. M., Netanyahu, N. S.,Piatko, C. D., Silverman, R., & Wu, A. Y.(2002). An efficient k-means clusteringalgorithm: analysis and implementation.IEEE Transactions on Pattern Analysis andMachine Intelligence, 24, 881-892.Koren, Y.  Bell, R, & Volinsky, C. (2009) Matrixfactorization techniques for recommendersystems. Computer 42.8  30-37Luengo, J.,  García, S., & Herrera, F. (2011) On thechoice of the best imputation methods formissing values considering three groups ofclassification methods. Knowledge andInformation Systems, 32, 77–108Luukka, P. (2009). Classification based on fuzzyrobust PCA algorithms and similarityclassifier. Expert Systems with Applications,36(4), 7463–7468.Luukka, P. (2011). Feature selection using fuzzyentropy measures with similarity classifier.Expert Systems with Applications, 38, 4600-4607.Meesad, P., & Yen, G. G. (2003). Combinednumerical and linguistic knowledgerepresentation and its application to medicaldiagnosis. Systems, Man and Cybernetics,Part A: Systems and Humans, IEEETransactions on, 33, 206-222.Michie, D., Spiegelhalter, D. J., & Taylor, C. C.(1994). Machine learning, neural andstatistical classification: Ellis Horwoo.Mukhopadhyay, S., Changhong, T., Huang, J.,Mulong, Y., & Palakal, M. (2002). Acomparative study of genetic sequenceclassification algorithms. In NeuralNetworks for Signal Processing, 57-66.Newman, D. J., Hettich, S., Blake, C. L., & Merz, C.J. (2007). UCI Repository of machinelearning databases. Irvine, CA: University ofCalifornia, Department of Information andComputer ScienceÖrkcü, H. H., & Bal, H. (2011). Comparingperformances of backpropagation andgenetic algorithms in the data classification.Expert Systems with Applications, 38, 3703-3709.Patil, B. M., Joshi, R. C., & Toshniwal, D. (2010).Hybrid prediction model for Type-2 diabeticpatients. Expert Systems with Applications,37, 8102-8108.Polat, K., & G¨unes, S(2006). “Hepatitis diseasediagnosis using a new hybrid system basedon feature selection (FS) and artificialimmune recognition system with fuzzyresource  allocation. Digital SignalProcessing, 16(6), pp. 889–  901Polat, K., & Gunes, S. (2007). An expert systemapproach based on principal componentanalysis and adaptive neuro-fuzzy  inferencesystem to diagnosis of diabetes disease.Digital Signal Processing, 17(4), 702–710.Polat, K., & Gunes, S. (2007). Prediction ofhepatitis disease based on principalcomponent analysis and artificial immunerecognition system. Applied Mathematicsand Computation, 189(2), 1282–1291.Polat, K., & Gunes, S. (2008). Computer aidedmedical diagnosis system based on principalcomponent analysis and artificial immunerecognition system classifier algorithm.",False,False,False,False,False,False
" David, W. (2007). On Classification with IncompleteData. IEEE Transactions on PatternAnalysis and Machine Intelligence, 29, 427-436.Delen, D., Walker, G., & Kadam, A. (2005).Predicting breast cancer survivability: Acomparison of three data mining methods.Artificial Intelligence in Medicine, 34(2),113–127.Dogantekin, E., Dogantekin, A., & Avci, D.(2009). Automatic hepatitis diagnosissystem based on linear discriminant analysisand adaptive network based on fuzzyinference system. Expert Systems withApplications, 36(8), 11282–11286.Downs, J., Harrison, R. F., Kennedy, R. L., & Cross,S. S. (1996). Application of the fuzzyARTMAP neural network model to medicalpattern classification tasks. ArtificialIntelligence in Medicine, 8, 403-428.Hammer, P., & Bonates, T. (2006). Logical analysisof data—An overview: From combinatorialoptimization to medical applications. Annalsof Operations Research, 148, 203-225.Han, J., & Kamber, M. (2006). Data mining:Concepts and techniques (2nd ed.). MorganKaufmann Publishers.Hathaway, R. J., & Bezdek, J. C. (2002). Clusteringincomplete relational data using the non-Euclidean relational fuzzy c-meansalgorithm. Pattern Recognition Letters, 23,151-160.llango, S. B.,  & Ramaraj, N. (2010). A hybridprediction model with F-score featureselection for type II Diabetes databases. In1st Amrita ACM-W Celebration on Womenin Computing in India (pp. 1-4).Coimbatore, India.Kahramanli, H., & Allahverdi, N. (2008). Design of ahybrid system for the diabetes and heartdiseases. Expert Systems with Applications,35, 82-89.Kanungo, T., Mount, D. M., Netanyahu, N. S.,Piatko, C. D., Silverman, R., & Wu, A. Y.(2002). An efficient k-means clusteringalgorithm: analysis and implementation.IEEE Transactions on Pattern Analysis andMachine Intelligence, 24, 881-892.Koren, Y.  Bell, R, & Volinsky, C. (2009) Matrixfactorization techniques for recommendersystems. Computer 42.8  30-37Luengo, J.,  García, S., & Herrera, F. (2011) On thechoice of the best imputation methods formissing values considering three groups ofclassification methods. Knowledge andInformation Systems, 32, 77–108Luukka, P. (2009). Classification based on fuzzyrobust PCA algorithms and similarityclassifier. Expert Systems with Applications,36(4), 7463–7468.Luukka, P. (2011). Feature selection using fuzzyentropy measures with similarity classifier.Expert Systems with Applications, 38, 4600-4607.Meesad, P., & Yen, G. G. (2003). Combinednumerical and linguistic knowledgerepresentation and its application to medicaldiagnosis. Systems, Man and Cybernetics,Part A: Systems and Humans, IEEETransactions on, 33, 206-222.Michie, D., Spiegelhalter, D. J., & Taylor, C. C.(1994). Machine learning, neural andstatistical classification: Ellis Horwoo.Mukhopadhyay, S., Changhong, T., Huang, J.,Mulong, Y., & Palakal, M. (2002). Acomparative study of genetic sequenceclassification algorithms. In NeuralNetworks for Signal Processing, 57-66.Newman, D. J., Hettich, S., Blake, C. L., & Merz, C.J. (2007). UCI Repository of machinelearning databases. Irvine, CA: University ofCalifornia, Department of Information andComputer ScienceÖrkcü, H. H., & Bal, H. (2011). Comparingperformances of backpropagation andgenetic algorithms in the data classification.Expert Systems with Applications, 38, 3703-3709.Patil, B. M., Joshi, R. C., & Toshniwal, D. (2010).Hybrid prediction model for Type-2 diabeticpatients. Expert Systems with Applications,37, 8102-8108.Polat, K., & G¨unes, S(2006). “Hepatitis diseasediagnosis using a new hybrid system basedon feature selection (FS) and artificialimmune recognition system with fuzzyresource  allocation. Digital SignalProcessing, 16(6), pp. 889–  901Polat, K., & Gunes, S. (2007). An expert systemapproach based on principal componentanalysis and adaptive neuro-fuzzy  inferencesystem to diagnosis of diabetes disease.Digital Signal Processing, 17(4), 702–710.Polat, K., & Gunes, S. (2007). Prediction ofhepatitis disease based on principalcomponent analysis and artificial immunerecognition system. Applied Mathematicsand Computation, 189(2), 1282–1291.Polat, K., & Gunes, S. (2008). Computer aidedmedical diagnosis system based on principalcomponent analysis and artificial immunerecognition system classifier algorithm.",False,False,False,False,False,False
 ,False,False,False,False,False,False
"David, W. (2007). On Classification with Incomplete",False,False,False,False,False,False
Data. IEEE Transactions on Pattern,False,False,False,False,False,False
"Analysis and Machine Intelligence, 29, 427-",False,False,False,False,False,False
436.,False,False,False,False,False,False
"Delen, D., Walker, G., & Kadam, A. (2005).",False,False,False,False,False,False
Predicting breast cancer survivability: A,False,False,False,False,False,False
comparison of three data mining methods.,False,False,False,False,False,False
"Artificial Intelligence in Medicine, 34(2),",False,False,False,False,False,False
113–127.,False,False,False,False,False,False
"Dogantekin, E., Dogantekin, A., & Avci, D.",False,False,False,False,False,False
(2009). Automatic hepatitis diagnosis,False,False,False,False,False,False
system based on linear discriminant analysis,False,False,False,False,False,False
and adaptive network based on fuzzy,False,False,False,False,False,False
inference system. Expert Systems with,False,False,False,False,False,False
"Applications, 36(8), 11282–11286.",False,False,False,False,False,False
"Downs, J., Harrison, R. F., Kennedy, R. L., & Cross,",False,False,False,False,False,False
S. S. (1996). Application of the fuzzy,False,False,False,False,False,False
ARTMAP neural network model to medical,False,False,False,False,False,False
pattern classification tasks. Artificial,False,False,False,False,False,False
"Intelligence in Medicine, 8, 403-428.",False,False,False,False,False,False
"Hammer, P., & Bonates, T. (2006). Logical analysis",False,False,False,False,False,False
of data—An overview: From combinatorial,False,False,False,False,False,False
optimization to medical applications. Annals,False,False,False,False,False,False
"of Operations Research, 148, 203-225.",False,False,False,False,False,False
"Han, J., & Kamber, M. (2006). Data mining:",False,False,False,False,False,False
Concepts and techniques (2nd ed.). Morgan,False,False,False,False,False,False
Kaufmann Publishers.,False,False,False,False,False,False
"Hathaway, R. J., & Bezdek, J. C. (2002). Clustering",False,False,False,False,False,False
incomplete relational data using the non-,False,False,False,False,False,False
Euclidean relational fuzzy c-means,False,False,False,False,False,False
"algorithm. Pattern Recognition Letters, 23,",False,False,False,False,False,False
151-160.,False,False,False,False,False,False
"llango, S. B.,  & Ramaraj, N. (2010). A hybrid",False,False,False,False,False,False
prediction model with F-score feature,False,False,False,False,False,False
selection for type II Diabetes databases. In,False,False,False,False,False,False
1st Amrita ACM-W Celebration on Women,False,False,False,False,False,False
in Computing in India (pp. 1-4).,False,False,False,False,False,False
"Coimbatore, India.",False,False,False,False,False,False
"Kahramanli, H., & Allahverdi, N. (2008). Design of a",False,False,False,False,False,False
hybrid system for the diabetes and heart,False,False,False,False,False,False
"diseases. Expert Systems with Applications,",False,False,False,False,False,False
"35, 82-89.",False,False,False,False,False,False
"Kanungo, T., Mount, D. M., Netanyahu, N. S.,",False,False,False,False,False,False
"Piatko, C. D., Silverman, R., & Wu, A. Y.",False,False,False,False,False,False
(2002). An efficient k-means clustering,False,False,False,False,False,False
algorithm: analysis and implementation.,False,False,False,False,False,False
IEEE Transactions on Pattern Analysis and,False,False,False,False,False,False
"Machine Intelligence, 24, 881-892.",False,False,False,False,False,False
"Koren, Y.  Bell, R, & Volinsky, C. (2009) Matrix",False,False,False,False,False,False
factorization techniques for recommender,False,False,False,False,False,False
systems. Computer 42.8  30-37,False,False,False,False,False,False
"Luengo, J.,  García, S., & Herrera, F. (2011) On the",False,False,False,False,False,False
choice of the best imputation methods for,False,False,False,False,False,False
missing values considering three groups of,False,False,False,False,False,False
classification methods. Knowledge and,False,False,False,False,False,False
"Information Systems, 32, 77–108",False,False,False,False,False,False
"Luukka, P. (2009). Classification based on fuzzy",False,False,False,False,False,False
robust PCA algorithms and similarity,False,False,False,False,False,False
"classifier. Expert Systems with Applications,",False,False,False,False,False,False
"36(4), 7463–7468.",False,False,False,False,False,False
"Luukka, P. (2011). Feature selection using fuzzy",False,False,False,False,False,False
entropy measures with similarity classifier.,False,False,False,False,False,False
"Expert Systems with Applications, 38, 4600-",False,False,False,False,False,False
4607.,False,False,False,False,False,False
"Meesad, P., & Yen, G. G. (2003). Combined",False,False,False,False,False,False
numerical and linguistic knowledge,False,False,False,False,False,False
representation and its application to medical,False,False,False,False,False,False
"diagnosis. Systems, Man and Cybernetics,",False,False,False,False,False,False
"Part A: Systems and Humans, IEEE",False,False,False,False,False,False
"Transactions on, 33, 206-222.",False,False,False,False,False,False
"Michie, D., Spiegelhalter, D. J., & Taylor, C. C.",False,False,False,False,False,False
"(1994). Machine learning, neural and",False,False,False,False,False,False
statistical classification: Ellis Horwoo.,False,False,False,False,False,False
"Mukhopadhyay, S., Changhong, T., Huang, J.,",False,False,False,False,False,False
"Mulong, Y., & Palakal, M. (2002). A",False,False,False,False,False,False
comparative study of genetic sequence,False,False,False,False,False,False
classification algorithms. In Neural,False,False,False,False,False,False
"Networks for Signal Processing, 57-66.",False,False,False,False,False,False
"Newman, D. J., Hettich, S., Blake, C. L., & Merz, C.",False,False,False,False,False,False
J. (2007). UCI Repository of machine,False,False,False,False,False,False
"learning databases. Irvine, CA: University of",False,False,False,False,False,False
"California, Department of Information and",False,False,False,False,False,False
Computer Science,False,False,False,False,False,False
"Örkcü, H. H., & Bal, H. (2011). Comparing",False,False,False,False,False,False
performances of backpropagation and,False,False,False,False,False,False
genetic algorithms in the data classification.,False,False,False,False,False,False
"Expert Systems with Applications, 38, 3703-",False,False,False,False,False,False
3709.,False,False,False,False,False,False
"Patil, B. M., Joshi, R. C., & Toshniwal, D. (2010).",False,False,False,False,False,False
Hybrid prediction model for Type-2 diabetic,False,False,False,False,False,False
"patients. Expert Systems with Applications,",False,False,False,False,False,False
"37, 8102-8108.",False,False,False,False,False,False
"Polat, K., & G¨unes, S(2006). “Hepatitis disease",False,False,False,False,False,False
diagnosis using a new hybrid system based,False,False,False,False,False,False
on feature selection (FS) and artificial,False,False,False,False,False,False
immune recognition system with fuzzy,False,False,False,False,False,False
resource  allocation. Digital Signal,False,False,False,False,False,False
"Processing, 16(6), pp. 889–  901",False,False,False,False,False,False
"Polat, K., & Gunes, S. (2007). An expert system",False,False,False,False,False,False
approach based on principal component,False,False,False,False,False,False
analysis and adaptive neuro-fuzzy  inference,False,False,False,False,False,False
system to diagnosis of diabetes disease.,False,False,False,False,False,False
"Digital Signal Processing, 17(4), 702–710.",False,False,False,False,False,False
"Polat, K., & Gunes, S. (2007). Prediction of",False,False,False,False,False,False
hepatitis disease based on principal,False,False,False,False,False,False
component analysis and artificial immune,False,False,False,False,False,False
recognition system. Applied Mathematics,False,False,False,False,False,False
"and Computation, 189(2), 1282–1291.",False,False,False,False,False,False
"Polat, K., & Gunes, S. (2008). Computer aided",False,False,False,False,False,False
medical diagnosis system based on principal,False,False,False,False,False,False
component analysis and artificial immune,False,False,False,False,False,False
recognition system classifier algorithm.,False,False,False,False,False,False
" Expert Systems with Applications, 34(1),773–779.Polat, K., Sahan, S., Kodaz, H., & Günes, S. (2007).Breast cancer and liver disordersclassification using artificial immunerecognition system (AIRS) withperformance evaluation by fuzzy resourceallocation  mechanism. Expert Systems withApplications, 32(1), 172–183.Qin, Y., Zhang, S., Zhu, X., Zhang, J., & Zhang, C.(2007). Semi-parametric optimization formissing data imputation. AppliedIntelligence, 27, 79-88.Quinlan, J. (1996). Improved use of continuousattribute in C4.5. Journal of ArtificialIntelligence Research, 4, 77-90.Rumelhart, D. E., Geoffrey E. Hinton, and R. J.Williams. (1986). Learning InternalRepresentations by Error Propagation (Vol.1: Foundations. ): MIT Press.Saar-Tsechansky M, P. F. (2007). Handling missingvalues when applying classification models.The Journal of Machine Learning Research,8, 1625–1657Saastamoinen, K., & Ketola, J. (2006). Medical DataClassification using Logical SimilarityBased Measures. IEEE Conference onCybernetics and Intelligent Systems, (1-5).Scholz, M. Kaplan, F. Guy, C. L. Kopka, J. ,andSelbig, J. (2005 )Non-linear PCA: amissing data approach, InBioinformatics, Vol. 21, Number 20, pp.3887-3895, Oxford University Press,2005Seera, M., & Lim, C. P. (2014). A hybrid intelligentsystem for medical data classification.Expert Systems with Applications, 41, 2239-2249.Ster, B.,  &  Dobinkar, A  (1996), Neural Networksin medical diagnosis: Comparison withothers  methods. In proceeding ofinternational conference on engineeringapplications of neural networks, pp 427-430.Stoean, R., & Stoean, C. (2013). Modeling medicaldecision making by support vectormachines, explaining by rules ofevolutionary algorithms with featureselection. Expert Systems withApplications, 40(7), 2677–2686. Takács, G., Pilászy,  I., & Németh, B. (2008 ). Matrix factorization and neighbor basedalgorithms for the Netflix prize problem.ACM Conference on RecommenderSystems, 267-274.Thora, J., Ebba, T., Helgi, S., & Sven, S. (2008). Thefeasibility of constructing a predictiveoutcome model for breast cancer using thetools of data mining. Expert Systems withApplications, 34, 108–118Tsirogiannis, G. L., Frossyniotis, D., Stoitsis, J.,Golemati, S., Stafylopatis, A., & Nikita, K.S. (2004). Classification of medical datawith a robust multi-level combinationscheme., In Neural Networks  Proceedings.IEEE International Joint Conference on(Vol. 3, pp. 2483-2487 vol.2483).Witten, I. H., & Frank, E. (2000). Data mining:practical machine learning tools andtechniques with Java implementations. SanFrancisco: Morgan KaufmanZangooei M. H., Habibi J., & Alizadehsani R. (2014).Disease Diagnosis with a hybrid method SVRusing NSGA-II, Neurocomputing, 136, 14-29.",False,False,False,False,False,False
" Expert Systems with Applications, 34(1),773–779.Polat, K., Sahan, S., Kodaz, H., & Günes, S. (2007).Breast cancer and liver disordersclassification using artificial immunerecognition system (AIRS) withperformance evaluation by fuzzy resourceallocation  mechanism. Expert Systems withApplications, 32(1), 172–183.Qin, Y., Zhang, S., Zhu, X., Zhang, J., & Zhang, C.(2007). Semi-parametric optimization formissing data imputation. AppliedIntelligence, 27, 79-88.Quinlan, J. (1996). Improved use of continuousattribute in C4.5. Journal of ArtificialIntelligence Research, 4, 77-90.Rumelhart, D. E., Geoffrey E. Hinton, and R. J.Williams. (1986). Learning InternalRepresentations by Error Propagation (Vol.1: Foundations. ): MIT Press.Saar-Tsechansky M, P. F. (2007). Handling missingvalues when applying classification models.The Journal of Machine Learning Research,8, 1625–1657Saastamoinen, K., & Ketola, J. (2006). Medical DataClassification using Logical SimilarityBased Measures. IEEE Conference onCybernetics and Intelligent Systems, (1-5).Scholz, M. Kaplan, F. Guy, C. L. Kopka, J. ,andSelbig, J. (2005 )Non-linear PCA: amissing data approach, InBioinformatics, Vol. 21, Number 20, pp.3887-3895, Oxford University Press,2005Seera, M., & Lim, C. P. (2014). A hybrid intelligentsystem for medical data classification.Expert Systems with Applications, 41, 2239-2249.Ster, B.,  &  Dobinkar, A  (1996), Neural Networksin medical diagnosis: Comparison withothers  methods. In proceeding ofinternational conference on engineeringapplications of neural networks, pp 427-430.Stoean, R., & Stoean, C. (2013). Modeling medicaldecision making by support vectormachines, explaining by rules ofevolutionary algorithms with featureselection. Expert Systems withApplications, 40(7), 2677–2686. Takács, G., Pilászy,  I., & Németh, B. (2008 ). Matrix factorization and neighbor basedalgorithms for the Netflix prize problem.ACM Conference on RecommenderSystems, 267-274.Thora, J., Ebba, T., Helgi, S., & Sven, S. (2008). Thefeasibility of constructing a predictiveoutcome model for breast cancer using thetools of data mining. Expert Systems withApplications, 34, 108–118Tsirogiannis, G. L., Frossyniotis, D., Stoitsis, J.,Golemati, S., Stafylopatis, A., & Nikita, K.S. (2004). Classification of medical datawith a robust multi-level combinationscheme., In Neural Networks  Proceedings.IEEE International Joint Conference on(Vol. 3, pp. 2483-2487 vol.2483).Witten, I. H., & Frank, E. (2000). Data mining:practical machine learning tools andtechniques with Java implementations. SanFrancisco: Morgan KaufmanZangooei M. H., Habibi J., & Alizadehsani R. (2014).Disease Diagnosis with a hybrid method SVRusing NSGA-II, Neurocomputing, 136, 14-29.",False,False,False,False,False,False
 ,False,False,False,False,False,False
"Expert Systems with Applications, 34(1),",False,False,False,False,False,False
773–779.,False,False,False,False,False,False
"Polat, K., Sahan, S., Kodaz, H., & Günes, S. (2007).",False,False,False,False,False,False
Breast cancer and liver disorders,False,False,False,False,False,False
classification using artificial immune,False,False,False,False,False,False
recognition system (AIRS) with,False,False,False,False,False,False
performance evaluation by fuzzy resource,False,False,False,False,False,False
allocation  mechanism. Expert Systems with,False,False,False,False,False,False
"Applications, 32(1), 172–183.",False,False,False,False,False,False
"Qin, Y., Zhang, S., Zhu, X., Zhang, J., & Zhang, C.",False,False,False,False,False,False
(2007). Semi-parametric optimization for,False,False,False,False,False,False
missing data imputation. Applied,False,False,False,False,False,False
"Intelligence, 27, 79-88.",False,False,False,False,False,False
"Quinlan, J. (1996). Improved use of continuous",False,False,False,False,False,False
attribute in C4.5. Journal of Artificial,False,False,False,False,False,False
"Intelligence Research, 4, 77-90.",False,False,False,False,False,False
"Rumelhart, D. E., Geoffrey E. Hinton, and R. J.",False,False,False,False,False,False
Williams. (1986). Learning Internal,False,False,False,False,False,False
Representations by Error Propagation (Vol.,False,False,False,False,False,False
1: Foundations. ): MIT Press.,False,False,False,False,False,False
"Saar-Tsechansky M, P. F. (2007). Handling missing",False,False,False,False,False,False
values when applying classification models.,False,False,False,False,False,False
"The Journal of Machine Learning Research,",False,False,False,False,False,False
"8, 1625–1657",False,False,False,False,False,False
"Saastamoinen, K., & Ketola, J. (2006). Medical Data",False,False,False,False,False,False
Classification using Logical Similarity,False,False,False,False,False,False
Based Measures. IEEE Conference on,False,False,False,False,False,False
"Cybernetics and Intelligent Systems, (1-5).",False,False,False,False,False,False
"Scholz, M. Kaplan, F. Guy, C. L. Kopka, J. ,and",False,False,False,False,False,False
"Selbig, J. (2005 )Non-linear PCA: a",False,False,False,False,False,False
"missing data approach, In",False,False,False,False,False,False
"Bioinformatics, Vol. 21, Number 20, pp.",False,False,False,False,False,False
"3887-3895, Oxford University Press,",False,False,False,False,False,False
2005,False,False,False,False,False,False
"Seera, M., & Lim, C. P. (2014). A hybrid intelligent",False,False,False,False,False,False
system for medical data classification.,False,False,False,False,False,False
"Expert Systems with Applications, 41, 2239-",False,False,False,False,False,False
2249.,False,False,False,False,False,False
"Ster, B.,  &  Dobinkar, A  (1996), Neural Networks",False,False,False,False,False,False
in medical diagnosis: Comparison with,False,False,False,False,False,False
others  methods. In proceeding of,False,False,False,False,False,False
international conference on engineering,False,False,False,False,False,False
"applications of neural networks, pp 427-430.",False,False,False,False,False,False
"Stoean, R., & Stoean, C. (2013). Modeling medical",False,False,False,False,False,False
decision making by support vector,False,False,False,False,False,False
"machines, explaining by rules of",False,False,False,False,False,False
evolutionary algorithms with feature,False,False,False,False,False,False
selection. Expert Systems with,False,False,False,False,False,False
"Applications, 40(7), 2677–2686.",False,False,False,False,False,False
" Takács, G., Pilászy,  I., & Németh, B. (2008 ).",False,False,False,False,False,False
 Matrix factorization and neighbor based,False,False,False,False,False,False
algorithms for the Netflix prize problem.,False,False,False,False,False,False
ACM Conference on Recommender,False,False,False,False,False,False
"Systems, 267-274.",False,False,False,False,False,False
"Thora, J., Ebba, T., Helgi, S., & Sven, S. (2008). The",False,False,False,False,False,False
feasibility of constructing a predictive,False,False,False,False,False,False
outcome model for breast cancer using the,False,False,False,False,False,False
tools of data mining. Expert Systems with,False,False,False,False,False,False
"Applications, 34, 108–118",False,False,False,False,False,False
"Tsirogiannis, G. L., Frossyniotis, D., Stoitsis, J.,",False,False,False,False,False,False
"Golemati, S., Stafylopatis, A., & Nikita, K.",False,False,False,False,False,False
S. (2004). Classification of medical data,False,False,False,False,False,False
with a robust multi-level combination,False,False,False,False,False,False
"scheme., In Neural Networks  Proceedings.",False,False,False,False,False,False
IEEE International Joint Conference on,False,False,False,False,False,False
"(Vol. 3, pp. 2483-2487 vol.2483).",False,False,False,False,False,False
"Witten, I. H., & Frank, E. (2000). Data mining:",False,False,False,False,False,False
practical machine learning tools and,False,False,False,False,False,False
techniques with Java implementations. San,False,False,False,False,False,False
Francisco: Morgan Kaufman,False,False,False,False,False,False
"Zangooei M. H., Habibi J., & Alizadehsani R. (2014).",False,False,False,False,False,False
Disease Diagnosis with a hybrid method SVR,False,False,False,False,False,False
"using NSGA-II, Neurocomputing, 136, 14-29.",False,False,False,False,False,False
"  Highlights   Proposed novel Hybrid prediction model with missing value imputation. HPM-MI has improved accuracy, sensitivity, specificity, kappa & ROC on 2 datasets The best accuracy is achieved for diabetes and breast cancer datasets MVI is one of the important step of proposed model  ",False,False,False,False,False,False
"  Highlights   Proposed novel Hybrid prediction model with missing value imputation. HPM-MI has improved accuracy, sensitivity, specificity, kappa & ROC on 2 datasets The best accuracy is achieved for diabetes and breast cancer datasets MVI is one of the important step of proposed model  ",False,False,False,False,False,False
 ,False,False,False,False,False,False
