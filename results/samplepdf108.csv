Text,Is Capitalized,Is Roman Numeral,Is Number,is_heading,is_figure_heading,is_table_heading
of ,False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Environment 8) c* /* --v--w-- C- Figure 1: Events in the virtual  environment addfact Figure 1: Events in the virtual  environment addfacts  to the expert system fact  database, matching rules and causing an inte matching rules and causing an intelligent response. : to the expert system fact database, llligent response. Natural  Language Processimg Natural Language Processing (NLP)  provides a  theoretical framework for interpreting and representing multimodal  input. NLP  is  traditionally  concerned with  understanding of  written or spoken language, but may be modified to  incorporate other input modalities. In  our approach natural language techniques determine the representation scheme, while  an expert system specifies how  these representations are manipulated  in  an intelligent  way.  To  do  this  we  draw  on  established NL techniques such as Conceptual Dependencies and Scripts. A common theme in  the natural language literature is  that successful systems use a combination of top down and bottom up  processing, applying  contextual  knowledge to  assist  in’ low-level  parsing.  For  example,  BORIS[lO]  is  a  program which  reads stories and responds to  questions about  them. Understanding begins with bottom-up parsing of  textual input which  activates  higher  level  semantic  structures.  These structures  are  used  to  make  predictions  about  possible meaning which  guides processing of  subsequent input  in  a top-down manner. In  a similar  way,  intelligent  virtual  environments -should bring  semantic and contextual knowledge to  bear as early  as possible  in  the  parsing  process,  increasing  recognition accuracy. Also,  by  blending  conceptual representation of multimodal  input  at the lowest  semantic level  possible,  the user can switch input  modalities at any time  without  loss of understanding by the system [ 111. Expert System An  expert system can provide both  low  and high  level understanding of  the events occurring in  the virtual  world.  It does this  by  effectively  integrating  voice and gestural input into  a single  semantic form  which  can be matched against procedural and contextual knowledge to make inferences about user actions.  In  this  way  the  system  can recognize when specific actions occur in  the virtual  environment and provide automatic intelligent feedback. We use a rule-based system consisting of  expert knowledge encoded in  a  set of  if-then  production rules[l2],  such as: if FACT-O  then DO-THIS,  where FACT-O is  the  event necessary for  rule  activation and DO-THIS  the  consequence of  rule  activation.  The  user’s multimodal  input  and other events in  the  virtual  environment  generate facts  which  are passed to the expert system fact database and matched against the  appropriate rules,  causing  an  intelligent  response, as shown in Figure 1. Rule-based expert systems have previously been applied to virtual environments with  the KARMA  training  system.[ 131. In this case the rule-base was used to determine when to  show augmented reality  training  images to  the  user.  Stansfield et. al. [14] also use a rule-based expert system to monitor  low level events in a virtual  environment training application. Our approach is  more general in  that  it  supports both  low  level interpretation  of  multimodal  input and  higher  level understanding. Our  expert  system  rule-base  uses  various  levels  of understanding and knowledge representation, each based on those  below  it  and  each  progressively  more  knowledge intensive, as shown in  Figure  2  overleaf. The  lowest  level, feature based understanding, contains rules which identify  and respond to low level events such as body position. At  the next level,  semantic understanding, rules identify  and respond to isolated actions, such as the user giving a combined voice and gestural command. Next  there are rules  which  identify  and respond to  different  contexts  (groups  of  related  events). Finally,  pragmatic  understanding identifies  where  in  a sequence of tasks the user is, what they have already done and what  they  should  be  doing  next.  Each of  these levels  of understanding relies  on  those  below  it  and  contextual knowledge is applied at all levels to ensure we have both  top- down and bottom-up processing. 169 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Environment 8) c* /* --v--w-- C- Figure 1: Events in the virtual  environment addfact Figure 1: Events in the virtual  environment addfacts  to the expert system fact  database, matching rules and causing an inte matching rules and causing an intelligent response. : to the expert system fact database, llligent response. Natural  Language Processimg Natural Language Processing (NLP)  provides a  theoretical framework for interpreting and representing multimodal  input. NLP  is  traditionally  concerned with  understanding of  written or spoken language, but may be modified to  incorporate other input modalities. In  our approach natural language techniques determine the representation scheme, while  an expert system specifies how  these representations are manipulated  in  an intelligent  way.  To  do  this  we  draw  on  established NL techniques such as Conceptual Dependencies and Scripts. A common theme in  the natural language literature is  that successful systems use a combination of top down and bottom up  processing, applying  contextual  knowledge to  assist  in’ low-level  parsing.  For  example,  BORIS[lO]  is  a  program which  reads stories and responds to  questions about  them. Understanding begins with bottom-up parsing of  textual input which  activates  higher  level  semantic  structures.  These structures  are  used  to  make  predictions  about  possible meaning which  guides processing of  subsequent input  in  a top-down manner. In  a similar  way,  intelligent  virtual  environments -should bring  semantic and contextual knowledge to  bear as early  as possible  in  the  parsing  process,  increasing  recognition accuracy. Also,  by  blending  conceptual representation of multimodal  input  at the lowest  semantic level  possible,  the user can switch input  modalities at any time  without  loss of understanding by the system [ 111. Expert System An  expert system can provide both  low  and high  level understanding of  the events occurring in  the virtual  world.  It does this  by  effectively  integrating  voice and gestural input into  a single  semantic form  which  can be matched against procedural and contextual knowledge to make inferences about user actions.  In  this  way  the  system  can recognize when specific actions occur in  the virtual  environment and provide automatic intelligent feedback. We use a rule-based system consisting of  expert knowledge encoded in  a  set of  if-then  production rules[l2],  such as: if FACT-O  then DO-THIS,  where FACT-O is  the  event necessary for  rule  activation and DO-THIS  the  consequence of  rule  activation.  The  user’s multimodal  input  and other events in  the  virtual  environment  generate facts  which  are passed to the expert system fact database and matched against the  appropriate rules,  causing  an  intelligent  response, as shown in Figure 1. Rule-based expert systems have previously been applied to virtual environments with  the KARMA  training  system.[ 131. In this case the rule-base was used to determine when to  show augmented reality  training  images to  the  user.  Stansfield et. al. [14] also use a rule-based expert system to monitor  low level events in a virtual  environment training application. Our approach is  more general in  that  it  supports both  low  level interpretation  of  multimodal  input and  higher  level understanding. Our  expert  system  rule-base  uses  various  levels  of understanding and knowledge representation, each based on those  below  it  and  each  progressively  more  knowledge intensive, as shown in  Figure  2  overleaf. The  lowest  level, feature based understanding, contains rules which identify  and respond to low level events such as body position. At  the next level,  semantic understanding, rules identify  and respond to isolated actions, such as the user giving a combined voice and gestural command. Next  there are rules  which  identify  and respond to  different  contexts  (groups  of  related  events). Finally,  pragmatic  understanding identifies  where  in  a sequence of tasks the user is, what they have already done and what  they  should  be  doing  next.  Each of  these levels  of understanding relies  on  those  below  it  and  contextual knowledge is applied at all levels to ensure we have both  top- down and bottom-up processing. 169 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
FACT-O ,True,False,False,True,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Understandmg  Level  Prior  Knowledge  Used  Types  of  Rules  Matched Pragmatic Understanding Context, Semantic, Feature  Sequences of actions: t if user has made an incision  they should remove the tumor  next. Context Recognition t Context, Semantic, Feature  Sets of related actions: if user is holding a scalpel and touching skin they must be making an  incision. Semantic Understanding t Context, Feature Single actions: if user moves an object from  the floor  to the table then put  it back on the floor. Feature Understanding  Context  Single Events: if collision  is detected sound a warning. Figure  2: The diRerent levels of understanding and the types of rules matched at each level. Previous  multimodal  interfaces have  provided  feature based or semantic understanding of user input  [IS, 161, but our  approach  adds context  recognition  and  pragmatic understanding, vital  for  designing  applications  such  as intelligent  simulators. In the next two  sections we present two  prototype  interfaces which  demonstrate how  this  is accomplished. Feature  and  Semantic  Understanding The  first  prototype  was  designed to  allow  users  to communicate  with  an  intelligent  agent  in  a  virtual environment  using  multimodal  input.  Until  recently computer interfaces have typically  used variations of  direct or  indirect  manipulation  paradigms. The  former  has  the advantage of  being  highly  intuitive  and offering  a natural representation of  the task objects and actions[l7],  while the  latter  allows  manipulation  of  objects  that  are  not directly  in  view[6]  or  have  no  visual  representation. Intelligent  agents permit  a third  type  of  interaction.  Users can give agents commands by  direct manipulation  and the agents  in  turn  interact  indirectly  with  the  underlying applications,  thus  combining  the  advantages of  both paradigms. As  a  demonstration  of  agent  interaction  we  have developed a  simple  design  application  consisting  of  a virtual  house with  a number of  rooms  and objects, and an intelligent  agent represented by  a robot.  To  remodel the house the user can interact with  the  environment  directly or  use  voice  and gestures to  command the  agent.  The agent is  able to  freely  add, delete, move,  scale or  color objects in response to user input. The agent can also affect the user directly,  such as moving  them to new locations or attaching  objects  to  their  virtual  body.  In  this  way  the agent can be used as a navigational  aid or  to  assist in  the collection  of  objects. The  agent maintains  its  own  model of  the environment  which  is  continuously  updated as the virtual world changes. Graphics rendering,  interface devices, and  interactions were  controlled  using  Division  Ltd’s  dVS  and  dVISE software.  The  expert  system  and  speech  recognizing software were developed using  CLIPS,  an expert  system shell[ 181. The  virtual  environment  runs  on  a  Silicon Graphics Onyx  computer,  while  the expert system,  voice recognizer and  synthetic  speech output  run  on  a  DEC Alpha. The  computers are connected via  UNIX  sockets to enable  the  expert  system  to  query  the  virtual  world database for  object  information  and  issue  commands to modify  the  virtual  environment.  The  voice  recognition software permits  speaker-dependent recognition  with  a  70 phrase vocabulary, while  gestures are limited  to  pointing and grasping. The  rule-based expert  system  interprets  the  user’s multimodal  commands, plans  how  to  execute them  and sends commands to  the virtual  environment  to  carry  out these  plans.  In  doing  so,  syntactic  features  are  first extracted from  the  user’s  raw  input,  and  then  semantic meaning  assigned to  these  features.  Integration  of  the voice and gestural input occurs at the semantic level  where a unified  representation is  created before being  passed to the agent planning module. Syntactic  Feature  Extraction We  define  syntactic  features  as  the  smallest interpretable  units  of  input  for  each modality.  In  our interface these include words and phrases for  verbal input, whole  gestures  for  gestural  input  and  position  and 170 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Understandmg  Level  Prior  Knowledge  Used  Types  of  Rules  Matched Pragmatic Understanding Context, Semantic, Feature  Sequences of actions: t if user has made an incision  they should remove the tumor  next. Context Recognition t Context, Semantic, Feature  Sets of related actions: if user is holding a scalpel and touching skin they must be making an  incision. Semantic Understanding t Context, Feature Single actions: if user moves an object from  the floor  to the table then put  it back on the floor. Feature Understanding  Context  Single Events: if collision  is detected sound a warning. Figure  2: The diRerent levels of understanding and the types of rules matched at each level. Previous  multimodal  interfaces have  provided  feature based or semantic understanding of user input  [IS, 161, but our  approach  adds context  recognition  and  pragmatic understanding, vital  for  designing  applications  such  as intelligent  simulators. In the next two  sections we present two  prototype  interfaces which  demonstrate how  this  is accomplished. Feature  and  Semantic  Understanding The  first  prototype  was  designed to  allow  users  to communicate  with  an  intelligent  agent  in  a  virtual environment  using  multimodal  input.  Until  recently computer interfaces have typically  used variations of  direct or  indirect  manipulation  paradigms. The  former  has  the advantage of  being  highly  intuitive  and offering  a natural representation of  the task objects and actions[l7],  while the  latter  allows  manipulation  of  objects  that  are  not directly  in  view[6]  or  have  no  visual  representation. Intelligent  agents permit  a third  type  of  interaction.  Users can give agents commands by  direct manipulation  and the agents  in  turn  interact  indirectly  with  the  underlying applications,  thus  combining  the  advantages of  both paradigms. As  a  demonstration  of  agent  interaction  we  have developed a  simple  design  application  consisting  of  a virtual  house with  a number of  rooms  and objects, and an intelligent  agent represented by  a robot.  To  remodel the house the user can interact with  the  environment  directly or  use  voice  and gestures to  command the  agent.  The agent is  able to  freely  add, delete, move,  scale or  color objects in response to user input. The agent can also affect the user directly,  such as moving  them to new locations or attaching  objects  to  their  virtual  body.  In  this  way  the agent can be used as a navigational  aid or  to  assist in  the collection  of  objects. The  agent maintains  its  own  model of  the environment  which  is  continuously  updated as the virtual world changes. Graphics rendering,  interface devices, and  interactions were  controlled  using  Division  Ltd’s  dVS  and  dVISE software.  The  expert  system  and  speech  recognizing software were developed using  CLIPS,  an expert  system shell[ 181. The  virtual  environment  runs  on  a  Silicon Graphics Onyx  computer,  while  the expert system,  voice recognizer and  synthetic  speech output  run  on  a  DEC Alpha. The  computers are connected via  UNIX  sockets to enable  the  expert  system  to  query  the  virtual  world database for  object  information  and  issue  commands to modify  the  virtual  environment.  The  voice  recognition software permits  speaker-dependent recognition  with  a  70 phrase vocabulary, while  gestures are limited  to  pointing and grasping. The  rule-based expert  system  interprets  the  user’s multimodal  commands, plans  how  to  execute them  and sends commands to  the virtual  environment  to  carry  out these  plans.  In  doing  so,  syntactic  features  are  first extracted from  the  user’s  raw  input,  and  then  semantic meaning  assigned to  these  features.  Integration  of  the voice and gestural input occurs at the semantic level  where a unified  representation is  created before being  passed to the agent planning module. Syntactic  Feature  Extraction We  define  syntactic  features  as  the  smallest interpretable  units  of  input  for  each modality.  In  our interface these include words and phrases for  verbal input, whole  gestures  for  gestural  input  and  position  and 170 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
if ,False,False,False,False,False,False
if ,False,False,False,False,False,False
of ,False,False,False,False,False,False
of ,False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"orientation  coordinates for  the  user’s  location.  Other events such as object  collisions  are also recorded. These features are isolated and a syntactic representation assigned to each of them. Speech recognition is  achieved using the statistical techniques outlined in  1191 and head location is mad from  magnetic polhemus sensors. Gestural input  is through a six  degree of freedom wand and is  restricted to pointing and picking. These feature representations are used to  match against rules for monitoring  user position  and collisions  and as a basis for higher level understanding. For  example, when the  users hand collides with  an object  bounding  box  a collision  fact  is  inserted into  the  expert  system  fact database. Rules can be written  to  match certain collision facts and so  cause events to  occur in  response to  users touching or picking  objects. In  a similar  way events can occur when the user moves to  specific locations or  puts objects in certain spots. Semantic Representation Once the  significant  features have been extracted, the associated meaning can be found, in  much the same way that  humans recognize words in  a  conversation before trying  to  infer  meaning.  To  support  higher  level understanding, we  need a  semantic representation which satisfies two requirements[20]: l If  two  separate inputs  have the  same meaning they should have the same representation. . Information  implicitly  stated in  the  input  should be represented explicitly. There are a  number  of  suitable  representations, all developed from the theory that language is based on a very small  number  of  semantic  objects  and  the  relations between them[21].  In  our  interface we  use Conceptual Dependencies (CDs)[22]  for  semantic representation and combine groups of  CDs  into  Scripts[23]  for  pragmatic understanding. Conceptual  Dependencies  essentially  attempt  to represent every action as the composition  of  one or more primitive  actions[22].  CDs  use  a  simple  structure  to represent events, designed to  ensure that  no  matter what form  a  description  of  the  event  takes,  the  CD representation will  always  be  the  same.  Every  CD structure has a number of  empty  slots  that  need to  be tilled,  such as: an ACTOR an ACTION  performed by that actor an OBJECT that the action is performed upon a DIRECTION  in which the action is performed. The number and type of  slots depend on the particular primitive  used. For  example, the  “ATRANS”  primitive represents abstract  transfer  of  possession,  control  or ownership.  Using  ATRANS  we  can  form  CD representations of the following  sentences: “John gave Mary  a ball” “Mary  took a ball from John” actor:  John  actor:  Mary action:  ATRANS action:  ATRANS object: ball  object:  ball direction: TO Mary direction:  TO Mary FROM  John FROM John Although  both  sentences differ markedly syntactically, they have nearly the  same semantic  meaning and so the CDs are almost identical. Other primitives include: PTRANS  Physical transfer of location ATTEND  Focus a sense organ SPEAK Make a sound GRASP Grasp an object BUILD  Construct an object MTRANS  Mental transfer of information The primitive  chosen is  determined by  verb analysis - “gave”  and “took”  activates ATRANS,  “look”  implies ATTEND  etc.  More  complex sentences, and even entire passages can be represented by linking CDs together. Conceptual Dependencies have been used successfully for  text-based natural language processing, however with small  modifications  multimodal  input  can  also  be represented in  a CD  form.  Instead of  extracting primitive actions from speech only,  the user’s gestures, actions and location can also be analyzed for  verb equivalents.  For example, in  our virtual  environment a user could move a block  from  the  floor  to  a  table  top  with  the  vocal command “Put the block on top of  the table”.  This  would have the CD representation: actor: user action: PTRANS object:  block direction:  TO table top FROM  floor Alternatively,  the user could  pick  up  the  block  and move it  to  the table top  without  saying  anything.  The gesture of  picking  up a block  would cause the PTRANS primitive  to  be  activated and the  appropriate procedures called to fill  its object and direction slots. This would also produce the same CD representation as above. 171 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"orientation  coordinates for  the  user’s  location.  Other events such as object  collisions  are also recorded. These features are isolated and a syntactic representation assigned to each of them. Speech recognition is  achieved using the statistical techniques outlined in  1191 and head location is mad from  magnetic polhemus sensors. Gestural input  is through a six  degree of freedom wand and is  restricted to pointing and picking. These feature representations are used to  match against rules for monitoring  user position  and collisions  and as a basis for higher level understanding. For  example, when the  users hand collides with  an object  bounding  box  a collision  fact  is  inserted into  the  expert  system  fact database. Rules can be written  to  match certain collision facts and so  cause events to  occur in  response to  users touching or picking  objects. In  a similar  way events can occur when the user moves to  specific locations or  puts objects in certain spots. Semantic Representation Once the  significant  features have been extracted, the associated meaning can be found, in  much the same way that  humans recognize words in  a  conversation before trying  to  infer  meaning.  To  support  higher  level understanding, we  need a  semantic representation which satisfies two requirements[20]: l If  two  separate inputs  have the  same meaning they should have the same representation. . Information  implicitly  stated in  the  input  should be represented explicitly. There are a  number  of  suitable  representations, all developed from the theory that language is based on a very small  number  of  semantic  objects  and  the  relations between them[21].  In  our  interface we  use Conceptual Dependencies (CDs)[22]  for  semantic representation and combine groups of  CDs  into  Scripts[23]  for  pragmatic understanding. Conceptual  Dependencies  essentially  attempt  to represent every action as the composition  of  one or more primitive  actions[22].  CDs  use  a  simple  structure  to represent events, designed to  ensure that  no  matter what form  a  description  of  the  event  takes,  the  CD representation will  always  be  the  same.  Every  CD structure has a number of  empty  slots  that  need to  be tilled,  such as: an ACTOR an ACTION  performed by that actor an OBJECT that the action is performed upon a DIRECTION  in which the action is performed. The number and type of  slots depend on the particular primitive  used. For  example, the  “ATRANS”  primitive represents abstract  transfer  of  possession,  control  or ownership.  Using  ATRANS  we  can  form  CD representations of the following  sentences: “John gave Mary  a ball” “Mary  took a ball from John” actor:  John  actor:  Mary action:  ATRANS action:  ATRANS object: ball  object:  ball direction: TO Mary direction:  TO Mary FROM  John FROM John Although  both  sentences differ markedly syntactically, they have nearly the  same semantic  meaning and so the CDs are almost identical. Other primitives include: PTRANS  Physical transfer of location ATTEND  Focus a sense organ SPEAK Make a sound GRASP Grasp an object BUILD  Construct an object MTRANS  Mental transfer of information The primitive  chosen is  determined by  verb analysis - “gave”  and “took”  activates ATRANS,  “look”  implies ATTEND  etc.  More  complex sentences, and even entire passages can be represented by linking CDs together. Conceptual Dependencies have been used successfully for  text-based natural language processing, however with small  modifications  multimodal  input  can  also  be represented in  a CD  form.  Instead of  extracting primitive actions from speech only,  the user’s gestures, actions and location can also be analyzed for  verb equivalents.  For example, in  our virtual  environment a user could move a block  from  the  floor  to  a  table  top  with  the  vocal command “Put the block on top of  the table”.  This  would have the CD representation: actor: user action: PTRANS object:  block direction:  TO table top FROM  floor Alternatively,  the user could  pick  up  the  block  and move it  to  the table top  without  saying  anything.  The gesture of  picking  up a block  would cause the PTRANS primitive  to  be  activated and the  appropriate procedures called to fill  its object and direction slots. This would also produce the same CD representation as above. 171 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
l ,False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Finally,  the user could say “Put  the  block  over there (pointing at the table top)“.  In this case separate CDs will be generated for the speech and gestural input  with  empty slots for the unknown information: actor:  user  actor:  user action:  PTRANS action:  ATTEND object:  block  object: hand direction:  TO unknown direction:  TO table top FROM  floor FROM  unknown “Put  the block over there” Pointing at the table top Empty  slots  in  a  CD  structure  can  be  filled  by examining  CDs  from  other  modalities  generated at  the same  time.  Comparing and  combining  the  two representations above, it  is  simple  to  arrive at  the  same CD  produced by  the  purely  spoken  or  gestural  inputs. Using  the  same representation for  all  the  various  input modalities  considerably  simplifies  integration  of  the different modalities at the semantic level.  In  those cases where CDs  alone do not contain  the  needed information, empty slots can be used as a basis for  querying the virtual environment directly or reexamining the users input. A common CD  representation enhances the speech and gestural  recognition  accuracy by  allowing  one  input modality  to  be  considered in  terms  of  another. In  our system, the speech recognizer returns the  top  three most probable spoken  phrases. By  considering these phrases together with  any gestural CDs they can be disambiguated into the most  likely  input.  For  example, if  the user says “Bring  me  that”,  while  pointing  at  an  object,  the  top responses from the speech recognizer might be: “Bring me the mail. ” “Bring me that. ” “Where is the mail?” Since  they  were  pointing  while  they  spoke,  an ATTEND  CD  will  be in  the fact database and the  expert system  will  infer  .that  the  second phrase must  be  the correct one.  In the next section we show how higher level contextual  knowledge can  also  be  applied  to  improve recognition accuracy. The  CD  representation also  has  the  advantage of decoupling the input  modalities from  their  interpretation, allowing the same meaning to be expressed in a number of different ways. This makes it  easy to  develop higher level pragmatic  understanding, since  the  underlying  semantic representations will  be the  same, regardless of  the  input modalities  used to  express them.  In  this  way  if  the computer fails to interpret one type of  input  correctly, the user can try conveying the same meaning using a different modality. There are, however, some disadvantages in  using  CDs, the most obvious being that CD primitives  are mainly  for physical actions making it difficult  to  represent emotional states and goal-based behavior[21]. The  number of  CDs needed to  represent  natural  language  also  becomes unwieldy as the length of the input  increases. While  these are serious limitations  for  representing large amounts of text,  they  are not  so  prohibitive  for  input  in  a  virtual environment in  which  the  user interacts using  primarily physical actions and/or short spoken commands. In our interface we use CDs to recognize and respond to user  actions,  for  agent  planning,  and  to  represent commands sent to  the virtual  world.  For  example, if  the user says “Robot,  go  over  there”,  while  pointing  at  a location, a unified PTRANS  representation is  found from the  voice  and  gestural input  and inserted into  the  fact database: actor:  user action: PTRANS object:  robot direction:  TO point-target FROM  current-location. This  then  matches  the  rules  for  sending  a  move command to the virtual  environment and the robot  moves to the location pointed at. In  a similar  way, the command “What  is  that?” while  pointing  at an object,  puts  entries into  the  fact  database which  cause the  expert system to query the virtual world object database for the object name. Context  and  Pragmatic  Understanding The techniques described above are sufficient to monitor and  respond to  simple  user  actions  and  multimodal commands,  but  cannot  identify  sets  or  sequences of actions. By extending this  rule base and adding procedural knowledge  the  expert  system  can  have  higher  level contextual and pragmatic understanding. We define context understanding as being able to recognize a task made up  of a group of actions, while pragmatic understanding is being able  to  monitor  user  progress through  a  sequence of actions or contexts. To demonstrate this we now describe a prototype sinus  surgery interface developed to  meet two critical  needs; providing  accurate localization  information during the surgical procedure, and providing  students with medical simulators  that  give  expert  training  assistance. Complete details of the interface can be found in [24]. 172 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"Finally,  the user could say “Put  the  block  over there (pointing at the table top)“.  In this case separate CDs will be generated for the speech and gestural input  with  empty slots for the unknown information: actor:  user  actor:  user action:  PTRANS action:  ATTEND object:  block  object: hand direction:  TO unknown direction:  TO table top FROM  floor FROM  unknown “Put  the block over there” Pointing at the table top Empty  slots  in  a  CD  structure  can  be  filled  by examining  CDs  from  other  modalities  generated at  the same  time.  Comparing and  combining  the  two representations above, it  is  simple  to  arrive at  the  same CD  produced by  the  purely  spoken  or  gestural  inputs. Using  the  same representation for  all  the  various  input modalities  considerably  simplifies  integration  of  the different modalities at the semantic level.  In  those cases where CDs  alone do not contain  the  needed information, empty slots can be used as a basis for  querying the virtual environment directly or reexamining the users input. A common CD  representation enhances the speech and gestural  recognition  accuracy by  allowing  one  input modality  to  be  considered in  terms  of  another. In  our system, the speech recognizer returns the  top  three most probable spoken  phrases. By  considering these phrases together with  any gestural CDs they can be disambiguated into the most  likely  input.  For  example, if  the user says “Bring  me  that”,  while  pointing  at  an  object,  the  top responses from the speech recognizer might be: “Bring me the mail. ” “Bring me that. ” “Where is the mail?” Since  they  were  pointing  while  they  spoke,  an ATTEND  CD  will  be in  the fact database and the  expert system  will  infer  .that  the  second phrase must  be  the correct one.  In the next section we show how higher level contextual  knowledge can  also  be  applied  to  improve recognition accuracy. The  CD  representation also  has  the  advantage of decoupling the input  modalities from  their  interpretation, allowing the same meaning to be expressed in a number of different ways. This makes it  easy to  develop higher level pragmatic  understanding, since  the  underlying  semantic representations will  be the  same, regardless of  the  input modalities  used to  express them.  In  this  way  if  the computer fails to interpret one type of  input  correctly, the user can try conveying the same meaning using a different modality. There are, however, some disadvantages in  using  CDs, the most obvious being that CD primitives  are mainly  for physical actions making it difficult  to  represent emotional states and goal-based behavior[21]. The  number of  CDs needed to  represent  natural  language  also  becomes unwieldy as the length of the input  increases. While  these are serious limitations  for  representing large amounts of text,  they  are not  so  prohibitive  for  input  in  a  virtual environment in  which  the  user interacts using  primarily physical actions and/or short spoken commands. In our interface we use CDs to recognize and respond to user  actions,  for  agent  planning,  and  to  represent commands sent to  the virtual  world.  For  example, if  the user says “Robot,  go  over  there”,  while  pointing  at  a location, a unified PTRANS  representation is  found from the  voice  and  gestural input  and inserted into  the  fact database: actor:  user action: PTRANS object:  robot direction:  TO point-target FROM  current-location. This  then  matches  the  rules  for  sending  a  move command to the virtual  environment and the robot  moves to the location pointed at. In  a similar  way, the command “What  is  that?” while  pointing  at an object,  puts  entries into  the  fact  database which  cause the  expert system to query the virtual world object database for the object name. Context  and  Pragmatic  Understanding The techniques described above are sufficient to monitor and  respond to  simple  user  actions  and  multimodal commands,  but  cannot  identify  sets  or  sequences of actions. By extending this  rule base and adding procedural knowledge  the  expert  system  can  have  higher  level contextual and pragmatic understanding. We define context understanding as being able to recognize a task made up  of a group of actions, while pragmatic understanding is being able  to  monitor  user  progress through  a  sequence of actions or contexts. To demonstrate this we now describe a prototype sinus  surgery interface developed to  meet two critical  needs; providing  accurate localization  information during the surgical procedure, and providing  students with medical simulators  that  give  expert  training  assistance. Complete details of the interface can be found in [24]. 172 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"The sinus surgery interface consists of  an anatomical model of the inside of the nasal cavity connected to  a rule- based expert  system,  voice  recognition  software  and synthesized speech output.  The  virtual  environment  is viewed on a monitor,  simulating  .the view  from  a  video endoscope. Interaction with  the  model is  through  virtual instruments whose motions correspond to  the movements of  a polhemus magnetic sensor attached to  the  surgeons hand. The remaining hardware and software configurations are the  same as used for  the  previous  intelligent.  agent interface. The expert system contains the same feature-based and semantic understanding described above, but  it  can  also infer interaction context from  user actions and match this context  against  procedural  knowledge  of  the  steps undertaken in sinus surgery. Using  this  contextual  and pragmatic  knowledge  the users progress can be monitored through the operation step by  step and the  system ca.n respond automatically  when the user performs a task out  of  sequence. It  can also can give expert help during training.  If  a student asks “What do I do now?“, the system responds by vocalizing the next step in  the  operation. Alternatively,  if  the  user  asks “Show  me what to  do now”,  the expert system rules  for the next step are used to generate commands to  the virtual environment that move the virtual  instruments in a correct manner. Context Understanding To  identify  which  step the  user is  attempting  in  the operation, the expert system combines information  from the user’s multimodal  input  with  the state of  the virtual world  and  compares it  to  sets  of  rules  designed to recognize context.  Each set  of  rules  corresponds to  the beginning of a different step in  the surgical procedure and, when activated, causes the  rest of  the  user  interaction to  be interpreted from  within  that  surgical context. This contextual knowledge is  also used to  improve speech and gesture recognition. In a related problem,  De.long developed a NLP  system which could read news stories and find the context for  each story[23]. This was achieved by  encoding a wide range of contexts into  sets of  CDs,  and when  the  NLP  output matched on  one of  these s&s, the associated context  was activated. Context  activation  occurred in  a  number  of different ways: . Explicitly,  where the users input  contains an explicit reference to the desired context. . Implicitly,  where a  context  related to  the  current context is activated due to some implicit  connection. l Event induced, where events in the user input relate to a particular context In  our  interface we  use a  similar  approach. Explicit context activation occurs when the user says the  surgical step they are about to undertake. The words “I’m  going  to remove the  uncinate”,  will  produce a CD  representation that matches the  rule  for  setting  the  surgical context  to removal  of  the  uncinate process. Event  induced context recognition is more complex.  A  set of  events is  chosen which  happened immediately  prior  to  the  start  of  a particular surgical stage or  shortly  thereafter.  Rules  are then written  for  identifying  these events and the context established when these rules  are activated.  For  example the key events for recognizing when the user is  about to remove the uncinate process are: . The user is holding a sickle knife or cattle elevator. . The instrument has moved past the middle turbinate. . The instrument has collided with the uncinate bulge. These events are represented by the appropriate CDs and syntactic features and a set of  rules determine if  they have been inserted into the fact database. If  so, the context is set to the surgical step of removing the uncinate process. For example, if  the user is  holding the correct instrument the following  CD will  be present in the fact database: actor: User action:  ATRANS object: sickle knife or cattle elevator direction:  TO User FROM Instrument Tray Then  the  user  needs to  has  to  have  moved  the instrument back  far  enough;  if  so  a  Collide Turbinate-Plane  fact will  be in  the fact database, inserted when the instrument collides with  an invisible  horizontal plane across the front of the middle turbinate. Finally,  the user needs to  attempt  to  cut  the  uncinate process so  a Collide  UncinateBoundingBox  should  be  present.  If these three facts are present then the  system  recognizes that the user is  trying  to  remove the uncinate process and sets the context accordingly. Once a  particular  context  has  been  identified,  this knowledge can be  used to  improve  speech and gesture recognition at the feature or semantic understanding levels. Specific  vocabularies  can  be  associated  with  certain contexts and searched first  during recognition. In  this  way 173 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"The sinus surgery interface consists of  an anatomical model of the inside of the nasal cavity connected to  a rule- based expert  system,  voice  recognition  software  and synthesized speech output.  The  virtual  environment  is viewed on a monitor,  simulating  .the view  from  a  video endoscope. Interaction with  the  model is  through  virtual instruments whose motions correspond to  the movements of  a polhemus magnetic sensor attached to  the  surgeons hand. The remaining hardware and software configurations are the  same as used for  the  previous  intelligent.  agent interface. The expert system contains the same feature-based and semantic understanding described above, but  it  can  also infer interaction context from  user actions and match this context  against  procedural  knowledge  of  the  steps undertaken in sinus surgery. Using  this  contextual  and pragmatic  knowledge  the users progress can be monitored through the operation step by  step and the  system ca.n respond automatically  when the user performs a task out  of  sequence. It  can also can give expert help during training.  If  a student asks “What do I do now?“, the system responds by vocalizing the next step in  the  operation. Alternatively,  if  the  user  asks “Show  me what to  do now”,  the expert system rules  for the next step are used to generate commands to  the virtual environment that move the virtual  instruments in a correct manner. Context Understanding To  identify  which  step the  user is  attempting  in  the operation, the expert system combines information  from the user’s multimodal  input  with  the state of  the virtual world  and  compares it  to  sets  of  rules  designed to recognize context.  Each set  of  rules  corresponds to  the beginning of a different step in  the surgical procedure and, when activated, causes the  rest of  the  user  interaction to  be interpreted from  within  that  surgical context. This contextual knowledge is  also used to  improve speech and gesture recognition. In a related problem,  De.long developed a NLP  system which could read news stories and find the context for  each story[23]. This was achieved by  encoding a wide range of contexts into  sets of  CDs,  and when  the  NLP  output matched on  one of  these s&s, the associated context  was activated. Context  activation  occurred in  a  number  of different ways: . Explicitly,  where the users input  contains an explicit reference to the desired context. . Implicitly,  where a  context  related to  the  current context is activated due to some implicit  connection. l Event induced, where events in the user input relate to a particular context In  our  interface we  use a  similar  approach. Explicit context activation occurs when the user says the  surgical step they are about to undertake. The words “I’m  going  to remove the  uncinate”,  will  produce a CD  representation that matches the  rule  for  setting  the  surgical context  to removal  of  the  uncinate process. Event  induced context recognition is more complex.  A  set of  events is  chosen which  happened immediately  prior  to  the  start  of  a particular surgical stage or  shortly  thereafter.  Rules  are then written  for  identifying  these events and the context established when these rules  are activated.  For  example the key events for recognizing when the user is  about to remove the uncinate process are: . The user is holding a sickle knife or cattle elevator. . The instrument has moved past the middle turbinate. . The instrument has collided with the uncinate bulge. These events are represented by the appropriate CDs and syntactic features and a set of  rules determine if  they have been inserted into the fact database. If  so, the context is set to the surgical step of removing the uncinate process. For example, if  the user is  holding the correct instrument the following  CD will  be present in the fact database: actor: User action:  ATRANS object: sickle knife or cattle elevator direction:  TO User FROM Instrument Tray Then  the  user  needs to  has  to  have  moved  the instrument back  far  enough;  if  so  a  Collide Turbinate-Plane  fact will  be in  the fact database, inserted when the instrument collides with  an invisible  horizontal plane across the front of the middle turbinate. Finally,  the user needs to  attempt  to  cut  the  uncinate process so  a Collide  UncinateBoundingBox  should  be  present.  If these three facts are present then the  system  recognizes that the user is  trying  to  remove the uncinate process and sets the context accordingly. Once a  particular  context  has  been  identified,  this knowledge can be  used to  improve  speech and gesture recognition at the feature or semantic understanding levels. Specific  vocabularies  can  be  associated  with  certain contexts and searched first  during recognition. In  this  way 173 Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) 0-8186-7295-1/96 $10.00 © 1996 IEEE Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
l ,False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Food ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Food ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
$Name ,False,False,False,False,False,False
SO ,True,False,False,True,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
"[ IlNegroponte, ",False,False,False,False,False,False
Cambridge: ,False,False,False,False,False,False
"MIT,  1970. ",True,False,False,True,False,False
"[2]Sparrell,  C.J.,  Koons,  D.B.  Interpretation  of  Coverbal ",False,False,False,False,False,False
"Standford,  March  21-23,  1994. ",False,False,False,False,False,False
"[S]Herranz,  E.J. ",False,False,False,False,False,False
and ,False,False,False,False,False,False
"S.M.  Thesis,  MIT  Media ",False,False,False,False,False,False
"Arts  and Sciences section, MIT,  1992. ",False,False,False,False,False,False
"[4]Hauptman,  A.G.  and McAvinny,  P.  Gestures with  Speech ",False,False,False,False,False,False
for ,False,False,False,False,False,False
"vol..  38,  pp  231-249,  1993. ",False,False,False,False,False,False
"[S]Cohen,P.R.,  Dalrymple,M.,  Pereira,  F.C.N.,  Sulliv.an, ",False,False,False,False,False,False
"J.W.,  Gargan  Jr.,  R.A.,  Schlossberg,  J.L.,  Tyler,  S.W. ",False,False,False,False,False,False
Synergistic Use of Direct  Manipulation  and Natural  Language. ,False,False,False,False,False,False
"‘89),  pp. 227-233. Austin,  Texas,  ACM,  1989. ",False,False,False,False,False,False
"[6]Cohen, P.R. The Role of  Natural Language in  a Multimodal ",False,False,False,False,False,False
Interface. ,False,False,False,False,False,False
pp.  143-149. ,False,False,False,False,False,False
"[7]Weimer,  D.  and  Ganapathy,  S.K.  A  Synthetic  Visual ",False,False,False,False,False,False
Environment  with  Hand  Gesturing ,False,False,False,False,False,False
and  Voice  Input. ,False,False,False,False,False,False
"‘89), ~~235-240. Austin,  Texas, IEEE,  ACM,  30th April  - 4th ",False,False,False,False,False,False
May  1989. ,False,False,False,False,False,False
"[8]Salisbury,  M.W.,  Hendrickson,  J.H.,  Lammers,  T.L.,  Fu ",False,False,False,False,False,False
"C.,  Moody,  S.A.  Talk  and  Draw:  Bundling  Speech  and ",False,False,False,False,False,False
Graphics. ,False,False,False,False,False,False
"no. 8, pp.  59-65. ",False,False,False,False,False,False
"[9]Bolt,  R.A.  “Put-That-There”:  Voice  and  Gesture  at  the ",False,False,False,False,False,False
Graphics Interface. Computer ,False,False,False,False,False,False
"(no.  3),  pp  262- ",False,False,False,False,False,False
In ,False,False,False,False,False,False
of ,False,False,False,False,False,False
for ,False,False,False,False,False,False
MIT ,True,False,False,True,False,False
"Press, Cambridge,  Massachusetts,  1983. ",False,False,False,False,False,False
"[IllHill,  W.,  Wroblewski,  D.,  McCandless,  T.,  Cohen,  R. ",False,False,False,False,False,False
ed. ,False,False,False,False,False,False
"PWS  Publishing  Company,  Boston,  1993. ",False,False,False,False,False,False
of  the ,False,False,False,False,False,False
Vol. ,False,False,False,False,False,False
"36, No.  7, ",False,False,False,False,False,False
Application  of  Shared Virtual  Reality  to  Situational  Training. ,False,False,False,False,False,False
In ,False,False,False,False,False,False
"~~156-161.  Research Triangle  Park,  North  Carolina,  IEEE, ",False,False,False,False,False,False
ed.  M. ,False,False,False,False,False,False
"[ 16]Neal,  J.G.,  Shapiro,  S.C.  Intelligent  Multi-Media ",False,False,False,False,False,False
Interface  Technology.  In ,False,False,False,False,False,False
"Sullivan, ",False,False,False,False,False,False
"J.W.,  Tyler,  SW.  eds, Frontier  series, ACM  Press, New York, ",False,False,False,False,False,False
1991. ,False,False,True,True,False,False
"[ 17]Shneiderman,  B. ",False,False,False,False,False,False
Addison- ,False,False,False,False,False,False
"Wesley,  New  York,  1992. ",False,False,False,False,False,False
"Johnson Space Center, Houston, TX,  1994. ",False,False,False,False,False,False
"[19]Savage, J., Holden,  A.,  Billinghurst,  M.  A  Hybrid  System ",False,False,False,False,False,False
Recognition. ,False,False,False,False,False,False
"HIT  Lab, ",False,False,False,False,False,False
"University  of  Washington,  1994. ",False,False,False,False,False,False
Descendents. ,False,False,False,False,False,False
"Vol.  23,  No.  2-5,  pp. ",False,False,False,False,False,False
"51-73,  1992. ",False,False,False,False,False,False
"[21]Waltz, ",False,False,False,False,False,False
"Lawerance  Erlbaum  Associates, ",False,False,False,False,False,False
"Hillsdale,  New  Jersey, 1989. ",False,False,False,False,False,False
"[22]Schank,  R.C. ",False,False,False,False,False,False
North- ,False,False,False,False,False,False
"Holland  Amsterdam,  1975. ",False,False,False,False,False,False
"[23]Schank,  R.C.,  Abelson,  R. ",False,False,False,False,False,False
"Lawerance Erlbaum  Associates, Hillsdale,  New ",False,False,False,False,False,False
"Jersey,  1977. ",False,False,False,False,False,False
"[24]Billinghurst,  M.,  Savage, J.,  Oppenheimer,  P.,  Edmond, ",False,False,False,False,False,False
C.  The  Expert  Surgical  Assistant:  An  Intelligent  Virtual ,False,False,False,False,False,False
with  Multimodal  Input.  Submitted  to  Medicine ,False,False,False,False,False,False
"Meets Virtual  Reality  IV,  1996. ",False,False,False,False,False,False
for ,False,False,False,False,False,False
"ed. W.G.  Lehnert, ",False,False,False,False,False,False
"M.H.  Ringle,  pp.  149-176,  Lawerance Erlbaum  Associates, ",False,False,False,False,False,False
"[26]Savage,  J.,  Billinghurst,  M.,  Holden,  A.  Context ",False,False,False,False,False,False
Representation Used  in  Speech Recognition. ,False,False,False,False,False,False
"HIT  Lab,  University  of  Washington,  1995. ",False,False,False,False,False,False
175 ,False,False,False,False,False,False
Proceedings of the 1996 Virtual Reality Annual International Symposium (VRAIS '96) ,False,False,False,False,False,False
0-8186-7295-1/96 $10.00 © 1996 IEEE ,True,False,False,True,False,False
"Authorized licensed use limited to: Thakur College of Engineering and Technology. Downloaded on June 04,2010 at 06:18:58 UTC from IEEE Xplore.  Restrictions apply. ",False,False,False,False,False,False
